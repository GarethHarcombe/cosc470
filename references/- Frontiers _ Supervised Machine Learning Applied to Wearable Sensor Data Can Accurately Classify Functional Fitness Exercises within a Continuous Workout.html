
<!DOCTYPE html>
<html lang="en">
<head>
    <!-- anti-flicker snippet (recommended)  -->
    <style>
        .async-hide {
            opacity: 0 !important
        }
    </style>
    <script>
        (function (a, s, y, n, c, h, i, d, e) {
            s.className += ' ' + y; h.start = 1 * new Date;
            h.end = i = function () { s.className = s.className.replace(RegExp(' ?' + y), '') };
            (a[n] = a[n] || []).hide = h; setTimeout(function () { i(); h.end = null }, c); h.timeout = c;
        })(window, document.documentElement, 'async-hide', 'dataLayer', 4000,
            { 'OPT-KW964JJ' : true });</script>
    <!-- end anti-flicker snippet (recommended)  -->
    <!-- Google Tag Manager -->
    <script>
        (function (w, d, s, l, i) {
            w[l] = w[l] || []; w[l].push({ 'gtm.start': new Date().getTime(), event: 'gtm.js' });
            var f = d.getElementsByTagName(s)[0], j = d.createElement(s), dl = l != 'dataLayer' ? '&l=' + l : ''; j.async = true;
            j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl + '&gtm_auth=NsJH3Ts1-89I8lZURwQYmw&gtm_preview=env-1&gtm_cookies_win=x';
            f.parentNode.insertBefore(j, f);
        })(window, document, 'script', 'dataLayer', 'GTM-TFKV632');
    </script>
    <script>
        (function (w, d, s, l, i) {
            w[l] = w[l] || []; w[l].push({ 'gtm.start': new Date().getTime(), event: 'gtm.js' });
            var f = d.getElementsByTagName(s)[0], j = d.createElement(s), dl = l != 'dataLayer' ? '&l=' + l : ''; j.async = true;
            j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl + '&gtm_auth=jfKETtlWamfZ4l02YiFCQw&gtm_preview=env-1&gtm_cookies_win=x';
            f.parentNode.insertBefore(j, f);
        })(window, document, 'script', 'dataLayer', 'GTM-N279F7B');
    </script>
    <!-- End Google Tag Manager -->
    <script src="https://unpkg.com/vue@2.6.14/dist/vue.min.js"></script>
    <!--Snowplow Tag-->

        <script type="text/javascript">
            (function (p, l, o, w, i, n, g) {
                if (!p[i]) {
                    p.GlobalSnowplowNamespace = p.GlobalSnowplowNamespace || [];
                    p.GlobalSnowplowNamespace.push(i); p[i] = function () { (p[i].q = p[i].q || []).push(arguments) }; p[i].q = p[i].q || [];
                    n = l.createElement(o); g = l.getElementsByTagName(o)[0]; n.async = 1; n.src = w;
                    g.parentNode.insertBefore(n, g)
                }
            }(window, document, "script", "https://cdnjs.cloudflare.com/ajax/libs/snowplow/2.18.2/sp.min.js", "snowplow"));

            snowplow('newTracker', 'spTracker-inline-live', 'sp-live-collector.cdp.frontiersin.io', {
                appId: '1',
                eventMethod: 'post',
                anonymousTracking: true,
                stateStorageStrategy: 'none',
                contexts: {
                    webPage: true
                }
            });
            snowplow('enableActivityTracking', 30, 10);
            window.snowplow('trackPageView');
            snowplow('enableAnonymousTracking', { withServerAnonymisation: true });
            snowplow('clearUserData');
            snowplow('setUserId', '0');
        </script>
    <!--End Snowplow Tag-->

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge" /><script type="text/javascript">window.NREUM||(NREUM={});NREUM.info = {"beacon":"bam.nr-data.net","errorBeacon":"bam.nr-data.net","licenseKey":"598a124f17","applicationID":"3007887","transactionName":"MQcDMkECCkNSW0YMWghNLDBwTCVCR1FRCVAlDQ8SQQwIXFZKHSNACg41A0sXJkl3d3s=","queueTime":0,"applicationTime":608,"agent":"","atts":""}</script><script type="text/javascript">(window.NREUM||(NREUM={})).init={ajax:{deny_list:["bam.nr-data.net"]}};(window.NREUM||(NREUM={})).loader_config={xpid:"VgUHUl5WGwAAVFZaDwY=",licenseKey:"598a124f17",applicationID:"3007887"};;(()=>{var e,t,r={9071:(e,t,r)=>{"use strict";r.d(t,{I:()=>n});var n=0,i=navigator.userAgent.match(/Firefox[\/\s](\d+\.\d+)/);i&&(n=+i[1])},6562:(e,t,r)=>{"use strict";r.d(t,{P_:()=>p,Mt:()=>v,C5:()=>d,DL:()=>y,OP:()=>k,lF:()=>H,Yu:()=>E,Dg:()=>g,CX:()=>f,GE:()=>w,sU:()=>L});var n={};r.r(n),r.d(n,{agent:()=>x,match:()=>_,version:()=>O});var i=r(6797),o=r(909),a=r(8610);class s{constructor(e,t){try{if(!e||"object"!=typeof e)return(0,a.Z)("New setting a Configurable requires an object as input");if(!t||"object"!=typeof t)return(0,a.Z)("Setting a Configurable requires a model to set its initial properties");Object.assign(this,t),Object.entries(e).forEach((e=>{let[t,r]=e;const n=(0,o.q)(t);n.length&&r&&"object"==typeof r&&n.forEach((e=>{e in r&&((0,a.Z)('"'.concat(e,'" is a protected attribute and can not be changed in feature ').concat(t,".  It will have no effect.")),delete r[e])})),this[t]=r}))}catch(e){(0,a.Z)("An error occured while setting a Configurable",e)}}}const c={beacon:i.ce.beacon,errorBeacon:i.ce.errorBeacon,licenseKey:void 0,applicationID:void 0,sa:void 0,queueTime:void 0,applicationTime:void 0,ttGuid:void 0,user:void 0,account:void 0,product:void 0,extra:void 0,jsAttributes:{},userAttributes:void 0,atts:void 0,transactionName:void 0,tNamePlain:void 0},u={};function d(e){if(!e)throw new Error("All info objects require an agent identifier!");if(!u[e])throw new Error("Info for ".concat(e," was never set"));return u[e]}function f(e,t){if(!e)throw new Error("All info objects require an agent identifier!");u[e]=new s(t,c),(0,i.Qy)(e,u[e],"info")}const l={allow_bfcache:!1,privacy:{cookies_enabled:!0},ajax:{deny_list:void 0,enabled:!0,harvestTimeSeconds:10},distributed_tracing:{enabled:void 0,exclude_newrelic_header:void 0,cors_use_newrelic_header:void 0,cors_use_tracecontext_headers:void 0,allowed_origins:void 0},ssl:void 0,obfuscate:void 0,jserrors:{enabled:!0,harvestTimeSeconds:10},metrics:{enabled:!0,harvestTimeSeconds:10},page_action:{enabled:!0,harvestTimeSeconds:30},page_view_event:{enabled:!0},page_view_timing:{enabled:!0,harvestTimeSeconds:30},session_trace:{enabled:!0,harvestTimeSeconds:10},spa:{enabled:!0,harvestTimeSeconds:10}},h={};function p(e){if(!e)throw new Error("All configuration objects require an agent identifier!");if(!h[e])throw new Error("Configuration for ".concat(e," was never set"));return h[e]}function g(e,t){if(!e)throw new Error("All configuration objects require an agent identifier!");h[e]=new s(t,l),(0,i.Qy)(e,h[e],"config")}function v(e,t){if(!e)throw new Error("All configuration objects require an agent identifier!");var r=p(e);if(r){for(var n=t.split("."),i=0;i<n.length-1;i++)if("object"!=typeof(r=r[n[i]]))return;r=r[n[n.length-1]]}return r}const m={accountID:void 0,trustKey:void 0,agentID:void 0,licenseKey:void 0,applicationID:void 0,xpid:void 0},b={};function y(e){if(!e)throw new Error("All loader-config objects require an agent identifier!");if(!b[e])throw new Error("LoaderConfig for ".concat(e," was never set"));return b[e]}function w(e,t){if(!e)throw new Error("All loader-config objects require an agent identifier!");b[e]=new s(t,m),(0,i.Qy)(e,b[e],"loader_config")}const E=(0,i.mF)().o;var A=r(2053),x=null,O=null;if(navigator.userAgent){var T=navigator.userAgent,S=T.match(/Version\/(\S+)\s+Safari/);S&&-1===T.indexOf("Chrome")&&-1===T.indexOf("Chromium")&&(x="Safari",O=S[1])}function _(e,t){if(!x)return!1;if(e!==x)return!1;if(!t)return!0;if(!O)return!1;for(var r=O.split("."),n=t.split("."),i=0;i<n.length;i++)if(n[i]!==r[i])return!1;return!0}var I=r(5526),P=r(2374);const j="NRBA_SESSION_ID";function R(){if(!P.il)return null;try{let e;return null===(e=window.sessionStorage.getItem(j))&&(e=(0,I.ky)(16),window.sessionStorage.setItem(j,e)),e}catch(e){return null}}var D=r(8226);const C=e=>({customTransaction:void 0,disabled:!1,isolatedBacklog:!1,loaderType:void 0,maxBytes:3e4,offset:(0,A.yf)(),onerror:void 0,origin:""+P._A.location,ptid:void 0,releaseIds:{},sessionId:1==v(e,"privacy.cookies_enabled")?R():null,xhrWrappable:"function"==typeof P._A.XMLHttpRequest?.prototype?.addEventListener,userAgent:n,version:D.q}),N={};function k(e){if(!e)throw new Error("All runtime objects require an agent identifier!");if(!N[e])throw new Error("Runtime for ".concat(e," was never set"));return N[e]}function L(e,t){if(!e)throw new Error("All runtime objects require an agent identifier!");N[e]=new s(t,C(e)),(0,i.Qy)(e,N[e],"runtime")}function H(e){return function(e){try{const t=d(e);return!!t.licenseKey&&!!t.errorBeacon&&!!t.applicationID}catch(e){return!1}}(e)}},8226:(e,t,r)=>{"use strict";r.d(t,{q:()=>n});const n="1225.PROD"},9557:(e,t,r)=>{"use strict";r.d(t,{w:()=>o});var n=r(8610);const i={agentIdentifier:""};class o{constructor(e){try{if("object"!=typeof e)return(0,n.Z)("shared context requires an object as input");this.sharedContext={},Object.assign(this.sharedContext,i),Object.entries(e).forEach((e=>{let[t,r]=e;Object.keys(i).includes(t)&&(this.sharedContext[t]=r)}))}catch(e){(0,n.Z)("An error occured while setting SharedContext",e)}}}},4329:(e,t,r)=>{"use strict";r.d(t,{L:()=>d,R:()=>c});var n=r(3752),i=r(7022),o=r(4045),a=r(2325);const s={};function c(e,t){const r={staged:!1,priority:a.p[t]||0};u(e),s[e].get(t)||s[e].set(t,r)}function u(e){e&&(s[e]||(s[e]=new Map))}function d(){let e=arguments.length>0&&void 0!==arguments[0]?arguments[0]:"",t=arguments.length>1&&void 0!==arguments[1]?arguments[1]:"feature";if(u(e),!e||!s[e].get(t))return a(t);s[e].get(t).staged=!0;const r=Array.from(s[e]);function a(t){const r=e?n.ee.get(e):n.ee,a=o.X.handlers;if(r.backlog&&a){var s=r.backlog[t],c=a[t];if(c){for(var u=0;s&&u<s.length;++u)f(s[u],c);(0,i.D)(c,(function(e,t){(0,i.D)(t,(function(t,r){r[0].on(e,r[1])}))}))}delete a[t],r.backlog[t]=null,r.emit("drain-"+t,[])}}r.every((e=>{let[t,r]=e;return r.staged}))&&(r.sort(((e,t)=>e[1].priority-t[1].priority)),r.forEach((e=>{let[t]=e;a(t)})))}function f(e,t){var r=e[1];(0,i.D)(t[r],(function(t,r){var n=e[0];if(r[0]===n){var i=r[1],o=e[3],a=e[2];i.apply(o,a)}}))}},3752:(e,t,r)=>{"use strict";r.d(t,{c:()=>f,ee:()=>u});var n=r(6797),i=r(3916),o=r(7022),a=r(6562),s="nr@context";let c=(0,n.fP)();var u;function d(){}function f(e){return(0,i.X)(e,s,l)}function l(){return new d}function h(){(u.backlog.api||u.backlog.feature)&&(u.aborted=!0,u.backlog={})}c.ee?u=c.ee:(u=function e(t,r){var n={},c={},f={},p=!1;try{p=16===r.length&&(0,a.OP)(r).isolatedBacklog}catch(e){}var g={on:b,addEventListener:b,removeEventListener:y,emit:m,get:E,listeners:w,context:v,buffer:A,abort:h,aborted:!1,isBuffering:x,debugId:r,backlog:p?{}:t&&"object"==typeof t.backlog?t.backlog:{}};return g;function v(e){return e&&e instanceof d?e:e?(0,i.X)(e,s,l):l()}function m(e,r,n,i,o){if(!1!==o&&(o=!0),!u.aborted||i){t&&o&&t.emit(e,r,n);for(var a=v(n),s=w(e),d=s.length,f=0;f<d;f++)s[f].apply(a,r);var l=O()[c[e]];return l&&l.push([g,e,r,a]),a}}function b(e,t){n[e]=w(e).concat(t)}function y(e,t){var r=n[e];if(r)for(var i=0;i<r.length;i++)r[i]===t&&r.splice(i,1)}function w(e){return n[e]||[]}function E(t){return f[t]=f[t]||e(g,t)}function A(e,t){var r=O();g.aborted||(0,o.D)(e,(function(e,n){t=t||"feature",c[n]=t,t in r||(r[t]=[])}))}function x(e){return!!O()[c[e]]}function O(){return g.backlog}}(void 0,"globalEE"),c.ee=u)},9252:(e,t,r)=>{"use strict";r.d(t,{E:()=>n,p:()=>i});var n=r(3752).ee.get("handle");function i(e,t,r,i,o){o?(o.buffer([e],i),o.emit(e,t,r)):(n.buffer([e],i),n.emit(e,t,r))}},4045:(e,t,r)=>{"use strict";r.d(t,{X:()=>o});var n=r(9252);o.on=a;var i=o.handlers={};function o(e,t,r,o){a(o||n.E,i,e,t,r)}function a(e,t,r,i,o){o||(o="feature"),e||(e=n.E);var a=t[o]=t[o]||{};(a[r]=a[r]||[]).push([e,i])}},8544:(e,t,r)=>{"use strict";r.d(t,{bP:()=>s,iz:()=>c,m$:()=>a});var n=r(2374);let i=!1,o=!1;try{const e={get passive(){return i=!0,!1},get signal(){return o=!0,!1}};n._A.addEventListener("test",null,e),n._A.removeEventListener("test",null,e)}catch(e){}function a(e,t){return i||o?{capture:!!e,passive:i,signal:t}:!!e}function s(e,t){let r=arguments.length>2&&void 0!==arguments[2]&&arguments[2];window.addEventListener(e,t,a(r))}function c(e,t){let r=arguments.length>2&&void 0!==arguments[2]&&arguments[2];document.addEventListener(e,t,a(r))}},5526:(e,t,r)=>{"use strict";r.d(t,{Ht:()=>a,M:()=>o,Rl:()=>i,ky:()=>s});var n=r(2374);function i(){var e=null,t=0,r=n._A?.crypto||n._A?.msCrypto;function i(){return e?15&e[t++]:16*Math.random()|0}r&&r.getRandomValues&&(e=r.getRandomValues(new Uint8Array(31)));for(var o,a="xxxxxxxx-xxxx-4xxx-yxxx-xxxxxxxxxxxx",s="",c=0;c<a.length;c++)s+="x"===(o=a[c])?i().toString(16):"y"===o?(o=3&i()|8).toString(16):o;return s}function o(){return s(16)}function a(){return s(32)}function s(e){var t=null,r=0,n=self.crypto||self.msCrypto;n&&n.getRandomValues&&Uint8Array&&(t=n.getRandomValues(new Uint8Array(31)));for(var i=[],o=0;o<e;o++)i.push(a().toString(16));return i.join("");function a(){return t?15&t[r++]:16*Math.random()|0}}},2053:(e,t,r)=>{"use strict";r.d(t,{nb:()=>c,os:()=>u,yf:()=>s,zO:()=>a});var n=r(7145),i=(new Date).getTime(),o=i;function a(){return n.G&&performance.now?Math.round(performance.now()):(i=Math.max((new Date).getTime(),i))-o}function s(){return i}function c(e){o=e}function u(){return o}},7145:(e,t,r)=>{"use strict";r.d(t,{G:()=>n});const n=void 0!==r(2374)._A?.performance?.timing?.navigationStart},6625:(e,t,r)=>{"use strict";r.d(t,{s:()=>c,v:()=>u});var n=r(8283),i=r(9071),o=r(2053),a=r(7145),s=r(2374);let c=!0;function u(e){var t=function(){if(i.I&&i.I<9)return;if(a.G)return c=!1,s._A?.performance?.timing?.navigationStart}();t&&((0,n.B)(e,"starttime",t),(0,o.nb)(t))}},8283:(e,t,r)=>{"use strict";r.d(t,{B:()=>o,L:()=>a});var n=r(2053),i={};function o(e,t,r){void 0===r&&(r=(0,n.zO)()+(0,n.os)()),i[e]=i[e]||{},i[e][t]=r}function a(e,t,r,n){const o=e.sharedContext.agentIdentifier;var a=i[o]?.[r],s=i[o]?.[n];void 0!==a&&void 0!==s&&e.store("measures",t,{value:s-a})}},6368:(e,t,r)=>{"use strict";r.d(t,{e:()=>o});var n=r(2374),i={};function o(e){if(e in i)return i[e];if(0===(e||"").indexOf("data:"))return{protocol:"data"};let t;var r=n._A?.location,o={};if(n.il)t=document.createElement("a"),t.href=e;else try{t=new URL(e,r.href)}catch(e){return o}o.port=t.port;var a=t.href.split("://");!o.port&&a[1]&&(o.port=a[1].split("/")[0].split("@").pop().split(":")[1]),o.port&&"0"!==o.port||(o.port="https"===a[0]?"443":"80"),o.hostname=t.hostname||r.hostname,o.pathname=t.pathname,o.protocol=a[0],"/"!==o.pathname.charAt(0)&&(o.pathname="/"+o.pathname);var s=!t.protocol||":"===t.protocol||t.protocol===r.protocol,c=t.hostname===r.hostname&&t.port===r.port;return o.sameOrigin=s&&(!t.hostname||c),"/"===o.pathname&&(i[e]=o),o}},9548:(e,t,r)=>{"use strict";r.d(t,{T:()=>i});var n=r(2374);const i={isFileProtocol:function(){let e=Boolean("file:"===(0,n.lW)()?.location?.protocol);e&&(i.supportabilityMetricSent=!0);return e},supportabilityMetricSent:!1}},8610:(e,t,r)=>{"use strict";function n(e,t){console&&console.warn&&"function"==typeof console.warn&&(console.warn("New Relic: ".concat(e)),t&&console.warn(t))}r.d(t,{Z:()=>n})},3916:(e,t,r)=>{"use strict";r.d(t,{X:()=>i});var n=Object.prototype.hasOwnProperty;function i(e,t,r){if(n.call(e,t))return e[t];var i=r();if(Object.defineProperty&&Object.keys)try{return Object.defineProperty(e,t,{value:i,writable:!0,enumerable:!1}),i}catch(e){}return e[t]=i,i}},2374:(e,t,r)=>{"use strict";r.d(t,{_A:()=>o,il:()=>n,lW:()=>a,v6:()=>i});const n=Boolean("undefined"!=typeof window&&window.document),i=Boolean("undefined"!=typeof WorkerGlobalScope&&self.navigator instanceof WorkerNavigator);let o=(()=>{if(n)return window;if(i){if("undefined"!=typeof globalThis&&globalThis instanceof WorkerGlobalScope)return globalThis;if(self instanceof WorkerGlobalScope)return self}throw new Error('New Relic browser agent shutting down due to error: Unable to locate global scope. This is possibly due to code redefining browser global variables like "self" and "window".')})();function a(){return o}},7022:(e,t,r)=>{"use strict";r.d(t,{D:()=>i});var n=Object.prototype.hasOwnProperty;function i(e,t){var r=[],i="",o=0;for(i in e)n.call(e,i)&&(r[o]=t(i,e[i]),o+=1);return r}},9226:(e,t,r)=>{"use strict";r.d(t,{$c:()=>u,Ng:()=>d,RR:()=>c});var n=r(6562),i=r(9557),o=r(9548),a=r(8610),s={regex:/^file:\/\/(.*)/,replacement:"file://OBFUSCATED"};class c extends i.w{constructor(e){super(e)}shouldObfuscate(){return u(this.sharedContext.agentIdentifier).length>0}obfuscateString(e){if(!e||"string"!=typeof e)return e;for(var t=u(this.sharedContext.agentIdentifier),r=e,n=0;n<t.length;n++){var i=t[n].regex,o=t[n].replacement||"*";r=r.replace(i,o)}return r}}function u(e){var t=[],r=(0,n.Mt)(e,"obfuscate")||[];return t=t.concat(r),o.T.isFileProtocol()&&t.push(s),t}function d(e){for(var t=!1,r=!1,n=0;n<e.length;n++){"regex"in e[n]?"string"!=typeof e[n].regex&&e[n].regex.constructor!==RegExp&&((0,a.Z)('An obfuscation replacement rule contains a "regex" value with an invalid type (must be a string or RegExp)'),r=!0):((0,a.Z)('An obfuscation replacement rule was detected missing a "regex" value.'),r=!0);var i=e[n].replacement;i&&"string"!=typeof i&&((0,a.Z)('An obfuscation replacement rule contains a "replacement" value with an invalid type (must be a string)'),t=!0)}return!t&&!r}},2650:(e,t,r)=>{"use strict";r.d(t,{K:()=>a,b:()=>o});var n=r(8544);function i(){return"undefined"==typeof document||"complete"===document.readyState}function o(e,t){if(i())return e();(0,n.bP)("load",e,t)}function a(e){if(i())return e();(0,n.iz)("DOMContentLoaded",e)}},6797:(e,t,r)=>{"use strict";r.d(t,{EZ:()=>u,Qy:()=>c,ce:()=>o,fP:()=>a,gG:()=>d,mF:()=>s});var n=r(2053),i=r(2374);const o={beacon:"bam.nr-data.net",errorBeacon:"bam.nr-data.net"};function a(){return i._A.NREUM||(i._A.NREUM={}),void 0===i._A.newrelic&&(i._A.newrelic=i._A.NREUM),i._A.NREUM}function s(){let e=a();return e.o||(e.o={ST:i._A.setTimeout,SI:i._A.setImmediate,CT:i._A.clearTimeout,XHR:i._A.XMLHttpRequest,REQ:i._A.Request,EV:i._A.Event,PR:i._A.Promise,MO:i._A.MutationObserver,FETCH:i._A.fetch}),e}function c(e,t,r){let i=a();const o=i.initializedAgents||{},s=o[e]||{};return Object.keys(s).length||(s.initializedAt={ms:(0,n.zO)(),date:new Date}),i.initializedAgents={...o,[e]:{...s,[r]:t}},i}function u(e,t){a()[e]=t}function d(){return function(){let e=a();const t=e.info||{};e.info={beacon:o.beacon,errorBeacon:o.errorBeacon,...t}}(),function(){let e=a();const t=e.init||{};e.init={...t}}(),s(),function(){let e=a();const t=e.loader_config||{};e.loader_config={...t}}(),a()}},6998:(e,t,r)=>{"use strict";r.d(t,{N:()=>i,e:()=>o});var n=r(8544);function i(e){let t=arguments.length>1&&void 0!==arguments[1]&&arguments[1];return void(0,n.iz)("visibilitychange",(function(){if(t){if("hidden"!=document.visibilityState)return;e()}e(document.visibilityState)}))}function o(){return"hidden"===document.visibilityState?-1:1/0}},6408:(e,t,r)=>{"use strict";r.d(t,{W:()=>i});var n=r(2374);function i(){return"function"==typeof n._A?.PerformanceObserver}},7264:(e,t,r)=>{"use strict";r.d(t,{hF:()=>pe,Fp:()=>O,lx:()=>G,ow:()=>Y,ao:()=>te,G8:()=>oe,XV:()=>z,Zw:()=>N,ig:()=>we,em:()=>le,u5:()=>x,QU:()=>V,_L:()=>$,Gm:()=>ee,Lg:()=>ie,gy:()=>M,BV:()=>C,Kf:()=>ye});var n=r(3752),i=r(8683),o=r.n(i),a=r(2374);const s="nr@original";var c=Object.prototype.hasOwnProperty,u=!1;function d(e,t){return e||(e=n.ee),r.inPlace=function(e,t,n,i,o){n||(n="");var a,s,c,u="-"===n.charAt(0);for(c=0;c<t.length;c++)h(a=e[s=t[c]])||(e[s]=r(a,u?s+n:n,i,s,o))},r.flag=s,r;function r(t,r,n,a,c){return h(t)?t:(r||(r=""),nrWrapper[s]=t,l(t,nrWrapper,e),nrWrapper);function nrWrapper(){var s,u,d,l;try{u=this,s=o()(arguments),d="function"==typeof n?n(s,u):n||{}}catch(t){f([t,"",[s,u,a],d],e)}i(r+"start",[s,u,a],d,c);try{return l=t.apply(u,s)}catch(e){throw i(r+"err",[s,u,e],d,c),e}finally{i(r+"end",[s,u,l],d,c)}}}function i(r,n,i,o){if(!u||t){var a=u;u=!0;try{e.emit(r,n,i,t,o)}catch(t){f([t,r,n,i],e)}u=a}}}function f(e,t){t||(t=n.ee);try{t.emit("internal-error",e)}catch(e){}}function l(e,t,r){if(Object.defineProperty&&Object.keys)try{return Object.keys(e).forEach((function(r){Object.defineProperty(t,r,{get:function(){return e[r]},set:function(t){return e[r]=t,t}})})),t}catch(e){f([e],r)}for(var n in e)c.call(e,n)&&(t[n]=e[n]);return t}function h(e){return!(e&&e instanceof Function&&e.apply&&!e[s])}function p(e,t){e?.[t]?.[s]&&(e[t]=e[t][s])}var g="fetch-",v=g+"body-",m=["arrayBuffer","blob","json","text","formData"],b=a._A.Request,y=a._A.Response,w="prototype",E="nr@context";const A={};function x(e){const t=T(e);if(!(b&&y&&a._A.fetch))return t;if(A[t.debugId]++)return t;function r(e,r,n){var i=e[r];"function"==typeof i&&(e[r]=function(){var e,r=o()(arguments),s={};t.emit(n+"before-start",[r],s),s[E]&&s[E].dt&&(e=s[E].dt);var c=i.apply(this,r);return t.emit(n+"start",[r,e],c),a._A.Promise.resolve(c).then((function(e){return t.emit(n+"end",[null,e],c),e}),(function(e){throw t.emit(n+"end",[e],c),e}))},e[r][s]=i)}return A[t.debugId]=1,m.forEach((e=>{r(b[w],e,v),r(y[w],e,v)})),r(a._A,"fetch",g),t.on(g+"end",(function(e,r){var n=this;if(r){var i=r.headers.get("content-length");null!==i&&(n.rxSize=i),t.emit(g+"done",[null,r],n)}else t.emit(g+"done",[e],n)})),t}function O(e){const t=T(e);1==A[t.debugId]?(m.forEach((e=>{p(b[w],e),p(y[w],e)})),p(a._A,"fetch"),A[t.debugId]=1/0):A[t.debugId]--}function T(e){return(e||n.ee).get("fetch")}const S={},_="setTimeout",I="setInterval",P="clearTimeout",j="-start",R="-",D=[_,"setImmediate",I,P,"clearImmediate"];function C(e){const t=k(e);if(S[t.debugId]++)return t;S[t.debugId]=1;var r=d(t);return r.inPlace(a._A,D.slice(0,2),_+R),r.inPlace(a._A,D.slice(2,3),I+R),r.inPlace(a._A,D.slice(3),P+R),t.on(I+j,(function(e,t,n){e[0]=r(e[0],"fn-",null,n)})),t.on(_+j,(function(e,t,n){this.method=n,this.timerDuration=isNaN(e[1])?0:+e[1],e[0]=r(e[0],"fn-",this,n)})),t}function N(e){const t=k(e);1==S[t.debugId]?(D.forEach((e=>p(a._A,e))),S[t.debugId]=1/0):S[t.debugId]--}function k(e){return(e||n.ee).get("timer")}const L={},H="requestAnimationFrame";function M(e){const t=B(e);if(!a.il||L[t.debugId]++)return t;L[t.debugId]=1;var r=d(t);return r.inPlace(window,[H],"raf-"),t.on("raf-start",(function(e){e[0]=r(e[0],"fn-")})),t}function z(e){const t=B(e);1==L[t.debugId]?(p(window,H),L[t.debugId]=1/0):L[t.debugId]--}function B(e){return(e||n.ee).get("raf")}const U={},F=["pushState","replaceState"];function V(e){const t=q(e);return!a.il||U[t.debugId]++||(U[t.debugId]=1,d(t).inPlace(window.history,F,"-")),t}function G(e){const t=q(e);1==U[t.debugId]?(F.forEach((e=>p(window.history,e))),U[t.debugId]=1/0):U[t.debugId]--}function q(e){return(e||n.ee).get("history")}var W=r(8544);const X={},Z=["appendChild","insertBefore","replaceChild"];function $(e){const t=Q(e);if(!a.il||X[t.debugId])return t;X[t.debugId]=!0;var r=d(t),n=/[?&](?:callback|cb)=([^&#]+)/,i=/(.*)\.([^.]+)/,o=/^(\w+)(\.|$)(.*)$/;function s(e,t){var r=e.match(o),n=r[1],i=r[3];return i?s(i,t[n]):t[n]}return r.inPlace(Node.prototype,Z,"dom-"),t.on("dom-start",(function(e){!function(e){if(!e||"string"!=typeof e.nodeName||"script"!==e.nodeName.toLowerCase())return;if("function"!=typeof e.addEventListener)return;var o=(a=e.src,c=a.match(n),c?c[1]:null);var a,c;if(!o)return;var u=function(e){var t=e.match(i);if(t&&t.length>=3)return{key:t[2],parent:s(t[1],window)};return{key:e,parent:window}}(o);if("function"!=typeof u.parent[u.key])return;var d={};function f(){t.emit("jsonp-end",[],d),e.removeEventListener("load",f,(0,W.m$)(!1)),e.removeEventListener("error",l,(0,W.m$)(!1))}function l(){t.emit("jsonp-error",[],d),t.emit("jsonp-end",[],d),e.removeEventListener("load",f,(0,W.m$)(!1)),e.removeEventListener("error",l,(0,W.m$)(!1))}r.inPlace(u.parent,[u.key],"cb-",d),e.addEventListener("load",f,(0,W.m$)(!1)),e.addEventListener("error",l,(0,W.m$)(!1)),t.emit("new-jsonp",[e.src],d)}(e[0])})),t}function Y(e){const t=Q(e);!0===X[t.debugId]&&(Z.forEach((e=>p(Node.prototype,e))),X[t.debugId]="unwrapped")}function Q(e){return(e||n.ee).get("jsonp")}var K=r(6562);const J={};function ee(e){const t=re(e);if(!a.il||J[t.debugId])return t;J[t.debugId]=!0;var r=d(t),n=K.Yu.MO;return n&&(window.MutationObserver=function(e){return this instanceof n?new n(r(e,"fn-")):n.apply(this,arguments)},MutationObserver.prototype=n.prototype),t}function te(e){const t=re(e);!0===J[t.debugId]&&(window.MutationObserver=K.Yu.MO,J[t.debugId]="unwrapped")}function re(e){return(e||n.ee).get("mutation")}const ne={};function ie(e){const t=ae(e);if(ne[t.debugId])return t;ne[t.debugId]=!0;var r=n.c,i=d(t),o=K.Yu.PR;return o&&function(){function e(r){var n=t.context(),a=i(r,"executor-",n,null,!1);const s=Reflect.construct(o,[a],e);return t.context(s).getCtx=function(){return n},s}a._A.Promise=e,Object.defineProperty(e,"name",{value:"Promise"}),e.toString=function(){return o.toString()},Object.setPrototypeOf(e,o),["all","race"].forEach((function(r){const n=o[r];e[r]=function(e){let i=!1;e?.forEach((e=>{this.resolve(e).then(a("all"===r),a(!1))}));const o=n.apply(this,arguments);return o;function a(e){return function(){t.emit("propagate",[null,!i],o,!1,!1),i=i||!e}}}})),["resolve","reject"].forEach((function(r){const n=o[r];e[r]=function(e){const r=n.apply(this,arguments);return e!==r&&t.emit("propagate",[e,!0],r,!1,!1),r}})),e.prototype=Object.create(o.prototype),e.prototype.constructor=e,e.prototype.then=function(){var e=this,n=r(e);n.promise=e;for(var a=arguments.length,s=new Array(a),c=0;c<a;c++)s[c]=arguments[c];s[0]=i(s[0],"cb-",n,null,!1),s[1]=i(s[1],"cb-",n,null,!1);const u=o.prototype.then.apply(this,s);return n.nextPromise=u,t.emit("propagate",[e,!0],u,!1,!1),u},t.on("executor-start",(function(e){e[0]=i(e[0],"resolve-",this,null,!1),e[1]=i(e[1],"resolve-",this,null,!1)})),t.on("executor-err",(function(e,t,r){e[1](r)})),t.on("cb-end",(function(e,r,n){t.emit("propagate",[n,!0],this.nextPromise,!1,!1)})),t.on("propagate",(function(e,r,n){this.getCtx&&!r||(this.getCtx=function(){if(e instanceof Promise)var r=t.context(e);return r&&r.getCtx?r.getCtx():this})}))}(),t}function oe(e){const t=ae(e);!0===ne[t.debugId]&&(a._A.Promise=K.Yu.PR,ne[t.debugId]="unwrapped")}function ae(e){return(e||n.ee).get("promise")}var se=r(3916);const ce={},ue=XMLHttpRequest,de="addEventListener",fe="removeEventListener";function le(e){var t=ge(e);if(ce[t.debugId]++)return t;ce[t.debugId]=1;var r=d(t,!0);function n(e){r.inPlace(e,[de,fe],"-",i)}function i(e,t){return e[1]}return"getPrototypeOf"in Object&&(a.il&&he(document,n),he(a._A,n),he(ue.prototype,n)),t.on(de+"-start",(function(e,t){var n=e[1];if(null!==n&&("function"==typeof n||"object"==typeof n)){var i=(0,se.X)(n,"nr@wrapped",(function(){var e={object:function(){if("function"!=typeof n.handleEvent)return;return n.handleEvent.apply(n,arguments)},function:n}[typeof n];return e?r(e,"fn-",null,e.name||"anonymous"):n}));this.wrapped=e[1]=i}})),t.on(fe+"-start",(function(e){e[1]=this.wrapped||e[1]})),t}function he(e,t){let r=e;for(;"object"==typeof r&&!Object.prototype.hasOwnProperty.call(r,de);)r=Object.getPrototypeOf(r);for(var n=arguments.length,i=new Array(n>2?n-2:0),o=2;o<n;o++)i[o-2]=arguments[o];r&&t(r,...i)}function pe(e){const t=ge(e);1==ce[t.debugId]?([de,fe].forEach((e=>{"object"==typeof document&&he(document,p,e),he(a._A,p,e),he(ue.prototype,p,e)})),ce[t.debugId]=1/0):ce[t.debugId]--}function ge(e){return(e||n.ee).get("events")}var ve=r(8610);const me={},be=["open","send"];function ye(e){var t=e||n.ee;const r=Ee(t);if(me[r.debugId]++)return r;me[r.debugId]=1,le(t);var i=d(r),o=K.Yu.XHR,s=K.Yu.MO,c=K.Yu.PR,u=K.Yu.SI,f="readystatechange",l=["onload","onerror","onabort","onloadstart","onloadend","onprogress","ontimeout"],h=[],p=a._A.XMLHttpRequest.listeners,g=a._A.XMLHttpRequest=function(e){var t=new o(e);function n(){try{r.emit("new-xhr",[t],t),t.addEventListener(f,m,(0,W.m$)(!1))}catch(e){(0,ve.Z)("An error occured while intercepting XHR",e);try{r.emit("internal-error",[e])}catch(e){}}}return this.listeners=p?[...p,n]:[n],this.listeners.forEach((e=>e())),t};function v(e,t){i.inPlace(t,["onreadystatechange"],"fn-",A)}function m(){var e=this,t=r.context(e);e.readyState>3&&!t.resolved&&(t.resolved=!0,r.emit("xhr-resolved",[],e)),i.inPlace(e,l,"fn-",A)}if(function(e,t){for(var r in e)t[r]=e[r]}(o,g),g.prototype=o.prototype,i.inPlace(g.prototype,be,"-xhr-",A),r.on("send-xhr-start",(function(e,t){v(e,t),function(e){h.push(e),s&&(b?b.then(E):u?u(E):(y=-y,w.data=y))}(t)})),r.on("open-xhr-start",v),s){var b=c&&c.resolve();if(!u&&!c){var y=1,w=document.createTextNode(y);new s(E).observe(w,{characterData:!0})}}else t.on("fn-end",(function(e){e[0]&&e[0].type===f||E()}));function E(){for(var e=0;e<h.length;e++)v(0,h[e]);h.length&&(h=[])}function A(e,t){return t}return r}function we(e){const t=Ee(e);1==me[t.debugId]?(pe(t),a._A.XMLHttpRequest=K.Yu.XHR,be.forEach((e=>{p(a._A.XMLHttpRequest.prototype,e)})),me[t.debugId]=1/0):me[t.debugId]--}function Ee(e){return(e||n.ee).get("xhr")}},8675:(e,t,r)=>{"use strict";r.d(t,{t:()=>n});const n=r(2325).D.ajax},948:(e,t,r)=>{"use strict";r.r(t),r.d(t,{Instrument:()=>j});var n=r(6562),i=r(9252),o=r(3916),a=r(2374),s=1,c="nr@id";function u(e){var t=typeof e;return!e||"object"!==t&&"function"!==t?-1:e===a._A?0:(0,o.X)(e,c,(function(){return s++}))}var d=r(9071);function f(e){if("string"==typeof e&&e.length)return e.length;if("object"==typeof e){if("undefined"!=typeof ArrayBuffer&&e instanceof ArrayBuffer&&e.byteLength)return e.byteLength;if("undefined"!=typeof Blob&&e instanceof Blob&&e.size)return e.size;if(!("undefined"!=typeof FormData&&e instanceof FormData))try{return JSON.stringify(e).length}catch(e){return}}}var l=r(8544),h=r(2053),p=r(7264),g=r(6368),v=r(5526);class m{constructor(e){this.agentIdentifier=e,this.generateTracePayload=this.generateTracePayload.bind(this),this.shouldGenerateTrace=this.shouldGenerateTrace.bind(this)}generateTracePayload(e){if(!this.shouldGenerateTrace(e))return null;var t=(0,n.DL)(this.agentIdentifier);if(!t)return null;var r=(t.accountID||"").toString()||null,i=(t.agentID||"").toString()||null,o=(t.trustKey||"").toString()||null;if(!r||!i)return null;var a=(0,v.M)(),s=(0,v.Ht)(),c=Date.now(),u={spanId:a,traceId:s,timestamp:c};return(e.sameOrigin||this.isAllowedOrigin(e)&&this.useTraceContextHeadersForCors())&&(u.traceContextParentHeader=this.generateTraceContextParentHeader(a,s),u.traceContextStateHeader=this.generateTraceContextStateHeader(a,c,r,i,o)),(e.sameOrigin&&!this.excludeNewrelicHeader()||!e.sameOrigin&&this.isAllowedOrigin(e)&&this.useNewrelicHeaderForCors())&&(u.newrelicHeader=this.generateTraceHeader(a,s,c,r,i,o)),u}generateTraceContextParentHeader(e,t){return"00-"+t+"-"+e+"-01"}generateTraceContextStateHeader(e,t,r,n,i){return i+"@nr=0-1-"+r+"-"+n+"-"+e+"----"+t}generateTraceHeader(e,t,r,n,i,o){if(!("function"==typeof a._A?.btoa))return null;var s={v:[0,1],d:{ty:"Browser",ac:n,ap:i,id:e,tr:t,ti:r}};return o&&n!==o&&(s.d.tk=o),btoa(JSON.stringify(s))}shouldGenerateTrace(e){return this.isDtEnabled()&&this.isAllowedOrigin(e)}isAllowedOrigin(e){var t=!1,r={};if((0,n.Mt)(this.agentIdentifier,"distributed_tracing")&&(r=(0,n.P_)(this.agentIdentifier).distributed_tracing),e.sameOrigin)t=!0;else if(r.allowed_origins instanceof Array)for(var i=0;i<r.allowed_origins.length;i++){var o=(0,g.e)(r.allowed_origins[i]);if(e.hostname===o.hostname&&e.protocol===o.protocol&&e.port===o.port){t=!0;break}}return t}isDtEnabled(){var e=(0,n.Mt)(this.agentIdentifier,"distributed_tracing");return!!e&&!!e.enabled}excludeNewrelicHeader(){var e=(0,n.Mt)(this.agentIdentifier,"distributed_tracing");return!!e&&!!e.exclude_newrelic_header}useNewrelicHeaderForCors(){var e=(0,n.Mt)(this.agentIdentifier,"distributed_tracing");return!!e&&!1!==e.cors_use_newrelic_header}useTraceContextHeadersForCors(){var e=(0,n.Mt)(this.agentIdentifier,"distributed_tracing");return!!e&&!!e.cors_use_tracecontext_headers}}var b=r(6114),y=r(8675),w=r(2325);function E(e,t){!function(e,t){if(t.has(e))throw new TypeError("Cannot initialize the same private elements twice on an object")}(e,t),t.add(e)}var A,x,O,T=["load","error","abort","timeout"],S=T.length,_=n.Yu.REQ,I=a._A.XMLHttpRequest,P=new WeakSet;class j extends b.S{constructor(e,t){let r=!(arguments.length>2&&void 0!==arguments[2])||arguments[2];super(e,t,y.t,r),E(this,P),(0,n.OP)(e).xhrWrappable&&(this.dt=new m(e),this.handler=(e,t,r,n)=>(0,i.p)(e,t,r,n,this.ee),(0,p.u5)(this.ee),(0,p.Kf)(this.ee),function(e,t,r,i){function o(e){var t=this;t.totalCbs=0,t.called=0,t.cbTime=0,t.end=D,t.ended=!1,t.xhrGuids={},t.lastSize=null,t.loadCaptureCalled=!1,t.params=this.params||{},t.metrics=this.metrics||{},e.addEventListener("load",(function(r){N(t,e)}),(0,l.m$)(!1)),d.I||e.addEventListener("progress",(function(e){t.lastSize=e.loaded}),(0,l.m$)(!1))}function s(e){this.params={method:e[0]},C(this,e[1]),this.metrics={}}function c(t,r){var o=(0,n.DL)(e);"xpid"in o&&this.sameOrigin&&r.setRequestHeader("X-NewRelic-ID",o.xpid);var a=i.generateTracePayload(this.parsedOrigin);if(a){var s=!1;a.newrelicHeader&&(r.setRequestHeader("newrelic",a.newrelicHeader),s=!0),a.traceContextParentHeader&&(r.setRequestHeader("traceparent",a.traceContextParentHeader),a.traceContextStateHeader&&r.setRequestHeader("tracestate",a.traceContextStateHeader),s=!0),s&&(this.dt=a)}}function p(e,r){var n=this.metrics,i=e[0],o=this;if(n&&i){var a=f(i);a&&(n.txSize=a)}this.startTime=(0,h.zO)(),this.listener=function(e){try{"abort"!==e.type||o.loadCaptureCalled||(o.params.aborted=!0),("load"!==e.type||o.called===o.totalCbs&&(o.onloadCalled||"function"!=typeof r.onload)&&"function"==typeof o.end)&&o.end(r)}catch(e){try{t.emit("internal-error",[e])}catch(e){}}};for(var s=0;s<S;s++)r.addEventListener(T[s],this.listener,(0,l.m$)(!1))}function v(e,t,r){this.cbTime+=e,t?this.onloadCalled=!0:this.called+=1,this.called!==this.totalCbs||!this.onloadCalled&&"function"==typeof r.onload||"function"!=typeof this.end||this.end(r)}function m(e,t){var r=""+u(e)+!!t;this.xhrGuids&&!this.xhrGuids[r]&&(this.xhrGuids[r]=!0,this.totalCbs+=1)}function b(e,t){var r=""+u(e)+!!t;this.xhrGuids&&this.xhrGuids[r]&&(delete this.xhrGuids[r],this.totalCbs-=1)}function y(){this.endTime=(0,h.zO)()}function E(e,r){r instanceof I&&"load"===e[0]&&t.emit("xhr-load-added",[e[1],e[2]],r)}function A(e,r){r instanceof I&&"load"===e[0]&&t.emit("xhr-load-removed",[e[1],e[2]],r)}function x(e,t,r){t instanceof I&&("onload"===r&&(this.onload=!0),("load"===(e[0]&&e[0].type)||this.onload)&&(this.xhrCbStart=(0,h.zO)()))}function O(e,r){this.xhrCbStart&&t.emit("xhr-cb-time",[(0,h.zO)()-this.xhrCbStart,this.onload,r],r)}function P(e){var t,r=e[1]||{};"string"==typeof e[0]?t=e[0]:e[0]&&e[0].url?t=e[0].url:a._A?.URL&&e[0]&&e[0]instanceof URL&&(t=e[0].href),t&&(this.parsedOrigin=(0,g.e)(t),this.sameOrigin=this.parsedOrigin.sameOrigin);var n=i.generateTracePayload(this.parsedOrigin);if(n&&(n.newrelicHeader||n.traceContextParentHeader))if("string"==typeof e[0]||a._A?.URL&&e[0]&&e[0]instanceof URL){var o={};for(var s in r)o[s]=r[s];o.headers=new Headers(r.headers||{}),c(o.headers,n)&&(this.dt=n),e.length>1?e[1]=o:e.push(o)}else e[0]&&e[0].headers&&c(e[0].headers,n)&&(this.dt=n);function c(e,t){var r=!1;return t.newrelicHeader&&(e.set("newrelic",t.newrelicHeader),r=!0),t.traceContextParentHeader&&(e.set("traceparent",t.traceContextParentHeader),t.traceContextStateHeader&&e.set("tracestate",t.traceContextStateHeader),r=!0),r}}function j(e,t){this.params={},this.metrics={},this.startTime=(0,h.zO)(),this.dt=t,e.length>=1&&(this.target=e[0]),e.length>=2&&(this.opts=e[1]);var r,n=this.opts||{},i=this.target;"string"==typeof i?r=i:"object"==typeof i&&i instanceof _?r=i.url:a._A?.URL&&"object"==typeof i&&i instanceof URL&&(r=i.href),C(this,r);var o=(""+(i&&i instanceof _&&i.method||n.method||"GET")).toUpperCase();this.params.method=o,this.txSize=f(n.body)||0}function R(e,t){var n;this.endTime=(0,h.zO)(),this.params||(this.params={}),this.params.status=t?t.status:0,"string"==typeof this.rxSize&&this.rxSize.length>0&&(n=+this.rxSize);var i={txSize:this.txSize,rxSize:n,duration:(0,h.zO)()-this.startTime};r("xhr",[this.params,i,this.startTime,this.endTime,"fetch"],this,w.D.ajax)}function D(e){var t=this.params,n=this.metrics;if(!this.ended){this.ended=!0;for(var i=0;i<S;i++)e.removeEventListener(T[i],this.listener,!1);t.aborted||(n.duration=(0,h.zO)()-this.startTime,this.loadCaptureCalled||4!==e.readyState?null==t.status&&(t.status=0):N(this,e),n.cbTime=this.cbTime,r("xhr",[t,n,this.startTime,this.endTime,"xhr"],this,w.D.ajax))}}function C(e,t){var r=(0,g.e)(t),n=e.params;n.hostname=r.hostname,n.port=r.port,n.protocol=r.protocol,n.host=r.hostname+":"+r.port,n.pathname=r.pathname,e.parsedOrigin=r,e.sameOrigin=r.sameOrigin}function N(e,t){e.params.status=t.status;var r=function(e,t){var r=e.responseType;return"json"===r&&null!==t?t:"arraybuffer"===r||"blob"===r||"json"===r?f(e.response):"text"===r||""===r||void 0===r?f(e.responseText):void 0}(t,e.lastSize);if(r&&(e.metrics.rxSize=r),e.sameOrigin){var n=t.getResponseHeader("X-NewRelic-App-Data");n&&(e.params.cat=n.split(", ").pop())}e.loadCaptureCalled=!0}t.on("new-xhr",o),t.on("open-xhr-start",s),t.on("open-xhr-end",c),t.on("send-xhr-start",p),t.on("xhr-cb-time",v),t.on("xhr-load-added",m),t.on("xhr-load-removed",b),t.on("xhr-resolved",y),t.on("addEventListener-end",E),t.on("removeEventListener-end",A),t.on("fn-end",O),t.on("fetch-before-start",P),t.on("fetch-start",j),t.on("fn-start",x),t.on("fetch-done",R)}(e,this.ee,this.handler,this.dt),this.abortHandler=function(e,t,r){if(!t.has(e))throw new TypeError("attempted to get private field on non-instance");return r}(this,P,R),this.importAggregator())}}function R(){(0,p.Fp)(this.ee),(0,p.ig)(this.ee),this.abortHandler=void 0}A=j,x="featureName",O=y.t,(x=function(e){var t=function(e,t){if("object"!=typeof e||null===e)return e;var r=e[Symbol.toPrimitive];if(void 0!==r){var n=r.call(e,t||"default");if("object"!=typeof n)return n;throw new TypeError("@@toPrimitive must return a primitive value.")}return("string"===t?String:Number)(e)}(e,"string");return"symbol"==typeof t?t:String(t)}(x))in A?Object.defineProperty(A,x,{value:O,enumerable:!0,configurable:!0,writable:!0}):A[x]=O},8322:(e,t,r)=>{"use strict";r.d(t,{A:()=>i,t:()=>n});const n=r(2325).D.jserrors,i="nr@seenError"},9715:(e,t,r)=>{"use strict";r.r(t),r.d(t,{Instrument:()=>T});var n,i=r(9252),o=r(2053),a=r(3916),s=r(7264),c=r(8683),u=r.n(c),d=r(3752),f=r(7022),l={};try{n=localStorage.getItem("__nr_flags").split(","),console&&"function"==typeof console.log&&(l.console=!0,-1!==n.indexOf("dev")&&(l.dev=!0),-1!==n.indexOf("nr_dev")&&(l.nrDev=!0))}catch(e){}function h(e){try{l.console&&h(e)}catch(e){}}l.nrDev&&d.ee.on("internal-error",(function(e){h(e.stack)})),l.dev&&d.ee.on("fn-err",(function(e,t,r){h(r.stack)})),l.dev&&(h("NR AGENT IN DEVELOPMENT MODE"),h("flags: "+(0,f.D)(l,(function(e,t){return e})).join(", ")));var p=r(6114),g=r(8322),v=r(2325),m=r(2374),b=r(8544),y=r(6562);function w(e,t){!function(e,t){if(t.has(e))throw new TypeError("Cannot initialize the same private elements twice on an object")}(e,t),t.add(e)}var E,A,x,O=new WeakSet;class T extends p.S{constructor(e,t){var r;let n=!(arguments.length>2&&void 0!==arguments[2])||arguments[2];super(e,t,g.t,n),r=this,w(this,O),this.skipNext=0,this.origOnerror=m._A.onerror;try{this.removeOnAbort=new AbortController}catch(e){}const c=this;c.ee.on("fn-start",(function(e,t,r){c.abortHandler&&(c.skipNext+=1)})),c.ee.on("fn-err",(function(e,t,r){c.abortHandler&&!r[g.A]&&((0,a.X)(r,g.A,(function(){return!0})),this.thrown=!0,I(r,void 0,c.ee))})),c.ee.on("fn-end",(function(){c.abortHandler&&!this.thrown&&c.skipNext>0&&(c.skipNext-=1)})),c.ee.on("internal-error",(function(e){(0,i.p)("ierr",[e,(0,o.zO)(),!0],void 0,v.D.jserrors,c.ee)})),m._A.onerror=function(){return r.origOnerror&&r.origOnerror(...arguments),r.onerrorHandler(...arguments),!1},m._A.addEventListener("unhandledrejection",(e=>{const t=function(e){let t="Unhandled Promise Rejection: ";if(e instanceof Error)return e.message=t+e.message,e;if(void 0===e)return new Error(t);try{return new Error(t+JSON.stringify(e))}catch(e){return new Error(t)}}(e.reason);(0,i.p)("err",[t,(0,o.zO)(),!1,{unhandledPromiseRejection:1}],void 0,v.D.jserrors,this.ee)}),(0,b.m$)(!1,this.removeOnAbort?.signal)),(0,s.gy)(this.ee),(0,s.BV)(this.ee),(0,s.em)(this.ee),(0,y.OP)(e).xhrWrappable&&(0,s.Kf)(this.ee),this.abortHandler=function(e,t,r){if(!t.has(e))throw new TypeError("attempted to get private field on non-instance");return r}(this,O,S),this.importAggregator()}onerrorHandler(e,t,r,n,a){try{this.skipNext?this.skipNext-=1:I(a||new _(e,t,r),!0,this.ee)}catch(e){try{(0,i.p)("ierr",[e,(0,o.zO)(),!0],void 0,v.D.jserrors,this.ee)}catch(e){}}return"function"==typeof this.origOnerror&&this.origOnerror.apply(this,u()(arguments))}}function S(){m._A.onerror=this.origOnerror,this.removeOnAbort?.abort(),(0,s.XV)(this.ee),(0,s.Zw)(this.ee),(0,s.hF)(this.ee),(0,y.OP)(this.agentIdentifier).xhrWrappable&&(0,s.ig)(this.ee),this.abortHandler=void 0}function _(e,t,r){this.message=e||"Uncaught error with no additional information",this.sourceURL=t,this.line=r}function I(e,t,r){var n=t?null:(0,o.zO)();(0,i.p)("err",[e,n],void 0,v.D.jserrors,r)}E=T,A="featureName",x=g.t,(A=function(e){var t=function(e,t){if("object"!=typeof e||null===e)return e;var r=e[Symbol.toPrimitive];if(void 0!==r){var n=r.call(e,t||"default");if("object"!=typeof n)return n;throw new TypeError("@@toPrimitive must return a primitive value.")}return("string"===t?String:Number)(e)}(e,"string");return"symbol"==typeof t?t:String(t)}(A))in E?Object.defineProperty(E,A,{value:x,enumerable:!0,configurable:!0,writable:!0}):E[A]=x},6034:(e,t,r)=>{"use strict";r.d(t,{t:()=>n});const n=r(2325).D.metrics},600:(e,t,r)=>{"use strict";r.r(t),r.d(t,{Instrument:()=>O,constants:()=>T});var n=r(9252),i=r(4045),o=r(6114),a=r(2374),s={REACT:"React",ANGULAR:"Angular",ANGULARJS:"AngularJS",BACKBONE:"Backbone",EMBER:"Ember",VUE:"Vue",METEOR:"Meteor",ZEPTO:"Zepto",JQUERY:"Jquery"};function c(){if(!a.il)return[];var e=[];try{(function(){try{if(window.React||window.ReactDOM||window.ReactRedux)return!0;if(document.querySelector("[data-reactroot], [data-reactid]"))return!0;for(var e=document.querySelectorAll("body > div"),t=0;t<e.length;t++)if(Object.keys(e[t]).indexOf("_reactRootContainer")>=0)return!0;return!1}catch(e){return!1}})()&&e.push(s.REACT),function(){try{return!!window.angular||(!!document.querySelector(".ng-binding, [ng-app], [data-ng-app], [ng-controller], [data-ng-controller], [ng-repeat], [data-ng-repeat]")||!!document.querySelector('script[src*="angular.js"], script[src*="angular.min.js"]'))}catch(e){return!1}}()&&e.push(s.ANGULARJS),function(){try{return!!(window.hasOwnProperty("ng")&&window.ng.hasOwnProperty("coreTokens")&&window.ng.coreTokens.hasOwnProperty("NgZone"))||!!document.querySelectorAll("[ng-version]").length}catch(e){return!1}}()&&e.push(s.ANGULAR),window.Backbone&&e.push(s.BACKBONE),window.Ember&&e.push(s.EMBER),window.Vue&&e.push(s.VUE),window.Meteor&&e.push(s.METEOR),window.Zepto&&e.push(s.ZEPTO),window.jQuery&&e.push(s.JQUERY)}catch(e){}return e}var u=r(9548),d=r(9226),f=r(8226),l=r(2650),h=r(8544),p=r(6562),g=r(8610);const v={dedicated:Boolean(a._A?.Worker),shared:Boolean(a._A?.SharedWorker),service:Boolean(a._A?.navigator?.serviceWorker)};let m,b,y;var w=r(6034);var E,A,x;class O extends o.S{constructor(e,t){var r;let n=!(arguments.length>2&&void 0!==arguments[2])||arguments[2];super(e,t,w.t,n),r=this,this.singleChecks(),this.eachSessionChecks(),(0,i.X)("record-supportability",(function(){return r.recordSupportability(...arguments)}),this.featureName,this.ee),(0,i.X)("record-custom",(function(){return r.recordCustom(...arguments)}),this.featureName,this.ee),this.importAggregator()}recordSupportability(e,t){var r=["sm",e,{name:e},t];return(0,n.p)("storeMetric",r,null,this.featureName,this.ee),r}recordCustom(e,t){var r=["cm",e,{name:e},t];return(0,n.p)("storeEventMetrics",r,null,this.featureName,this.ee),r}singleChecks(){this.recordSupportability("Generic/Version/".concat(f.q,"/Detected"));const{loaderType:e}=(0,p.OP)(this.agentIdentifier);e&&this.recordSupportability("Generic/LoaderType/".concat(e,"/Detected")),a.il&&(0,l.K)((()=>{c().forEach((e=>{this.recordSupportability("Framework/"+e+"/Detected")}))})),u.T.isFileProtocol()&&(this.recordSupportability("Generic/FileProtocol/Detected"),u.T.supportabilityMetricSent=!0);const t=(0,d.$c)(this.agentIdentifier);t.length>0&&this.recordSupportability("Generic/Obfuscate/Detected"),t.length>0&&!(0,d.Ng)(t)&&this.recordSupportability("Generic/Obfuscate/Invalid"),function(e){if(!m){if(v.dedicated){m=Worker;try{a._A.Worker=r(m,"Dedicated")}catch(e){o(e,"Dedicated")}if(v.shared){b=SharedWorker;try{a._A.SharedWorker=r(b,"Shared")}catch(e){o(e,"Shared")}}else n("Shared");if(v.service){y=navigator.serviceWorker.register;try{a._A.navigator.serviceWorker.register=(t=y,function(){for(var e=arguments.length,r=new Array(e),n=0;n<e;n++)r[n]=arguments[n];return i("Service",r[1]?.type),t.apply(navigator.serviceWorker,r)})}catch(e){o(e,"Service")}}else n("Service");var t;return}n("All")}function r(e,t){return"undefined"==typeof Proxy?e:new Proxy(e,{construct:(e,r)=>(i(t,r[1]?.type),new e(...r))})}function n(t){a.v6||e("Workers/".concat(t,"/Unavailable"))}function i(t,r){e("Workers/".concat(t,"module"===r?"/Module":"/Classic"))}function o(t,r){e("Workers/".concat(r,"/SM/Unsupported")),(0,g.Z)("NR Agent: Unable to capture ".concat(r," workers."),t)}}(this.recordSupportability.bind(this))}eachSessionChecks(){a.il&&(0,h.bP)("pageshow",(e=>{e.persisted&&this.recordSupportability("Generic/BFCache/PageRestored")}))}}E=O,A="featureName",x=w.t,(A=function(e){var t=function(e,t){if("object"!=typeof e||null===e)return e;var r=e[Symbol.toPrimitive];if(void 0!==r){var n=r.call(e,t||"default");if("object"!=typeof n)return n;throw new TypeError("@@toPrimitive must return a primitive value.")}return("string"===t?String:Number)(e)}(e,"string");return"symbol"==typeof t?t:String(t)}(A))in E?Object.defineProperty(E,A,{value:x,enumerable:!0,configurable:!0,writable:!0}):E[A]=x;var T={SUPPORTABILITY_METRIC:"sm",CUSTOM_METRIC:"cm"}},6486:(e,t,r)=>{"use strict";r.d(t,{t:()=>n});const n=r(2325).D.pageAction},488:(e,t,r)=>{"use strict";r.r(t),r.d(t,{Instrument:()=>c});var n,i,o,a=r(6114),s=r(6486);class c extends a.S{constructor(e,t){let r=!(arguments.length>2&&void 0!==arguments[2])||arguments[2];super(e,t,s.t,r),this.importAggregator()}}n=c,i="featureName",o=s.t,(i=function(e){var t=function(e,t){if("object"!=typeof e||null===e)return e;var r=e[Symbol.toPrimitive];if(void 0!==r){var n=r.call(e,t||"default");if("object"!=typeof n)return n;throw new TypeError("@@toPrimitive must return a primitive value.")}return("string"===t?String:Number)(e)}(e,"string");return"symbol"==typeof t?t:String(t)}(i))in n?Object.defineProperty(n,i,{value:o,enumerable:!0,configurable:!0,writable:!0}):n[i]=o},2484:(e,t,r)=>{"use strict";r.d(t,{t:()=>n});const n=r(2325).D.pageViewEvent},5637:(e,t,r)=>{"use strict";r.r(t),r.d(t,{Instrument:()=>g});var n,i,o,a=r(9252),s=r(2053),c=r(8283),u=r(6625),d=r(6114),f=r(2650),l=r(2484),h=r(2325),p=r(2374);class g extends d.S{constructor(e,t){let r=!(arguments.length>2&&void 0!==arguments[2])||arguments[2];super(e,t,l.t,r),p.il&&((0,u.v)(e),(0,c.B)(e,"firstbyte",(0,s.yf)()),(0,f.K)((()=>this.measureDomContentLoaded())),(0,f.b)((()=>this.measureWindowLoaded()),!0),this.importAggregator())}measureWindowLoaded(){var e=(0,s.zO)();(0,c.B)(this.agentIdentifier,"onload",e+(0,s.os)()),(0,a.p)("timing",["load",e],void 0,h.D.pageViewTiming,this.ee)}measureDomContentLoaded(){(0,c.B)(this.agentIdentifier,"domContent",(0,s.zO)()+(0,s.os)())}}n=g,i="featureName",o=l.t,(i=function(e){var t=function(e,t){if("object"!=typeof e||null===e)return e;var r=e[Symbol.toPrimitive];if(void 0!==r){var n=r.call(e,t||"default");if("object"!=typeof n)return n;throw new TypeError("@@toPrimitive must return a primitive value.")}return("string"===t?String:Number)(e)}(e,"string");return"symbol"==typeof t?t:String(t)}(i))in n?Object.defineProperty(n,i,{value:o,enumerable:!0,configurable:!0,writable:!0}):n[i]=o},6382:(e,t,r)=>{"use strict";r.d(t,{t:()=>n});const n=r(2325).D.pageViewTiming},7817:(e,t,r)=>{"use strict";r.r(t),r.d(t,{Instrument:()=>g});var n,i,o,a=r(9252),s=r(6998),c=r(8544),u=r(2053),d=r(6562),f=r(6114),l=r(6382),h=r(2325),p=r(2374);class g extends f.S{constructor(e,t){var r;let n=!(arguments.length>2&&void 0!==arguments[2])||arguments[2];if(super(e,t,l.t,n),r=this,p.il){if(this.pageHiddenTime=(0,s.e)(),this.performanceObserver,this.lcpPerformanceObserver,this.clsPerformanceObserver,this.fiRecorded=!1,"PerformanceObserver"in window&&"function"==typeof window.PerformanceObserver){this.performanceObserver=new PerformanceObserver((function(){return r.perfObserver(...arguments)}));try{this.performanceObserver.observe({entryTypes:["paint"]})}catch(e){}this.lcpPerformanceObserver=new PerformanceObserver((function(){return r.lcpObserver(...arguments)}));try{this.lcpPerformanceObserver.observe({entryTypes:["largest-contentful-paint"]})}catch(e){}this.clsPerformanceObserver=new PerformanceObserver((function(){return r.clsObserver(...arguments)}));try{this.clsPerformanceObserver.observe({type:"layout-shift",buffered:!0})}catch(e){}}this.fiRecorded=!1;["click","keydown","mousedown","pointerdown","touchstart"].forEach((e=>{(0,c.iz)(e,(function(){return r.captureInteraction(...arguments)}))})),(0,s.N)((()=>{this.pageHiddenTime=(0,u.zO)(),(0,a.p)("docHidden",[this.pageHiddenTime],void 0,h.D.pageViewTiming,this.ee)}),!0),(0,c.bP)("pagehide",(()=>(0,a.p)("winPagehide",[(0,u.zO)()],void 0,h.D.pageViewTiming,this.ee))),this.importAggregator()}}perfObserver(e,t){e.getEntries().forEach((e=>{"first-paint"===e.name?(0,a.p)("timing",["fp",Math.floor(e.startTime)],void 0,h.D.pageViewTiming,this.ee):"first-contentful-paint"===e.name&&(0,a.p)("timing",["fcp",Math.floor(e.startTime)],void 0,h.D.pageViewTiming,this.ee)}))}lcpObserver(e,t){var r=e.getEntries();if(r.length>0){var n=r[r.length-1];if(this.pageHiddenTime<n.startTime)return;var i=[n],o=this.addConnectionAttributes({});o&&i.push(o),(0,a.p)("lcp",i,void 0,h.D.pageViewTiming,this.ee)}}clsObserver(e){e.getEntries().forEach((e=>{e.hadRecentInput||(0,a.p)("cls",[e],void 0,h.D.pageViewTiming,this.ee)}))}addConnectionAttributes(e){var t=navigator.connection||navigator.mozConnection||navigator.webkitConnection;if(t)return t.type&&(e["net-type"]=t.type),t.effectiveType&&(e["net-etype"]=t.effectiveType),t.rtt&&(e["net-rtt"]=t.rtt),t.downlink&&(e["net-dlink"]=t.downlink),e}captureInteraction(e){if(e instanceof d.Yu.EV&&!this.fiRecorded){var t=Math.round(e.timeStamp),r={type:e.type};this.addConnectionAttributes(r),t<=(0,u.zO)()?r.fid=(0,u.zO)()-t:t>(0,u.os)()&&t<=Date.now()?(t-=(0,u.os)(),r.fid=(0,u.zO)()-t):t=(0,u.zO)(),this.fiRecorded=!0,(0,a.p)("timing",["fi",t,r],void 0,h.D.pageViewTiming,this.ee)}}}n=g,i="featureName",o=l.t,(i=function(e){var t=function(e,t){if("object"!=typeof e||null===e)return e;var r=e[Symbol.toPrimitive];if(void 0!==r){var n=r.call(e,t||"default");if("object"!=typeof n)return n;throw new TypeError("@@toPrimitive must return a primitive value.")}return("string"===t?String:Number)(e)}(e,"string");return"symbol"==typeof t?t:String(t)}(i))in n?Object.defineProperty(n,i,{value:o,enumerable:!0,configurable:!0,writable:!0}):n[i]=o},2628:(e,t,r)=>{"use strict";r.r(t),r.d(t,{ADD_EVENT_LISTENER:()=>g,BST_RESOURCE:()=>a,BST_TIMER:()=>l,END:()=>u,FEATURE_NAME:()=>i,FN_END:()=>f,FN_START:()=>d,ORIG_EVENT:()=>p,PUSH_STATE:()=>h,RESOURCE:()=>s,RESOURCE_TIMING_BUFFER_FULL:()=>o,START:()=>c});var n=r(6562);const i=r(2325).D.sessionTrace,o="resourcetimingbufferfull",a="bstResource",s="resource",c="-start",u="-end",d="fn"+c,f="fn"+u,l="bstTimer",h="pushState",p=n.Yu.EV,g="addEventListener"},6649:(e,t,r)=>{"use strict";r.r(t),r.d(t,{Instrument:()=>j});var n=r(9252),i=r(7264),o=r(6408),a=r(8544),s=r(2053),c=r(6114),u=r(2628),d=r(2325),f=r(2374);function l(e,t){!function(e,t){if(t.has(e))throw new TypeError("Cannot initialize the same private elements twice on an object")}(e,t),t.add(e)}const{BST_RESOURCE:h,BST_TIMER:p,END:g,FEATURE_NAME:v,FN_END:m,FN_START:b,ADD_EVENT_LISTENER:y,PUSH_STATE:w,RESOURCE:E,RESOURCE_TIMING_BUFFER_FULL:A,START:x,ORIG_EVENT:O}=u,T="clearResourceTimings";var S,_,I,P=new WeakSet;class j extends c.S{constructor(e,t){if(super(e,t,v,!(arguments.length>2&&void 0!==arguments[2])||arguments[2]),l(this,P),!f.il)return;const r=this.ee;this.timerEE=(0,i.BV)(r),this.rafEE=(0,i.gy)(r),(0,i.QU)(r),(0,i.em)(r),r.on(b,(function(e,t){e[0]instanceof O&&(this.bstStart=(0,s.zO)())})),r.on(m,(function(e,t){var i=e[0];i instanceof O&&(0,n.p)("bst",[i,t,this.bstStart,(0,s.zO)()],void 0,d.D.sessionTrace,r)})),this.timerEE.on(b,(function(e,t,r){this.bstStart=(0,s.zO)(),this.bstType=r})),this.timerEE.on(m,(function(e,t){(0,n.p)(p,[t,this.bstStart,(0,s.zO)(),this.bstType],void 0,d.D.sessionTrace,r)})),this.rafEE.on(b,(function(){this.bstStart=(0,s.zO)()})),this.rafEE.on(m,(function(e,t){(0,n.p)(p,[t,this.bstStart,(0,s.zO)(),"requestAnimationFrame"],void 0,d.D.sessionTrace,r)})),r.on(w+x,(function(e){this.time=(0,s.zO)(),this.startPath=location.pathname+location.hash})),r.on(w+g,(function(e){(0,n.p)("bstHist",[location.pathname+location.hash,this.startPath,this.time],void 0,d.D.sessionTrace,r)})),(0,o.W)()?((0,n.p)(h,[window.performance.getEntriesByType("resource")],void 0,d.D.sessionTrace,r),function(){var e=new PerformanceObserver(((e,t)=>{var i=e.getEntries();(0,n.p)(h,[i],void 0,d.D.sessionTrace,r)}));try{e.observe({entryTypes:["resource"]})}catch(e){}}()):window.performance[T]&&window.performance[y]&&window.performance.addEventListener(A,this.onResourceTimingBufferFull,(0,a.m$)(!1)),document.addEventListener("scroll",this.noOp,(0,a.m$)(!1)),document.addEventListener("keypress",this.noOp,(0,a.m$)(!1)),document.addEventListener("click",this.noOp,(0,a.m$)(!1)),this.abortHandler=function(e,t,r){if(!t.has(e))throw new TypeError("attempted to get private field on non-instance");return r}(this,P,R),this.importAggregator()}noOp(e){}onResourceTimingBufferFull(e){if((0,n.p)(h,[window.performance.getEntriesByType(E)],void 0,d.D.sessionTrace,this.ee),window.performance[T])try{window.performance.removeEventListener(A,this.onResourceTimingBufferFull,!1)}catch(e){}}}function R(){window.performance.removeEventListener(A,this.onResourceTimingBufferFull,!1),(0,i.Zw)(this.ee),(0,i.XV)(this.ee),(0,i.lx)(this.ee),(0,i.hF)(this.ee),this.abortHandler=void 0}S=j,I=v,(_=function(e){var t=function(e,t){if("object"!=typeof e||null===e)return e;var r=e[Symbol.toPrimitive];if(void 0!==r){var n=r.call(e,t||"default");if("object"!=typeof n)return n;throw new TypeError("@@toPrimitive must return a primitive value.")}return("string"===t?String:Number)(e)}(e,"string");return"symbol"==typeof t?t:String(t)}(_="featureName"))in S?Object.defineProperty(S,_,{value:I,enumerable:!0,configurable:!0,writable:!0}):S[_]=I},1509:(e,t,r)=>{"use strict";r.d(t,{W:()=>s});var n=r(6562),i=r(3752),o=r(2384),a=r(6797);class s{constructor(e,t,r){this.agentIdentifier=e,this.aggregator=t,this.ee=i.ee.get(e,(0,n.OP)(this.agentIdentifier).isolatedBacklog),this.featureName=r,this.blocked=!1,this.checkConfiguration()}checkConfiguration(){if(!(0,n.lF)(this.agentIdentifier)){let e={...(0,a.gG)().info?.jsAttributes};try{e={...e,...(0,n.C5)(this.agentIdentifier)?.jsAttributes}}catch(e){}(0,o.j)(this.agentIdentifier,{...(0,a.gG)(),info:{...(0,a.gG)().info,jsAttributes:e}})}}}},6114:(e,t,r)=>{"use strict";r.d(t,{S:()=>c});var n=r(4329),i=r(1509),o=r(2650),a=r(2374),s=r(8610);class c extends i.W{constructor(e,t,r){let i=!(arguments.length>3&&void 0!==arguments[3])||arguments[3];super(e,t,r),this.hasAggregator=!1,this.auto=i,this.abortHandler,i&&(0,n.R)(e,r)}importAggregator(){if(this.hasAggregator||!this.auto)return;this.hasAggregator=!0;const e=async()=>{try{const{lazyLoader:e}=await r.e(729).then(r.bind(r,8110)),{Aggregate:t}=await e(this.featureName,"aggregate");new t(this.agentIdentifier,this.aggregator)}catch(e){(0,s.Z)("Downloading ".concat(this.featureName," failed...")),this.abortHandler?.()}};a.v6?e():(0,o.b)((()=>e()),!0)}}},2384:(e,t,r)=>{"use strict";r.d(t,{j:()=>y});var n=r(8683),i=r.n(n),o=r(2325),a=r(6562),s=r(9252),c=r(7022),u=r(3752),d=r(2053),f=r(4329),l=r(2650),h=r(2374),p=r(8610);function g(e){["setErrorHandler","finished","addToTrace","inlineHit","addRelease","addPageAction","setCurrentRouteName","setPageViewName","setCustomAttribute","interaction","noticeError"].forEach((t=>{e[t]=function(){for(var r=arguments.length,n=new Array(r),i=0;i<r;i++)n[i]=arguments[i];return function(t){for(var r=arguments.length,n=new Array(r>1?r-1:0),i=1;i<r;i++)n[i-1]=arguments[i];Object.values(e.initializedAgents).forEach((e=>{e.exposed&&e.api[t]&&e.api[t](...n)}))}(t,...n)}}))}var v=r(6797);const m={stn:o.D.sessionTrace,err:o.D.jserrors,ins:o.D.pageAction,spa:o.D.spa};const b={};function y(e){let t=arguments.length>1&&void 0!==arguments[1]?arguments[1]:{},n=arguments.length>2?arguments[2]:void 0,y=arguments.length>3?arguments[3]:void 0,{init:w,info:E,loader_config:A,runtime:x={loaderType:n},exposed:O=!0}=t;const T=(0,v.gG)();let S={};return E||(w=T.init,E=T.info,A=T.loader_config,S=T),h.v6&&(E.jsAttributes={...E.jsAttributes,isWorker:!0}),(0,a.CX)(e,E),(0,a.Dg)(e,w||{}),(0,a.GE)(e,A||{}),(0,a.sU)(e,x),function(e,t,n){n||(0,f.R)(e,"api"),g(t);var v=u.ee.get(e),m=v.get("tracer"),b="api-",y=b+"ixn-";function w(){}(0,c.D)(["setErrorHandler","finished","addToTrace","inlineHit","addRelease"],(function(e,r){t[r]=A(b,r,!0,"api")})),t.addPageAction=A(b,"addPageAction",!0,o.D.pageAction),t.setCurrentRouteName=A(b,"routeName",!0,o.D.spa),t.setPageViewName=function(t,r){if("string"==typeof t)return"/"!==t.charAt(0)&&(t="/"+t),(0,a.OP)(e).customTransaction=(r||"http://custom.transaction")+t,A(b,"setPageViewName",!0,"api")()},t.setCustomAttribute=function(t,r){const n=(0,a.C5)(e);return(0,a.CX)(e,{...n,jsAttributes:{...n.jsAttributes,[t]:r}}),A(b,"setCustomAttribute",!0,"api")()},t.interaction=function(){return(new w).get()};var E=w.prototype={createTracer:function(e,t){var r={},n=this,i="function"==typeof t;return(0,s.p)(y+"tracer",[(0,d.zO)(),e,r],n,o.D.spa,v),function(){if(m.emit((i?"":"no-")+"fn-start",[(0,d.zO)(),n,i],r),i)try{return t.apply(this,arguments)}catch(e){throw m.emit("fn-err",[arguments,this,"string"==typeof e?new Error(e):e],r),e}finally{m.emit("fn-end",[(0,d.zO)()],r)}}}};function A(e,t,r,n){return function(){return(0,s.p)("record-supportability",["API/"+t+"/called"],void 0,o.D.metrics,v),(0,s.p)(e+t,[(0,d.zO)()].concat(i()(arguments)),r?null:this,n,v),r?void 0:this}}function x(){r.e(439).then(r.bind(r,5692)).then((t=>{let{setAPI:r}=t;r(e),(0,f.L)(e,"api")})).catch((()=>(0,p.Z)("Downloading runtime APIs failed...")))}(0,c.D)("actionText,setName,setAttribute,save,ignore,onEnd,getContext,end,get".split(","),(function(e,t){E[t]=A(y,t,void 0,o.D.spa)})),t.noticeError=function(e,t){"string"==typeof e&&(e=new Error(e)),(0,s.p)("record-supportability",["API/noticeError/called"],void 0,o.D.metrics,v),(0,s.p)("err",[e,(0,d.zO)(),!1,t],void 0,o.D.jserrors,v)},h.v6?x():(0,l.b)((()=>x()),!0)}(e,S,y),(0,v.Qy)(e,T,"api"),(0,v.Qy)(e,O,"exposed"),h.v6||((0,v.EZ)("activatedFeatures",b),(0,v.EZ)("setToken",(t=>function(e,t){var r=u.ee.get(t);e&&"object"==typeof e&&((0,c.D)(e,(function(e,t){if(!t)return(0,s.p)("block-"+e,[],void 0,m[e],r);t&&!b[e]&&((0,s.p)("feat-"+e,[],void 0,m[e],r),b[e]=!0)})),(0,f.L)(t,o.D.pageViewEvent))}(t,e)))),S}},909:(e,t,r)=>{"use strict";r.d(t,{Z:()=>i,q:()=>o});var n=r(2325);function i(e){switch(e){case n.D.ajax:return[n.D.jserrors];case n.D.sessionTrace:return[n.D.ajax,n.D.pageViewEvent];case n.D.pageViewTiming:return[n.D.pageViewEvent];default:return[]}}function o(e){return e===n.D.jserrors?[]:["auto"]}},2325:(e,t,r)=>{"use strict";r.d(t,{D:()=>n,p:()=>i});const n={ajax:"ajax",jserrors:"jserrors",metrics:"metrics",pageAction:"page_action",pageViewEvent:"page_view_event",pageViewTiming:"page_view_timing",sessionTrace:"session_trace",spa:"spa"},i={[n.pageViewEvent]:1,[n.pageViewTiming]:2,[n.metrics]:3,[n.jserrors]:4,[n.ajax]:5,[n.sessionTrace]:6,[n.pageAction]:7,[n.spa]:8}},8683:e=>{e.exports=function(e,t,r){t||(t=0),void 0===r&&(r=e?e.length:0);for(var n=-1,i=r-t||0,o=Array(i<0?0:i);++n<i;)o[n]=e[t+n];return o}}},n={};function i(e){var t=n[e];if(void 0!==t)return t.exports;var o=n[e]={exports:{}};return r[e](o,o.exports,i),o.exports}i.m=r,i.n=e=>{var t=e&&e.__esModule?()=>e.default:()=>e;return i.d(t,{a:t}),t},i.d=(e,t)=>{for(var r in t)i.o(t,r)&&!i.o(e,r)&&Object.defineProperty(e,r,{enumerable:!0,get:t[r]})},i.f={},i.e=e=>Promise.all(Object.keys(i.f).reduce(((t,r)=>(i.f[r](e,t),t)),[])),i.u=e=>(({78:"page_action-aggregate",147:"metrics-aggregate",193:"session_trace-aggregate",225:"ajax-instrument",317:"jserrors-aggregate",348:"page_view_timing-aggregate",439:"async-api",578:"jserrors-instrument",729:"lazy-loader",757:"session_trace-instrument",786:"page_view_event-aggregate",873:"spa-aggregate",876:"spa-instrument",898:"ajax-aggregate",908:"page_action-instrument"}[e]||e)+"."+{78:"92657d87",118:"d37755e4",147:"b4a54ed9",193:"94c80cda",225:"1e8aaf7f",264:"bcaf68fc",317:"ef250e1c",348:"e791ce32",439:"6bb277af",578:"1eaebfa5",729:"48127245",757:"64f1d623",786:"29613e65",873:"6bec5056",876:"f3685aa9",898:"fc672923",908:"64360627"}[e]+"-1225.min.js"),i.o=(e,t)=>Object.prototype.hasOwnProperty.call(e,t),e={},t="NRBA:",i.l=(r,n,o,a)=>{if(e[r])e[r].push(n);else{var s,c;if(void 0!==o)for(var u=document.getElementsByTagName("script"),d=0;d<u.length;d++){var f=u[d];if(f.getAttribute("src")==r||f.getAttribute("data-webpack")==t+o){s=f;break}}s||(c=!0,(s=document.createElement("script")).charset="utf-8",s.timeout=120,i.nc&&s.setAttribute("nonce",i.nc),s.setAttribute("data-webpack",t+o),s.src=r),e[r]=[n];var l=(t,n)=>{s.onerror=s.onload=null,clearTimeout(h);var i=e[r];if(delete e[r],s.parentNode&&s.parentNode.removeChild(s),i&&i.forEach((e=>e(n))),t)return t(n)},h=setTimeout(l.bind(null,void 0,{type:"timeout",target:s}),12e4);s.onerror=l.bind(null,s.onerror),s.onload=l.bind(null,s.onload),c&&document.head.appendChild(s)}},i.r=e=>{"undefined"!=typeof Symbol&&Symbol.toStringTag&&Object.defineProperty(e,Symbol.toStringTag,{value:"Module"}),Object.defineProperty(e,"__esModule",{value:!0})},i.p="https://js-agent.newrelic.com/",(()=>{var e={137:0,308:0,225:0,578:0,908:0,757:0,264:0};i.f.j=(t,r)=>{var n=i.o(e,t)?e[t]:void 0;if(0!==n)if(n)r.push(n[2]);else{var o=new Promise(((r,i)=>n=e[t]=[r,i]));r.push(n[2]=o);var a=i.p+i.u(t),s=new Error;i.l(a,(r=>{if(i.o(e,t)&&(0!==(n=e[t])&&(e[t]=void 0),n)){var o=r&&("load"===r.type?"missing":r.type),a=r&&r.target&&r.target.src;s.message="Loading chunk "+t+" failed.\n("+o+": "+a+")",s.name="ChunkLoadError",s.type=o,s.request=a,n[1](s)}}),"chunk-"+t,t)}};var t=(t,r)=>{var n,o,[a,s,c]=r,u=0;if(a.some((t=>0!==e[t]))){for(n in s)i.o(s,n)&&(i.m[n]=s[n]);if(c)c(i)}for(t&&t(r);u<a.length;u++)o=a[u],i.o(e,o)&&e[o]&&e[o][0](),e[o]=0},r=window.webpackChunkNRBA=window.webpackChunkNRBA||[];r.forEach(t.bind(null,0)),r.push=t.bind(null,r.push.bind(r))})();var o={};(()=>{"use strict";i.r(o);var e=i(2325),t=i(6562);const r=Object.values(e.D);function n(e){const n={};return r.forEach((r=>{n[r]=function(e,r){return!1!==(0,t.Mt)(r,"".concat(e,".enabled"))}(r,e)})),n}var a=i(2384),s=i(909),c=i(9557),u=i(7022);class d extends c.w{constructor(e){super(e),this.aggregatedData={}}store(e,t,r,n,i){var o=this.getBucket(e,t,r,i);return o.metrics=function(e,t){t||(t={count:0});return t.count+=1,(0,u.D)(e,(function(e,r){t[e]=f(r,t[e])})),t}(n,o.metrics),o}merge(e,t,r,n,i){var o=this.getBucket(e,t,n,i);if(o.metrics){var a=o.metrics;a.count+=r.count,(0,u.D)(r,(function(e,t){if("count"!==e){var n=a[e],i=r[e];i&&!i.c?a[e]=f(i.t,n):a[e]=function(e,t){if(!t)return e;t.c||(t=l(t.t));return t.min=Math.min(e.min,t.min),t.max=Math.max(e.max,t.max),t.t+=e.t,t.sos+=e.sos,t.c+=e.c,t}(i,a[e])}}))}else o.metrics=r}storeMetric(e,t,r,n){var i=this.getBucket(e,t,r);return i.stats=f(n,i.stats),i}getBucket(e,t,r,n){this.aggregatedData[e]||(this.aggregatedData[e]={});var i=this.aggregatedData[e][t];return i||(i=this.aggregatedData[e][t]={params:r||{}},n&&(i.custom=n)),i}get(e,t){return t?this.aggregatedData[e]&&this.aggregatedData[e][t]:this.aggregatedData[e]}take(e){for(var t={},r="",n=!1,i=0;i<e.length;i++)t[r=e[i]]=h(this.aggregatedData[r]),t[r].length&&(n=!0),delete this.aggregatedData[r];return n?t:null}}function f(e,t){return null==e?function(e){e?e.c++:e={c:1};return e}(t):t?(t.c||(t=l(t.t)),t.c+=1,t.t+=e,t.sos+=e*e,e>t.max&&(t.max=e),e<t.min&&(t.min=e),t):{t:e}}function l(e){return{t:e,min:e,max:e,sos:e*e,c:1}}function h(e){return"object"!=typeof e?[]:(0,u.D)(e,p)}function p(e,t){return t}var g=i(6797),v=i(5526),m=i(8610);var b=i(5637),y=i(7817),w=i(600),E=i(9715),A=i(948),x=i(6649),O=i(488);new class{constructor(t){let r=arguments.length>1&&void 0!==arguments[1]?arguments[1]:(0,v.ky)(16);this.agentIdentifier=r,this.sharedAggregator=new d({agentIdentifier:this.agentIdentifier}),this.features={},this.desiredFeatures=t.features||[],this.desiredFeatures.sort(((t,r)=>e.p[t.featureName]-e.p[r.featureName])),Object.assign(this,(0,a.j)(this.agentIdentifier,t,t.loaderType||"agent")),this.start()}get config(){return{info:(0,t.C5)(this.agentIdentifier),init:(0,t.P_)(this.agentIdentifier),loader_config:(0,t.DL)(this.agentIdentifier),runtime:(0,t.OP)(this.agentIdentifier)}}start(){const e="features";try{const t=n(this.agentIdentifier);this.desiredFeatures.forEach((e=>{if(t[e.featureName]){const r=(0,s.Z)(e.featureName);r.every((e=>t[e]))||(0,m.Z)("".concat(e.featureName," is enabled but one or more dependent features has been disabled (").concat(JSON.stringify(r),"). This may cause unintended consequences or missing data...")),this.features[e.featureName]=new e(this.agentIdentifier,this.sharedAggregator)}})),(0,g.Qy)(this.agentIdentifier,this.features,e)}catch(t){(0,m.Z)("Failed to initialize all enabled instrument classes (agent aborted) -",t);for(const e in this.features)this.features[e].abortHandler?.();const r=(0,g.fP)();return delete r.initializedAgents[this.agentIdentifier]?.api,delete r.initializedAgents[this.agentIdentifier]?.[e],delete this.sharedAggregator,delete r.ee?.get(this.agentIdentifier),!1}}}({features:[b.Instrument,y.Instrument,x.Instrument,A.Instrument,w.Instrument,O.Instrument,E.Instrument],loaderType:"pro"})})(),window.NRBA=o})();</script>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

    <title>Frontiers | Supervised Machine Learning Applied to Wearable Sensor Data Can Accurately Classify Functional Fitness Exercises Within a Continuous Workout</title>

    <link rel="shortcut icon" href="https://3718aeafc638f96f5bd6-d4a9ca15fc46ba40e71f94dec0aad28c.ssl.cf1.rackcdn.com/favicon_16x16.ico" type="image/x-icon" />

    <meta property="og:type" content="article" />
    <meta property="frontiers:type" content="Article" />
    <meta property="og:site_name" name="site_name" content="Frontiers" />
    <meta property="og:title" name="Title" content="Supervised Machine Learning Applied to Wearable Sensor Data Can Accurately Classify Functional Fitness Exercises Within a Continuous Workout" />
    <meta property="og:description" name="Description" content="Observing, classifying and assessing human movements is important in many applied fields, including human-computer interface, clinical assessment, activity monitoring and sports performance. The redundancy of options in planning and implementing motor programmes, the inter- and intra-individual variability in movement execution, and the time-continuous, high-dimensional nature of motion data make segmenting sequential movements into a smaller set of discrete classes of actions non-trivial. We aimed to develop and validate a method for the automatic classification of four popular functional fitness drills, which are commonly performed in current circuit training routines. Five inertial measurement units were located on the upper and lower limb, and on the trunk of fourteen participants. Positions were chosen by keeping into account the dynamics of the movement and the positions where commercially-available smart technologies are typically secured. Accelerations and angular velocities were acquired continuously from the units and used to train and test different supervised learning models, including k-Nearest Neighbors (kNN) and support-vector machine (SVM) algorithms. The use of different kernel functions, as well as different strategies to segment continuous inertial data were explored. Classification performance was assessed from both the training dataset (k-fold cross-validation), and a test dataset (leave-one-subject-out validation). Classification from different subset..." />
    <meta property="og:url" name="url" content="https://www.frontiersin.org/articles/10.3389/fbioe.2020.00664/full" />


        <meta property="og:image" content="https://www.frontiersin.org/files/MyHome%20Article%20Library/527814/527814_Thumb_400.jpg" />
            <meta name="citation_volume" content="8" />
        <meta name="citation_journal_title" content="Frontiers in Bioengineering and Biotechnology" />
        <meta name="citation_publisher" content="Frontiers" />
        <meta name="citation_journal_abbrev" content="Front. Bioeng. Biotechnol." />
        <meta name="citation_issn" content="2296-4185" />
        <meta name="citation_doi" content="10.3389/fbioe.2020.00664" />
        <meta name="citation_pages" content="664" />
        <meta name="citation_language" content="English" />
        <meta name="citation_title" content="Supervised Machine Learning Applied to Wearable Sensor Data Can Accurately Classify Functional Fitness Exercises Within a Continuous Workout" />
        <meta name="citation_keywords" content="automatic classification; Inertial measurement unit (IMU); Sport; On-field testing; activity monitoring; machine learning; wearable sensors" />
        <meta name="citation_abstract" content="Observing, classifying and assessing human movements is important in many applied fields, including human-computer interface, clinical assessment, activity monitoring and sports performance. The redundancy of options in planning and implementing motor programmes, the inter- and intra-individual variability in movement execution, and the time-continuous, high-dimensional nature of motion data make segmenting sequential movements into a smaller set of discrete classes of actions non-trivial. We aimed to develop and validate a method for the automatic classification of four popular functional fitness drills, which are commonly performed in current circuit training routines. Five inertial measurement units were located on the upper and lower limb, and on the trunk of fourteen participants. Positions were chosen by keeping into account the dynamics of the movement and the positions where commercially-available smart technologies are typically secured. Accelerations and angular velocities were acquired continuously from the units and used to train and test different supervised learning models, including k-Nearest Neighbours (kNN) and support-vector machine (SVM) algorithms. The use of different kernel functions, as well as different strategies to segment continuous inertial data were explored. Classification performance was assessed from both the training dataset (k-fold cross-validation), and a test dataset (leave-one-subject-out validation). Classification from different subsets of the measurement units was also evaluated (1-sensor and 2-sensor data). SVM with a cubic kernel and fed with data from 600 ms windows with a 10% overlap gave the best classification performances, yielding to an overall accuracy of 97.8%. This approach did not misclassify any functional fitness movement for another, but confused relatively frequently (2.8-18.9%) a fitness movement phase with the transition between subsequent repetitions of the same task or different drills. Among 1-sensor configurations, the upper arm achieved the best classification performance (96.4% accuracy), whereas combining the upper arm and the thigh sensors obtained the highest level of accuracy (97.6%) from 2-sensors movement tracking. We found that supervised learning can successfully classify complex sequential movements such as those of functional fitness workouts. Our approach, which could exploit technologies currently available in the consumer market, demonstrated exciting potential for future on-field applications including unstructured training." />
        <meta name="description" content="Observing, classifying and assessing human movements is important in many applied fields, including human-computer interface, clinical assessment, activity monitoring and sports performance. The redundancy of options in planning and implementing motor programmes, the inter- and intra-individual variability in movement execution, and the time-continuous, high-dimensional nature of motion data make segmenting sequential movements into a smaller set of discrete classes of actions non-trivial. We aimed to develop and validate a method for the automatic classification of four popular functional fitness drills, which are commonly performed in current circuit training routines. Five inertial measurement units were located on the upper and lower limb, and on the trunk of fourteen participants. Positions were chosen by keeping into account the dynamics of the movement and the positions where commercially-available smart technologies are typically secured. Accelerations and angular velocities were acquired continuously from the units and used to train and test different supervised learning models, including k-Nearest Neighbours (kNN) and support-vector machine (SVM) algorithms. The use of different kernel functions, as well as different strategies to segment continuous inertial data were explored. Classification performance was assessed from both the training dataset (k-fold cross-validation), and a test dataset (leave-one-subject-out validation). Classification from different subsets of the measurement units was also evaluated (1-sensor and 2-sensor data). SVM with a cubic kernel and fed with data from 600 ms windows with a 10% overlap gave the best classification performances, yielding to an overall accuracy of 97.8%. This approach did not misclassify any functional fitness movement for another, but confused relatively frequently (2.8-18.9%) a fitness movement phase with the transition between subsequent repetitions of the same task or different drills. Among 1-sensor configurations, the upper arm achieved the best classification performance (96.4% accuracy), whereas combining the upper arm and the thigh sensors obtained the highest level of accuracy (97.6%) from 2-sensors movement tracking. We found that supervised learning can successfully classify complex sequential movements such as those of functional fitness workouts. Our approach, which could exploit technologies currently available in the consumer market, demonstrated exciting potential for future on-field applications including unstructured training." />
        <meta name="citation_online_date" content="2020/05/28" />
        <meta name="citation_publication_date" content="2020/07/07" />
        <meta name="citation_author" content="Preatoni, Ezio" />
        <meta name="citation_author_institution" content="Department for Health, University of Bath, United Kingdom" />
        <meta name="citation_author" content="Nodari, Stefano" />
        <meta name="citation_author_institution" content="Dipartimento di Ingegneria dell&#39;Informazione, Universit&#224; degli Studi di Brescia, Italy" />
        <meta name="citation_author" content="Lopomo, Nicola Francesco" />
        <meta name="citation_author_institution" content="Dipartimento di Ingegneria dell&#39;Informazione, Universit&#224; degli Studi di Brescia, Italy" />
    <meta name="Keywords" content="automatic classification, Inertial measurement unit (IMU), Sport, On-field testing, activity monitoring, machine learning, wearable sensors" />

        <meta name="dc.identifier" content="doi:10.3389/fbioe.2020.00664">
        <!--CrossMark widget-->
    
    <script type="text/javascript">

        var CurrentIBarMenu = 'bysubjects';
        var CurrentPageCode = 'ARTICLE_PAGE_NEW';

        var FRConfiguration = (function () {
            return {
                Environment: 'Live',
                SANVirtualPath: 'https://www.frontiersin.org/files/',
                SharepointWebsiteUrl: 'https://www.frontiersin.org',
                FrontiersJournalUIUrl: 'https://www.frontiersin.org',
                FrontiersJournalAPIUrl: 'https://api-journal.frontiersin.org',
                FrontiersReviewUIUrl: '',
                FrontiersReviewAPIUrl: '',
                FrontiersCookie: 'frontiersN',
                FrontiersCookieRememberMe: 'frontiersNt',
                FrontiersLoginUrl: 'https://www.frontiersin.org/Login.aspx',
                FrontiersRegistrationUrl: 'http://www.frontiersin.org/Registration/Register.aspx',
                IsCommentVisible: 'True',
                IsPreview: 'False',
                TenantName: 'Frontiers'
            }
        })();

        var FRLanguage = (function() {
            var languageSet = {"article_accepteddate":"Accepted: ","article_analyticstooltip":"View numbers are not up-to-date. We are working to restore this functionality and offer updated views.","article_analyticsviewimpact":"View Article Impact","article_bibtex":"BibTex","article_copyrighttext":"This is an open-access article distributed under the terms of the \u003ca href=\"http://creativecommons.org/licenses/by/4.0/\"\u003eCreative Commons Attribution License (CC BY)\u003c/a\u003e. The use, distribution or reproduction in other forums is permitted, provided the original author(s) or licensor are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.","article_downloadarticle":"Download Article","article_downloadpdf":"Download PDF","article_downloadprovisionalarticle":"Download Provisional Article","article_endnote":"EndNote","article_epub":"EPUB","article_exportcitation":"Export Citation","article_jats":"JATS","article_nlm":"XML (NLM)","article_pdf":"PDF","article_provisionalpdf":"Provisional PDF","article_publisheddate":"Published online: ","article_readfulltext":"Read Full Text","article_receiveddate":"Received:","article_referencemanager":"Reference Manager","article_shareon":"SHARE ON","article_simpletextfile":"Simple TEXT file","article_supplementaldata":"SUPPLEMENTAL DATA","article_tableofcontent":"TABLE OF CONTENTS","article_viewenhancedpdf":"ReadCube","article_xml":"XML","browserwarningtext":"\u003ch2\u003eWarning!\u003c/h2\u003e\u003cp\u003eYou are using an \u003cstrong\u003eoutdated\u003c/strong\u003e browser. This page doesn\u0027t support Internet Explorer 6, 7 and 8.\u003cbr /\u003ePlease \u003ca class=\"blue\" href=\"http://browsehappy.com/\"\u003eupgrade your browser\u003c/a\u003e or \u003ca class=\"blue\" href=\"http://www.google.com/chromeframe/?redirect=true\"\u003eactivate Google Chrome Frame\u003c/a\u003e to improve your experience.\u003c/p\u003e","impact_backtoarticle":"Back to article","people_also_lookedat":"People also looked at","article_analyticstotalviews":"total views","ART_FRONTIERS":"Frontiers","Article_AnalyticsTotalViews":"total views","Article_ArchiveLinkText":"Articles","Article_Citation":"Citation:","Article_Commentary":"COMMENTARY","Article_Copyright":"Copyright:","Article_Correspondence":"* Correspondence:","Article_DownloadProvisionalPDF":"Download Provisional PDF","Article_EditedBy":"Edited by:","Article_Keywords":"Keywords:","Article_OriginalArticle":"ORIGINAL ARTICLE","Article_PaperPendingPublishedDate":"Paper pending published:","Article_ReviewedBy":"Reviewed by: ","Article_RTInfoText":"This article is part of the Research Topic","COMMENT_HEADERTEXT":"Comment text too long","COMMENT_WARNINGTEXT":"Comments must be less than 4,000 characters. You have entered "};

            return {
                value: function(key) {
                    if (languageSet[key]) {
                        return languageSet[key];
                    } else {
                        throw new Error('Unable to get the value status from the language set'); // Use Error, not FRError
                    }
                }
            };

        })();

        var FRJournalDetails = (function() {
            return {
                JournalType: 'section',
                JournalId: '452',
                SectionId: '661'
            };
        })();

        var FRArticleDetails = (function() {
            return {
                ArticleId: '527814',
                DisclaimerText: 'All claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors and the reviewers. Any product that may be evaluated in this article or claim that may be made by its manufacturer is not guaranteed or endorsed by the publisher.'
            };
        })();
        var FRArticleRecaptchaSettings = (function() {
            return {
                RecaptchaSiteKey: '6LdG3i0UAAAAAOC4qUh35ubHgJotEHp_STXHgr_v'
            };
        })();

    </script>

    <link href="https://209cd62b0febf2a55d40-715eb384bc6027b876e93ad33d5c5ec3.ssl.cf3.rackcdn.com/font-awesome-4.3.0/css/font-awesome.min.css" rel="stylesheet"/>

    <link href="https://9d0dd7a648345f19af83-877d2ecaf11b88d5e17327c758e17ef6.ssl.cf2.rackcdn.com/museo-sans-1.0.1/css/museo-sans.css" rel="stylesheet"/>

    <link href="https://static.frontiersin.org/areas/articles/css/app?v=xQphOZc0NyhWtu7TTvP_xo2f4l7fql5SDB9HIL8DrlI1" rel="stylesheet"/>

    <style type="text/css"> .journal-bioengineering-and-biotechnology {
 background: #fff url('https://3718aeafc638f96f5bd6-d4a9ca15fc46ba40e71f94dec0aad28c.ssl.cf1.rackcdn.com/journal-bioengineering-and-biotechnology.png') no-repeat 100% -10px;
	background-size: 48%;
}
/* Displays/Screens (e.g. 19" WS @ 1440x900) --------------- */ @media only screen and (max-width: 1649px) {
 .journal-bioengineering-and-biotechnology {
  background: #fff url('https://3718aeafc638f96f5bd6-d4a9ca15fc46ba40e71f94dec0aad28c.ssl.cf1.rackcdn.com/journal-bioengineering-and-biotechnology.png') no-repeat 102% 38px;
	background-size: 65%;
 }}
/* Displays/Screens (e.g. MacBook @ 1280x800) -------------- */
@media only screen and (max-width: 1409px) {
 .journal-bioengineering-and-biotechnology {
  background: #fff url('https://3718aeafc638f96f5bd6-d4a9ca15fc46ba40e71f94dec0aad28c.ssl.cf1.rackcdn.com/journal-bioengineering-and-biotechnology.png') no-repeat 102% 33px;
	background-size: 74%;
 }}
/* Large Devices, Wide Screens --------- */  @media only screen and (max-width : 1250px) {
.journal-bioengineering-and-biotechnology {
  background: #fff url('https://3718aeafc638f96f5bd6-d4a9ca15fc46ba40e71f94dec0aad28c.ssl.cf1.rackcdn.com/journal-bioengineering-and-biotechnology.png') no-repeat 102% 38px;
	background-size: 75%;
 }}

/* Medium Devices, Desktops ---------- */  @media only screen and (max-width : 992px) { {
.journal-bioengineering-and-biotechnology {
  background: #fff url('https://3718aeafc638f96f5bd6-d4a9ca15fc46ba40e71f94dec0aad28c.ssl.cf1.rackcdn.com/journal-bioengineering-and-biotechnology.png') no-repeat -600px 60px;
 }}
/* Small Devices, Tablets ------------ */  @media only screen and (max-width : 768px) {
.journal-bioengineering-and-biotechnology {
  background: #fff url('https://3718aeafc638f96f5bd6-d4a9ca15fc46ba40e71f94dec0aad28c.ssl.cf1.rackcdn.com/journal-bioengineering-and-biotechnology.png') no-repeat -900px 60px;
 }} 

/* Small Devices, Tablets  --------- */ @media only screen and (max-width : 640px) {
.journal-bioengineering-and-biotechnology {
  background: #fff url('https://3718aeafc638f96f5bd6-d4a9ca15fc46ba40e71f94dec0aad28c.ssl.cf1.rackcdn.com/journal-bioengineering-and-biotechnology.png') no-repeat -900px 60px;
 }}

/* Extra Small Devices, Phones-----------  */  @media only screen and (max-width : 480px) { 
.journal-bioengineering-and-biotechnology {
  background: #fff url('https://3718aeafc638f96f5bd6-d4a9ca15fc46ba40e71f94dec0aad28c.ssl.cf1.rackcdn.com/journal-bioengineering-and-biotechnology.png') no-repeat -1000px 60px;
 }} </style>
    
</head>
<body class="journal-bioengineering-and-biotechnology section-biomechanics">

    
    <a href="#main-content" class="bypassBlock-wrapper" id="bypass">
        <span class="bypassBlock-button">Skip to main content</span>
    </a>
    <script type="text/javascript">

        const container = document.getElementById("bypass");

        //eventListeners
        container.addEventListener('focus', function () {
            document.documentElement.setAttribute("bypass-focus", "true");
        }, true);

        container.addEventListener('blur', function () {
            document.documentElement.setAttribute("bypass-focus", "false");
        }, true);

    </script>
    <!-- Google Tag Manager (noscript) -->

    <noscript>
        <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-TFKV632&gtm_auth=NsJH3Ts1-89I8lZURwQYmw&gtm_preview=env-1&gtm_cookies_win=x"
                height="0" width="0" style="display:none;visibility:hidden"></iframe>
    </noscript>

    <!-- Google Tag Manager (noscript) -->
    <noscript>
        <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-N279F7B&gtm_auth=jfKETtlWamfZ4l02YiFCQw&gtm_preview=env-1&gtm_cookies_win=x"
                height="0" width="0" style="display:none;visibility:hidden"></iframe>
    </noscript>
    <!-- End Google Tag Manager (noscript) -->

    <script src="https://static.frontiersin.org/areas/articles/js/vendors?v=DePgy-bzWn8MT3oeRnhgNZnKY-DDBrukcfla6sr3SlM1"></script>



    <frontiers-ibar main-domain="frontiersin.org" loop-url="https://loop.frontiersin.org" journal-id="452"></frontiers-ibar>
    <div class="page-container sidemenu-collapsed" id="main-content">
        


<div id="article" class="boxed white no-padding">
    <div class="container-fluid main-container-xxl">
        <div class="row" id="similar-articles">
            <div class="new-wrapper">
                


<style>
    .disabled-supplementary-btn {
        cursor: not-allowed;
        pointer-events: none;
        opacity: .65;
        filter: alpha(opacity=65);
        -webkit-box-shadow: none;
        box-shadow: none;
    }
</style>
<aside class="right-container">

        <div class="side-article-download clearfix">
            <ul class="list-unstyled list-inline clearfix">
                    <li class="dropdown text-center paper">
                        <button data-target="#" class="dropdown-toggle" data-test-id="download-button" data-toggle="dropdown" role="button" aria-expanded="false">
                            <span class="icon-label" data-event="download-button-click">Download Article</span>
                        </button>

                        <div class="dropdown-menu-wrapper">
                            <div class="dropdown-menu-mobile-title">
                                Download Article
                            </div>
                            <ul class="dropdown-menu" role="menu">
                                    <li>
                                        <a class="download-files-pdf action-link" href="/articles/10.3389/fbioe.2020.00664/pdf" data-test-id="article-downloadpdf" data-event="downloadpdf-button-click">
                                            Download PDF
                                        </a>
                                    </li>
                                    <li>
                                        <a class="download-files-readcube" href="http://www.readcube.com/articles/10.3389/fbioe.2020.00664" data-test-id="article-viewenhancedpdf" data-event="downloadreadcube-button-click">
                                            ReadCube
                                        </a>
                                    </li>
                                    <li>
                                        <a class="download-files-epub" href="/articles/10.3389/fbioe.2020.00664/epub" data-test-id="article-epub" data-event="downloadepub-button-click">
                                            EPUB
                                        </a>
                                    </li>
                                    <li>
                                        <a class="download-files-nlm" href="/articles/10.3389/fbioe.2020.00664/xml/nlm" data-test-id="article-nlm" data-event="downloadnlm-button-click">
                                            XML (NLM)
                                        </a>
                                    </li>
                                
                            </ul>
                            <button class="dropdown-menu-mobile-close-button" aria-label="Close" data-toggle="dropdown"></button>
                        </div>
                    </li>
                            <li class="dropdown">
                <button type="button" class="navbar-toggle navbar-share" data-toggle="dropdown" data-target="#" aria-expanded="false"></button>
                <div class="dropdown-menu-wrapper">
                    <div class="dropdown-menu-mobile-title">
                        Share on
                    </div>
                    <ul class="dropdown-menu" role="menu">
                        <li class="social_block">
                            <div class="atButton">
                                <a class="addthis_button_twitter at300b"
                                   addthis:title="Supervised Machine Learning Applied to Wearable Sensor Data Can Accurately Classify Functional Fitness Exercises Within a Continuous Workout"
                                   addthis:description="Observing, classifying and assessing human movements is important in many applied fields, including human-computer interface, clinical assessment, activity monitoring and sports performance. The redundancy of options in planning and implementing motor programmes, the inter- and intra-individual variability in movement execution, and the time-continuous, high-dimensional nature of motion data make segmenting sequential movements into a smaller set of discrete classes of actions non-trivial. We aimed to develop and validate a method for the automatic classification of four popular functional fitness drills, which are commonly performed in current circuit training routines. Five inertial measurement units were located on the upper and lower limb, and on the trunk of fourteen participants. Positions were chosen by keeping into account the dynamics of the movement and the positions where commercially-available smart technologies are typically secured. Accelerations and angular velocities were acquired continuously from the units and used to train and test different supervised learning models, including k-Nearest Neighbors (kNN) and support-vector machine (SVM) algorithms. The use of different kernel functions, as well as different strategies to segment continuous inertial data were explored. Classification performance was assessed from both the training dataset (k-fold cross-validation), and a test dataset (leave-one-subject-out validation). Classification from different subset..."
                                   addthis:url="https://www.frontiersin.org/articles/10.3389/fbioe.2020.00664"
                                   data-event-param="Supervised Machine Learning Applied to Wearable Sensor Data Can Accurately Classify Functional Fitness Exercises Within a Continuous Workout"
                                   title="Tweet"
                                   target="_blank"
                                   href="#">
                                    <span class="at-icon-wrapper"></span>
                                </a>
                            </div>
                        </li>
                        <li class="social_block">
                            <div class="atButton">
                                <a class="addthis_button_linkedin at300b"
                                   addthis:title="Supervised Machine Learning Applied to Wearable Sensor Data Can Accurately Classify Functional Fitness Exercises Within a Continuous Workout"
                                   addthis:description="Observing, classifying and assessing human movements is important in many applied fields, including human-computer interface, clinical assessment, activity monitoring and sports performance. The redundancy of options in planning and implementing motor programmes, the inter- and intra-individual variability in movement execution, and the time-continuous, high-dimensional nature of motion data make segmenting sequential movements into a smaller set of discrete classes of actions non-trivial. We aimed to develop and validate a method for the automatic classification of four popular functional fitness drills, which are commonly performed in current circuit training routines. Five inertial measurement units were located on the upper and lower limb, and on the trunk of fourteen participants. Positions were chosen by keeping into account the dynamics of the movement and the positions where commercially-available smart technologies are typically secured. Accelerations and angular velocities were acquired continuously from the units and used to train and test different supervised learning models, including k-Nearest Neighbors (kNN) and support-vector machine (SVM) algorithms. The use of different kernel functions, as well as different strategies to segment continuous inertial data were explored. Classification performance was assessed from both the training dataset (k-fold cross-validation), and a test dataset (leave-one-subject-out validation). Classification from different subset..."
                                   addthis:url="https://www.frontiersin.org/articles/10.3389/fbioe.2020.00664"
                                   data-event-param="Supervised Machine Learning Applied to Wearable Sensor Data Can Accurately Classify Functional Fitness Exercises Within a Continuous Workout"
                                   title="LinkedIn"
                                   target="_blank"
                                   href="#">
                                    <span class="at-icon-wrapper"></span>
                                </a>
                            </div>
                        </li>
                        <li class="social_block">
                            <div class="atButton">
                                <a class="addthis_button_facebook at300b"
                                   addthis:title="Supervised Machine Learning Applied to Wearable Sensor Data Can Accurately Classify Functional Fitness Exercises Within a Continuous Workout"
                                   addthis:description="Observing, classifying and assessing human movements is important in many applied fields, including human-computer interface, clinical assessment, activity monitoring and sports performance. The redundancy of options in planning and implementing motor programmes, the inter- and intra-individual variability in movement execution, and the time-continuous, high-dimensional nature of motion data make segmenting sequential movements into a smaller set of discrete classes of actions non-trivial. We aimed to develop and validate a method for the automatic classification of four popular functional fitness drills, which are commonly performed in current circuit training routines. Five inertial measurement units were located on the upper and lower limb, and on the trunk of fourteen participants. Positions were chosen by keeping into account the dynamics of the movement and the positions where commercially-available smart technologies are typically secured. Accelerations and angular velocities were acquired continuously from the units and used to train and test different supervised learning models, including k-Nearest Neighbors (kNN) and support-vector machine (SVM) algorithms. The use of different kernel functions, as well as different strategies to segment continuous inertial data were explored. Classification performance was assessed from both the training dataset (k-fold cross-validation), and a test dataset (leave-one-subject-out validation). Classification from different subset..."
                                   addthis:url="https://www.frontiersin.org/articles/10.3389/fbioe.2020.00664"
                                   data-event-param="Supervised Machine Learning Applied to Wearable Sensor Data Can Accurately Classify Functional Fitness Exercises Within a Continuous Workout"
                                   title="Facebook"
                                   target="_blank"
                                   href="#">
                                    <span class="at-icon-wrapper"></span>
                                </a>
                            </div>
                        </li>
                    </ul>
                    <button class="dropdown-menu-mobile-close-button"
                            aria-label="Close"
                            data-toggle="dropdown"></button>
                </div>
            </li>

            <li class="dropdown">
                <button type="button" class="navbar-toggle navbar-citations" data-event="citation-button-click" data-toggle="dropdown" data-target="#" aria-expanded="false">
                </button>
                <div class="dropdown-menu-wrapper">
                    <div class="dropdown-menu-mobile-title">
                        Export citation
                    </div>
                    <ul class="dropdown-menu" role="menu">
                            <li>
                                <a data-test-id="article-endnote" href="/articles/10.3389/fbioe.2020.00664/endNote" data-event="endnotedownload-button-click">
                                    EndNote
                                </a>
                            </li>
                            <li>
                                <a data-test-id="article-referencemanager" href="/articles/10.3389/fbioe.2020.00664/reference" data-event="referencemanagerdownload-button-click">
                                    Reference Manager
                                </a>
                            </li>
                            <li>
                                <a data-test-id="article-simpletextfile" href="/articles/10.3389/fbioe.2020.00664/text" data-event="simpletextfiledownload-button-click">
                                    Simple TEXT file
                                </a>
                            </li>
                            <li>
                                <a data-test-id="article-bibtex" href="/articles/10.3389/fbioe.2020.00664/bibTex" data-event="bibtexdownload-button-click">
                                    BibTex
                                </a>
                            </li>
                    </ul>
                    <button class="dropdown-menu-mobile-close-button"
                            aria-label="Close"
                            data-toggle="dropdown"></button>
                </div>
            </li>
            </ul>
        </div>
    
    <div class="side-article-impact">
        <ul class="nav">
                <li class="impact-data" title="View numbers are not up-to-date. We are working to restore this functionality and offer updated views.">
                    <span class="title-number"></span>
                    <span class="title-text">total views</span>
                </li>
            <li class="altmetric-wrapper">
                <div class='altmetric-embed' data-badge-type='donut' data-event="altmetric-button-click" data-badge-popover="bottom" data-doi='10.3389/fbioe.2020.00664' data-condensed="true" data-link-target="new"></div>
            </li>
        </ul>
            <a type="button" class="btn btn-default btn-impact " data-test-id="view-article-impact" data-event="impact-button-click" href="http://loop-impact.frontiersin.org/impact/article/527814#views" target="_blank">
                <span></span> View Article Impact
            </a>
    </div>


    <div class="side-article-share ">
        <h5 class="like-h4">SHARE ON</h5>
        <ul class="share-media clearfix" style="padding: 0;">
            <li class="social_block">
                <div class="atButton">
                    <a class="addthis_button_facebook at300b" data-event="shareicon-button-click" data-event-param="Supervised Machine Learning Applied to Wearable Sensor Data Can Accurately Classify Functional Fitness Exercises Within a Continuous Workout" addthis:title="Supervised Machine Learning Applied to Wearable Sensor Data Can Accurately Classify Functional Fitness Exercises Within a Continuous Workout" addthis:description="Observing, classifying and assessing human movements is important in many applied fields, including human-computer interface, clinical assessment, activity monitoring and sports performance. The redundancy of options in planning and implementing motor programmes, the inter- and intra-individual variability in movement execution, and the time-continuous, high-dimensional nature of motion data make segmenting sequential movements into a smaller set of discrete classes of actions non-trivial. We aimed to develop and validate a method for the automatic classification of four popular functional fitness drills, which are commonly performed in current circuit training routines. Five inertial measurement units were located on the upper and lower limb, and on the trunk of fourteen participants. Positions were chosen by keeping into account the dynamics of the movement and the positions where commercially-available smart technologies are typically secured. Accelerations and angular velocities were acquired continuously from the units and used to train and test different supervised learning models, including k-Nearest Neighbors (kNN) and support-vector machine (SVM) algorithms. The use of different kernel functions, as well as different strategies to segment continuous inertial data were explored. Classification performance was assessed from both the training dataset (k-fold cross-validation), and a test dataset (leave-one-subject-out validation). Classification from different subset..." addthis:url="https://www.frontiersin.org/articles/10.3389/fbioe.2020.00664" title="Facebook"><span class="at-icon-wrapper" style="
                              background-color: rgb(29, 161, 242);
                              line-height: 16px;
                              height: 16px;
                              width: 16px;
                            "></span></a>
                </div>
                <a><span class="facebook_count"></span></a>
            </li>
            <li class="social_block">
                <div class="atButton">
                    <a class="addthis_button_twitter at300b" data-event="shareicon-button-click" data-event-param="Supervised Machine Learning Applied to Wearable Sensor Data Can Accurately Classify Functional Fitness Exercises Within a Continuous Workout" addthis:title="Supervised Machine Learning Applied to Wearable Sensor Data Can Accurately Classify Functional Fitness Exercises Within a Continuous Workout" addthis:description="Observing, classifying and assessing human movements is important in many applied fields, including human-computer interface, clinical assessment, activity monitoring and sports performance. The redundancy of options in planning and implementing motor programmes, the inter- and intra-individual variability in movement execution, and the time-continuous, high-dimensional nature of motion data make segmenting sequential movements into a smaller set of discrete classes of actions non-trivial. We aimed to develop and validate a method for the automatic classification of four popular functional fitness drills, which are commonly performed in current circuit training routines. Five inertial measurement units were located on the upper and lower limb, and on the trunk of fourteen participants. Positions were chosen by keeping into account the dynamics of the movement and the positions where commercially-available smart technologies are typically secured. Accelerations and angular velocities were acquired continuously from the units and used to train and test different supervised learning models, including k-Nearest Neighbors (kNN) and support-vector machine (SVM) algorithms. The use of different kernel functions, as well as different strategies to segment continuous inertial data were explored. Classification performance was assessed from both the training dataset (k-fold cross-validation), and a test dataset (leave-one-subject-out validation). Classification from different subset..." addthis:url="https://www.frontiersin.org/articles/10.3389/fbioe.2020.00664" title="Tweet"><span class="at-icon-wrapper" style="
                              background-color: rgb(29, 161, 242);
                              line-height: 16px;
                              height: 16px;
                              width: 16px;
                            "></span></a>
                </div>
                <a><span class="twitter_count"></span></a>
            </li>
            <li class="social_block">
                <div class="atButton">
                    <a class="addthis_button_google at300b" data-event="shareicon-button-click" data-event-param="Supervised Machine Learning Applied to Wearable Sensor Data Can Accurately Classify Functional Fitness Exercises Within a Continuous Workout" addthis:title="Supervised Machine Learning Applied to Wearable Sensor Data Can Accurately Classify Functional Fitness Exercises Within a Continuous Workout" addthis:description="Observing, classifying and assessing human movements is important in many applied fields, including human-computer interface, clinical assessment, activity monitoring and sports performance. The redundancy of options in planning and implementing motor programmes, the inter- and intra-individual variability in movement execution, and the time-continuous, high-dimensional nature of motion data make segmenting sequential movements into a smaller set of discrete classes of actions non-trivial. We aimed to develop and validate a method for the automatic classification of four popular functional fitness drills, which are commonly performed in current circuit training routines. Five inertial measurement units were located on the upper and lower limb, and on the trunk of fourteen participants. Positions were chosen by keeping into account the dynamics of the movement and the positions where commercially-available smart technologies are typically secured. Accelerations and angular velocities were acquired continuously from the units and used to train and test different supervised learning models, including k-Nearest Neighbors (kNN) and support-vector machine (SVM) algorithms. The use of different kernel functions, as well as different strategies to segment continuous inertial data were explored. Classification performance was assessed from both the training dataset (k-fold cross-validation), and a test dataset (leave-one-subject-out validation). Classification from different subset..." addthis:url="https://www.frontiersin.org/articles/10.3389/fbioe.2020.00664" title="Google Bookmark"><span class="at-icon-wrapper" style="
                              background-color: rgb(29, 161, 242);
                              line-height: 16px;
                              height: 16px;
                              width: 16px;
                            "></span></a>
                </div>
                <a><span class="twitter_count"></span></a>
            </li>
            <li class="social_block">
                <div class="atButton">
                    <a class="addthis_button_linkedin at300b" data-event="shareicon-button-click" data-event-param="Supervised Machine Learning Applied to Wearable Sensor Data Can Accurately Classify Functional Fitness Exercises Within a Continuous Workout" addthis:title="Supervised Machine Learning Applied to Wearable Sensor Data Can Accurately Classify Functional Fitness Exercises Within a Continuous Workout" addthis:description="Observing, classifying and assessing human movements is important in many applied fields, including human-computer interface, clinical assessment, activity monitoring and sports performance. The redundancy of options in planning and implementing motor programmes, the inter- and intra-individual variability in movement execution, and the time-continuous, high-dimensional nature of motion data make segmenting sequential movements into a smaller set of discrete classes of actions non-trivial. We aimed to develop and validate a method for the automatic classification of four popular functional fitness drills, which are commonly performed in current circuit training routines. Five inertial measurement units were located on the upper and lower limb, and on the trunk of fourteen participants. Positions were chosen by keeping into account the dynamics of the movement and the positions where commercially-available smart technologies are typically secured. Accelerations and angular velocities were acquired continuously from the units and used to train and test different supervised learning models, including k-Nearest Neighbors (kNN) and support-vector machine (SVM) algorithms. The use of different kernel functions, as well as different strategies to segment continuous inertial data were explored. Classification performance was assessed from both the training dataset (k-fold cross-validation), and a test dataset (leave-one-subject-out validation). Classification from different subset..." addthis:url="https://www.frontiersin.org/articles/10.3389/fbioe.2020.00664" title="LinkedIn"><span class="at-icon-wrapper" style="
                              background-color: rgb(29, 161, 242);
                              line-height: 16px;
                              height: 16px;
                              width: 16px;
                            "></span></a>
                </div>
                <a><span class="linkedin_count"></span></a>
            </li>
            <li class="social_block">
                <div class="atButton">
                    <a class="addthis_button_more at300b" data-event="shareicon-button-click" data-event-param="Supervised Machine Learning Applied to Wearable Sensor Data Can Accurately Classify Functional Fitness Exercises Within a Continuous Workout" addthis:title="Supervised Machine Learning Applied to Wearable Sensor Data Can Accurately Classify Functional Fitness Exercises Within a Continuous Workout" addthis:description="Observing, classifying and assessing human movements is important in many applied fields, including human-computer interface, clinical assessment, activity monitoring and sports performance. The redundancy of options in planning and implementing motor programmes, the inter- and intra-individual variability in movement execution, and the time-continuous, high-dimensional nature of motion data make segmenting sequential movements into a smaller set of discrete classes of actions non-trivial. We aimed to develop and validate a method for the automatic classification of four popular functional fitness drills, which are commonly performed in current circuit training routines. Five inertial measurement units were located on the upper and lower limb, and on the trunk of fourteen participants. Positions were chosen by keeping into account the dynamics of the movement and the positions where commercially-available smart technologies are typically secured. Accelerations and angular velocities were acquired continuously from the units and used to train and test different supervised learning models, including k-Nearest Neighbors (kNN) and support-vector machine (SVM) algorithms. The use of different kernel functions, as well as different strategies to segment continuous inertial data were explored. Classification performance was assessed from both the training dataset (k-fold cross-validation), and a test dataset (leave-one-subject-out validation). Classification from different subset..." addthis:url="https://www.frontiersin.org/articles/10.3389/fbioe.2020.00664" title="View more services"><span class="at-icon-wrapper" style="
                              background-color: rgb(29, 161, 242);
                              line-height: 16px;
                              height: 16px;
                              width: 16px;
                            "></span></a>
                </div>
                <a><span class="total_count"></span></a>
            </li>
        </ul>
    </div>

        <aside id="anchors" class="pull-left table-of-contents side-article">
            <div class="side-people hidden-sm hidden-xs">
                    <section class="side-article-editors">
                                <header><h5 class="like-h4">Edited by</h5></header>
                                <a class="authors" href="https://loop.frontiersin.org/people/638591/overview" target="_blank">
                                    <img alt=" " class="pr5 pull-left" onerror="this.src = &#39;/Areas/Articles/Images/Frontiers/Common/Profile/default-loop-profile.jpg&#39;" src="https://loop.frontiersin.org/images/profile/638591/24"></img>
                                    <h6 class="author-link-aside">Matteo Zago</h6>
                                </a>
                                <div class="clearfix"></div>
                                <p><span class="notes">University of Milan, Italy</span></p>
                                                        <header><h5 class="like-h4"> Reviewed by</h5></header>
                                    <a class="authors" href="https://loop.frontiersin.org/people/886944/overview" target="_blank">
                                        <img alt=" " class="pr5 pull-left" onerror="this.src = &#39;/Areas/Articles/Images/Frontiers/Common/Profile/default-loop-profile.jpg&#39;" src="https://loop.frontiersin.org/images/profile/886944/24"></img>
                                        <h6 class="author-link-aside">Anwar P. P. Abdul Majeed</h6>
                                    </a>
                                    <div class="clearfix"></div>
                                    <p><span class="notes">Xi&#39;an Jiaotong-Liverpool University, China</span></p>
                                    <a class="authors" href="https://loop.frontiersin.org/people/896765/overview" target="_blank">
                                        <img alt=" " class="pr5 pull-left" onerror="this.src = &#39;/Areas/Articles/Images/Frontiers/Common/Profile/default-loop-profile.jpg&#39;" src="https://loop.frontiersin.org/images/profile/896765/24"></img>
                                        <h6 class="author-link-aside">Rabiu M. Musa</h6>
                                    </a>
                                    <div class="clearfix"></div>
                                    <p><span class="notes">University of Malaysia Terengganu, Malaysia</span></p>
                                            </section>
            </div>

        <nav>
                <header><h5 class="like-h4" style="padding-top: 14px; padding-left: 6px; margin-bottom: 6px;">TABLE OF CONTENTS</h5></header>
                <ul class="nav nav-list list-unstyled contents" data-event="toc-link-click">
                    <li>
<ul class="flyoutJournal">
<li><a href="#h1">Abstract</a></li>
<li><a href="#h2">Introduction</a></li>
<li><a href="#h3">Materials and Methods</a></li>
<li><a href="#h4">Results</a></li>
<li><a href="#h5">Discussion</a></li>
<li><a href="#h6">Conclusions</a></li>
<li><a href="#h7">Data Availability Statement</a></li>
<li><a href="#h8">Ethics Statement</a></li>
<li><a href="#h9">Consent Statement</a></li>
<li><a href="#h10">Author Contributions</a></li>
<li><a href="#h11">Funding</a></li>
<li><a href="#h12">Conflict of Interest</a></li>
<li><a href="#h13">References</a></li>
</ul>
                    </li>
                </ul>
        </nav>
    </aside>


    <div class="clearfix"></div>

        <div class="side-article-download side-export clearfix">
            <ul class="list-unstyled list-inline clearfix">
                                    <li class="dropdown text-center citation">
                        <button data-target="#" data-test-id="citation-button" data-toggle="dropdown" role="button" aria-expanded="false">
                            <span class="icon-label" data-event="citation-button-click">Export citation</span>
                        </button>
                        <ul class="dropdown-menu" role="menu">
                                <li>
                                    <a data-test-id="article-endnote" href="/articles/10.3389/fbioe.2020.00664/endNote" data-event="endnotedownload-button-click">
                                        EndNote
                                    </a>
                                </li>
                                <li>
                                    <a data-test-id="article-referencemanager" href="/articles/10.3389/fbioe.2020.00664/reference" data-event="referencemanagerdownload-button-click">
                                        Reference Manager
                                    </a>
                                </li>
                                <li>
                                    <a data-test-id="article-simpletextfile" href="/articles/10.3389/fbioe.2020.00664/text" data-event="simpletextfiledownload-button-click">
                                        Simple TEXT file
                                    </a>
                                </li>
                                <li>
                                    <a data-test-id="article-bibtex" href="/articles/10.3389/fbioe.2020.00664/bibTex" data-event="bibtexdownload-button-click">
                                        BibTex
                                    </a>
                                </li>
                        </ul>
                    </li>
            </ul>
        </div>

        <div class="side-crossmark">
            <script src="https://crossmark-cdn.crossref.org/widget/v2.0/widget.js"></script>
            <a data-target="crossmark" class="crossmark">
                <figure>
                    <img id="crossmark-icon" style="border: 0; width:64px; height:64px;" src="https://crossmark-cdn.crossref.org/widget/v2.0/logos/CROSSMARK_BW_square_no_text.svg" title="" alt="">
                </figure>
                <div id="crossmark-icon">Check for updates</div>
            </a>

        </div>

        <article class="widget-listing people-also-looked-at side-article-related hidden">

            <h5 class="like-h4" data-event="peoplelookedat-link-click">People also looked at</h5>
        </article>


</aside>


                
<main class="">
    <div class="article-section">
        <div class="article-container" data-html="True">
            <div class="abstract-container">
                <div class="article-header-container">
                    <!-- Start CrossMark Snippet v2.0 -->
                   
                    <!-- End CrossMark Snippet -->

                        <div class="header-bar-one">
                            <h2>
                                ORIGINAL RESEARCH article
                            </h2>
                        </div>
                    <div class="header-bar-three-container">
                        <div class="header-bar-three">
                            Front. Bioeng. Biotechnol., 07 July 2020<br>Sec. Biomechanics

                            <br />
                                <span class="volumeInfo"> Volume 8 - 2020 | </span>



                                    <a href="https://doi.org/10.3389/fbioe.2020.00664">https://doi.org/10.3389/fbioe.2020.00664</a>
                        </div>
                    </div>
                </div>

<div class="JournalAbstract">
<a id="h1" name="h1"></a>
<h1>Supervised Machine Learning Applied to Wearable Sensor Data Can Accurately Classify Functional Fitness Exercises Within a Continuous Workout</h1>
<div class="authors">
<a href="https://www.frontiersin.org/people/u/890020" class="user-id-890020"><img class="pr5" src="https://loop.frontiersin.org/images/profile/890020/24" onerror="this.src='https://f96a1a95aaa960e01625-a34624e694c43cdf8b40aa048a644ca4.ssl.cf2.rackcdn.com/Design/Images/newprofile_default_profileimage_new.jpg'">Ezio Preatoni</a><sup>1</sup>, <img class="pr5" src="https://f96a1a95aaa960e01625-a34624e694c43cdf8b40aa048a644ca4.ssl.cf2.rackcdn.com/Design/Images/newprofile_default_profileimage_new.jpg">Stefano Nodari<sup>2</sup> and <a href="https://www.frontiersin.org/people/u/148708" class="user-id-148708"><img class="pr5" src="https://loop.frontiersin.org/images/profile/148708/24" onerror="this.src='https://f96a1a95aaa960e01625-a34624e694c43cdf8b40aa048a644ca4.ssl.cf2.rackcdn.com/Design/Images/newprofile_default_profileimage_new.jpg'">Nicola Francesco Lopomo</a><sup>2</sup><sup>&#x0002A;</sup></div>
<ul class="notes">
<li><span><sup>1</sup></span>Department for Health, University of Bath, Bath, United Kingdom</li>
<li><span><sup>2</sup></span>Dipartimento di Ingegneria dell&#x00027;Informazione, Universit&#x000E0; degli Studi di Brescia, Brescia, Italy</li>
</ul>
<p>Observing, classifying and assessing human movements is important in many applied fields, including human-computer interface, clinical assessment, activity monitoring and sports performance. The redundancy of options in planning and implementing motor programmes, the inter- and intra-individual variability in movement execution, and the time-continuous, high-dimensional nature of motion data make segmenting sequential movements into a smaller set of discrete classes of actions non-trivial. We aimed to develop and validate a method for the automatic classification of four popular functional fitness drills, which are commonly performed in current circuit training routines. Five inertial measurement units were located on the upper and lower limb, and on the trunk of fourteen participants. Positions were chosen by keeping into account the dynamics of the movement and the positions where commercially-available smart technologies are typically secured. Accelerations and angular velocities were acquired continuously from the units and used to train and test different supervised learning models, including k-Nearest Neighbors (kNN) and support-vector machine (SVM) algorithms. The use of different kernel functions, as well as different strategies to segment continuous inertial data were explored. Classification performance was assessed from both the training dataset (k-fold cross-validation), and a test dataset (leave-one-subject-out validation). Classification from different subsets of the measurement units was also evaluated (1-sensor and 2-sensor data). SVM with a cubic kernel and fed with data from 600 ms windows with a 10% overlap gave the best classification performances, yielding to an overall accuracy of 97.8%. This approach did not misclassify any functional fitness movement for another, but confused relatively frequently (2.8&#x02013;18.9%) a fitness movement phase with the transition between subsequent repetitions of the same task or different drills. Among 1-sensor configurations, the upper arm achieved the best classification performance (96.4% accuracy), whereas combining the upper arm and the thigh sensors obtained the highest level of accuracy (97.6%) from 2-sensors movement tracking. We found that supervised learning can successfully classify complex sequential movements such as those of functional fitness workouts. Our approach, which could exploit technologies currently available in the consumer market, demonstrated exciting potential for future on-field applications including unstructured training.</p>
<div class="clear"></div>
</div>
<div class="JournalFullText">
<a id="h2" name="h2"></a><h2>Introduction</h2>
<p class="mb15">The problem of tracking, identifying and classifying human actions has received increasing interest over the years, as it plays a key role in many applied contexts, such as human-computer interface (<a href="#B32">Popoola and Wang, 2012</a>; <a href="#B36">Sarig Bahat et al., 2015</a>; <a href="#B34">Quitadamo et al., 2017</a>; <a href="#B6">Bachmann et al., 2018</a>), daily-life activity monitoring (<a href="#B26">Mannini and Sabatini, 2010</a>; <a href="#B9">Cheng et al., 2015</a>; <a href="#B10">Chetty and White, 2016</a>), clinical assessment (<a href="#B35">Rawashdeh et al., 2016</a>; <a href="#B4">Arifoglu and Bouchachia, 2017</a>; <a href="#B22">Howell et al., 2017</a>) and sports performance (<a href="#B5">Attal et al., 2015</a>; <a href="#B17">Ghazali et al., 2018</a>; <a href="#B23">Hsu et al., 2018</a>). The development of unobtrusive technologies for motion capture (e.g., wearable inertial measurement units&#x02014;IMUs), their widespread integration in relatively cheap, commercially available devices (e.g., smartphones, watches, activity trackers, heart rate monitors, sensorized insoles), and the push toward healthier, more active life styles, have generated a multitude of existing and potential applications where automatic movement classification and assessment is fundamental (<a href="#B5">Attal et al., 2015</a>; <a href="#B9">Cheng et al., 2015</a>; <a href="#B13">Cust et al., 2019</a>).</p>
<p class="mb15">Sport coaching and training still largely rely on visual observation and subjective feedback, and they could benefit from quantitative input supporting decision making. Having quantitative real-time information about the amount, quality and intensity of the work carried out may play an important role at multiple levels. It could inform coaching and strength &#x00026; conditioning planning, help monitoring training load, and evaluating the quality of movement performance (i.e., the outcome achieved) and movement execution (i.e., technique). It could also help improving injury prevention, as continuous monitoring could enable systematic screening of movement behavior, help identifying risk factors and mechanisms of injury, and support decision making in terms of pre- and rehabilitation programmes (<a href="#B24">Jones and Wallace, 2005</a>).</p>
<p class="mb15">Motion capture has traditionally relied on optical-based solutions, but recent development in microelectronics has generated increased interest and research efforts into wearable technologies (<a href="#B2">Adesida et al., 2019</a>). Wearable systems are particularly suitable to sport-specific needs (<a href="#B39">van der Kruk and Reijne, 2018</a>), since: (1) sport usually takes place in uncontrolled and unstructured settings, with environmental conditions difficult to be predicted <i>a priori</i> (e.g., weather, interaction with equipment and other people) and many possible measurement interferences (e.g., electromagnetic noise); (2) the size of the acquisition volume inherently depends on the type of practiced sport (e.g., team vs. individual, indoor vs. outdoor); (3) sensors used to capture sports movements should be both robust and non-obtrusive for the athlete (i.e., ecologically transparent). Systems based on wearable devices, including low-cost activity trackers, smartwatches and smartphones (<a href="#B3">Ahmad et al., 2017</a>), have kept evolving and are widely available for the consumer market, including clinical uses and sports applications (<a href="#B17">Ghazali et al., 2018</a>; <a href="#B23">Hsu et al., 2018</a>). Wearable technologies for motion analysis are predominantly inertial measurement units (IMUs) (<a href="#B14">Davila et al., 2017</a>), which, thanks to their low cost and minimal obtrusiveness, represent an optimal solution for tracking and assessing sports movement on-field (<a href="#B23">Hsu et al., 2018</a>; <a href="#B39">van der Kruk and Reijne, 2018</a>; <a href="#B2">Adesida et al., 2019</a>).</p>
<p class="mb15">Despite the widespread of wearable technology in both applied and research environments, the use of wearable data as input of algorithms for the detection and classification of human actions remains non-trivial, especially in sport. Indeed, sport activities typically involve a large variety of movements, execution technique demonstrates inherent inter- and intra-individual variability, and data is of high-dimensionality (<a href="#B15">Endres et al., 2012</a>; <a href="#B23">Hsu et al., 2018</a>). For this reason, no &#x0201C;one-size-fits-all&#x0201D; approach exists (<a href="#B11">Crema et al., 2019</a>), and bespoke solutions have been reported to address only specific needs, including: recognition/classification (i.e., &#x0201C;what type&#x0201D; of task a subject performs) or identification of the achieved performance (i.e., &#x0201C;how good&#x0201D; the subject performs the task, with respect to a specific reference). In this perspective, the literature has focused the analysis on very specific sport activities and tasks (<a href="#B13">Cust et al., 2019</a>).</p>
<p class="mb15">Among fitness activities, functional training combines aerobic conditioning, weightlifting, interval training, balancing, gymnastics, and functional fitness movements (i.e., exercises that mimic daily life requirements, such as lifting weights) performed at high level of intensity (<a href="#B25">Liebenson, 2006</a>). Functional fitness has been shown to improve cardiovascular capacity, muscle tone and central nervous system efficiency (<a href="#B7">Barbieri et al., 2019</a>; <a href="#B37">Singh and Saini, 2019</a>), but may also increase risk of musculoskeletal injuries affecting shoulder, lower back and knee joints (<a href="#B18">Gianzina and Kassotaki, 2019</a>). It is therefore important to provide athletes with reliable feedback about their efforts, and guide them toward safe movement technique. The availability of a quantitative system for the monitoring of movement completion and overall performance would aid coaching and judging. Functional fitness workouts often consist of continuous sequences of movements, and the identification and assessment of individual elements within the sequence currently relies on visual observation and the expertise of the coach. The wide spectrum of situations in terms of dynamics and body part involved represents a difficult challenge for the automatic classifications of activities and makes it a good proof of concept for the scope of our study. Being able to identify specific movements within a complex movement sequence could be the starting point of a number of useful applications such as counting the number of movement tasks completed, and hence assessing technique and training load for both performance and injury prevention purposes.</p>
<p class="mb0">We aimed to develop and validate a bespoke algorithm for the automatic recognition and classification of four popular functional fitness drills, when performed in a continuous workout. In particular, we wanted to test the capability of supervised machine learning approaches when fed with data from a network of five wearable inertial sensors on the body. Also, we carried out a sensitivity analysis, which could indicate whether subsets of the available measuring units could still provide acceptable classification performance.</p>
<a id="h3" name="h3"></a><h2>Materials and Methods</h2>
<h3 class="pt0">Population</h3>
<p class="mb0">Fourteen healthy participants (11 males and three females, age 18&#x02013;50) with at least 6-months experience in functional training activities volunteered to take part in this study. All participants were physically active, free from any neurological disease and musculoskeletal condition at the time of testing, and familiar with the movement tasks to be performed. The study protocol received ethical approval by the local research ethics committee (reference number EP 17/18 247). Volunteers were informed about experimental procedures and signed informed consent before participating. Based on the existing literature (<a href="#B13">Cust et al., 2019</a>) and the exploratory nature of the study, a sample size &#x0003E;12 was deemed adequate to address the research objectives.</p>
<h3>Experimental Setup</h3>
<p class="mb0">Five wearable units (Trigno Avanti Wireless EMG System, Delsys Inc., USA) were secured to the participants via double-sided hypoallergenic tape and elastic straps. IMUs were located onto specific anatomical landmarks (<a href="#F1">Figure 1</a>), which included the left ankle, thigh, upper arm and wrist, and trunk (L5-S1 level). These positions were chosen to: (a) reproduce the locations where commercially available devices with embedded motion monitors (e.g., smart watches, smart phones, shoe-sensors) could be positioned; and, (b) to capture whole body information and drill dynamics whilst allowing the natural execution of movements, avoiding obstruction or discomfort for the subject. The wearable units embed tri-axial accelerometers, gyroscopes and magnetometers and were able to synchronously communicate with the system base station via Bluetooth Low Energy (BLE) wireless protocol ensuring an acquisition rate of 148.15 Hz.</p>
<div class="DottedLine"></div>
<div class="Imageheaders">FIGURE 1</div>
<div class="FigureDesc">
<a href="https://www.frontiersin.org/files/Articles/527814/fbioe-08-00664-HTML/image_m/fbioe-08-00664-g001.jpg" name="figure1" target="_blank">
<img id="F1" alt="www.frontiersin.org" class="lazy" data-src="https://www.frontiersin.org/files/Articles/527814/fbioe-08-00664-HTML/image_m/fbioe-08-00664-g001.jpg"></a>
<p><strong>Figure 1</strong>. Experimental setup and movement tasks, where the position of IMU sensors has been highlighted. <strong>(A)</strong> &#x0201C;Clean and Jerk,&#x0201D; <strong>(B)</strong> &#x0201C;Box Jump,&#x0201D; <strong>(C)</strong> &#x0201C;American Swing,&#x0201D; and <strong>(D)</strong> &#x0201C;Burpee&#x0201D;. All the five sensors were worn by the participants throughout the execution of the protocol.</p>
</div>
<div class="clear"></div>
<div class="DottedLine"></div>
<p class="mb0 w100pc float_left mt15">Accelerations and angular velocities (&#x000B1;16 g, &#x000B1;2,000&#x000B0;/s) were acquired continuously throughout the workout by means of the wearable units; magnetometer measurements were excluded due to the presence of ferromagnetic materials, very close to the acquisition volume. Data coming from the sensors were synchronized with a commercial video-camera (Oqus Video 210c, Qualisys AB, Sweden; 50 Hz) via a dedicated trigger module (Trigger Module, Delsys Inc. USA) connected to both systems. The video camera was positioned in front of workout station, thus allowing the correct acquisition of all the performed movements.</p>
<h3>Experimental Protocol</h3>
<p class="mb15">Participants were asked to execute a workout session including four popular functional training drills (<a href="#F2">Figure 2</a>). These consisted of:</p>
<p style="margin-top:0em;margin-bottom:0em;margin-left:0.7em;text-indent:-0.7em;text-align:left">- &#x0201C;<i>Clean and Jerk</i>&#x0201D; (<i>C</i>&#x00026;<i>J</i>). A weighted barbell is lifted from the ground to over the head in two subsequent movements: the &#x0201C;clean,&#x0201D; where the barbell is pulled from the floor to a racked position across the shoulders, and &#x0201C;jerk,&#x0201D; where the barbell is raised above the head, and a stable position is achieved by keeping straight legs and arms, and feet, torso and barbell lie in the same plane.</p>
<p style="margin-top:0em;margin-bottom:0em;margin-left:0.7em;text-indent:-0.7em;text-align:left">- &#x0201C;<i>American Swing&#x0201D; (AS</i>). A kettlebell is grasped with both hands and swung from below the groin to above the head, keeping the arms straight. The upward momentum of the kettlebell is predominantly generated by the explosive extension of the hip.</p>
<p style="margin-top:0em;margin-bottom:0em;margin-left:0.7em;text-indent:-0.7em;text-align:left">- &#x0201C;<i>Box Jump&#x0201D; (BJ)</i>. The participant start from a standing position in front of a box, performs a countermovement jump to land on top of it, achieves a stable upright position, and completes the task by returning to the start position.</p>
<p style="margin-top:0em;margin-bottom:1em;margin-left:0.7em;text-indent:-0.7em;text-align:left">- &#x0201C;<i>Burpee&#x0201D; (BP)</i>. A four-stage exercise, where the participant starts from a standing position, squats placing the hands on the floor, kicks back into a plank position while keeping the arms extended, returns in the squat position and, jumps up extending the upper limbs overhead.</p>
<div class="DottedLine"></div>
<div class="Imageheaders">FIGURE 2</div>
<div class="FigureDesc">
<a href="https://www.frontiersin.org/files/Articles/527814/fbioe-08-00664-HTML/image_m/fbioe-08-00664-g002.jpg" name="figure2" target="_blank">
<img id="F2" alt="www.frontiersin.org" class="lazy" data-src="https://www.frontiersin.org/files/Articles/527814/fbioe-08-00664-HTML/image_m/fbioe-08-00664-g002.jpg"></a>
<p><strong>Figure 2</strong>. Schematic representation of the execution stages of the fitness training drills used in this study: <strong>(A)</strong> &#x0201C;Clean and Jerk,&#x0201D; <strong>(B)</strong> &#x0201C;American Swing,&#x0201D; <strong>(C)</strong> &#x0201C;Box Jump,&#x0201D; and <strong>(D)</strong> &#x0201C;Burpee&#x0201D;.</p>
</div>
<div class="clear"></div>
<div class="DottedLine"></div>
<p class="mb15 w100pc float_left mt15">All the movement tasks were illustrated to the participants at the start of the session, following the standards approved for competition (<a href="#B12">CrossFit, 2019</a>; <a href="#B42">WODstar, 2019</a>). A 50 cm box was used in the Box Jump exercise for all participants, whereas drills with an added resistance were differentiated between female and male participants, and set to, respectively: 20 and 40 kg in the Clean &#x00026; Jerk; 12 and 16 kg in the American Swing.</p>
<p class="mb15">After a self-directed warm up, and some repetitions to familiarize with the experimental setup, each participant performed 3 sets of functional fitness activities structured as follows:</p>
<p class="mb15">Set 1 (classifier training dataset):</p>
<p style="margin-top:0em;margin-bottom:0em;margin-left:0.7em;text-indent:-0.7em;text-align:left">- 3 &#x000D7; C&#x00026;J &#x0002B; 3 &#x000D7; BJ &#x0002B; 3 &#x000D7; AS &#x0002B; 3 &#x000D7; BP</p>
<p class="mb15">Set 2 (workout simulation session, classifier test dataset):</p>
<p style="margin-top:0em;margin-bottom:0em;margin-left:0.7em;text-indent:-0.7em;text-align:left">- <i>1st Round:</i> 1 &#x000D7; C&#x00026;J &#x0002B; 1 &#x000D7; BJ &#x0002B; 1 &#x000D7; AS &#x0002B; 1 &#x000D7; BP</p>
<p style="margin-top:0em;margin-bottom:0em;margin-left:0.7em;text-indent:-0.7em;text-align:left">- <i>2nd Round:</i> 2 &#x000D7; C&#x00026;J &#x0002B; 2 &#x000D7; BJ &#x0002B; 2 &#x000D7; AS &#x0002B; 2 &#x000D7; BP</p>
<p style="margin-top:0em;margin-bottom:0em;margin-left:0.7em;text-indent:-0.7em;text-align:left">- <i>3rd Round:</i> 3 &#x000D7; C&#x00026;J &#x0002B; 3 &#x000D7; BJ &#x0002B; 3 &#x000D7; AS &#x0002B; 3 &#x000D7; BP</p>
<p style="margin-top:0em;margin-bottom:1em;margin-left:0.7em;text-indent:-0.7em;text-align:left">- <i>4th Round:</i> 4 &#x000D7; C&#x00026;J &#x0002B; 4 &#x000D7; BJ &#x0002B; 4 &#x000D7; AS &#x0002B; 4 &#x000D7; BP</p>
<p class="mb15">Set 3 (classifier training dataset):</p>
<p style="margin-top:0em;margin-bottom:0em;margin-left:0.7em;text-indent:-0.7em;text-align:left">- 3 &#x000D7; C&#x00026;J &#x0002B; 3 &#x000D7; BJ &#x0002B; 3 &#x000D7; AS &#x0002B; 3 &#x000D7; BP</p>
<p class="mb15">Five-minute recovery was allowed between sets, whereas movements were executed sequentially with no rest allowed between repetitions of the same exercise, different exercises or rounds. This was done to ensure ecological validity with respect to a real functional fitness training session, and to challenge the capability of the classifying algorithm to recognize movements when they are performed without clear breaks in-between them. The order of movement execution was randomized between participants, to avoid possible bias due to repetitive patterns.</p>
<p class="mb0">The workout simulation (Set 2) was preceded (Set 1) and followed (Set 3) by a sequence of three repetitions of each task. The pre- and post-workout session were used as training sets for the machine learning algorithms, and were both included so that the classification method could be robust to the possible changes in movement execution caused by fatigue or learning effects in the participants.</p>
<h3>Data Analysis</h3>
<p class="mb0">The three components of acceleration and angular velocity from the five IMUs (6 &#x000D7; 5 = 30 continuous timeseries) were used as input data for the classifying algorithm. Kinematic quantities were not filtered, and frequency-domain signals were attained through transforming the time-domain signals via Fast Fourier Transform. Data features in the time and frequency domain (<a href="#T1">Table 1</a>) were extracted from data windows moving across the original kinematics timeseries. This process aimed to reduce the signals into distinctive characteristics of specific movement tasks or part of them. The more each movement can be separated in feature space, the higher the achieved recognition and classification performance is <a href="#B43">Zhang and Sawchuk (2013)</a> and <a href="#B21">Hoettinger et al. (2016)</a>.</p>
<div class="DottedLine"></div>
<div class="Imageheaders">TABLE 1</div>
<div class="FigureDesc">
<a href="https://www.frontiersin.org/files/Articles/527814/fbioe-08-00664-HTML/image_m/fbioe-08-00664-t001.jpg" name="Table1" target="_blank">
<img id="T1" alt="www.frontiersin.org" class="lazy" data-src="https://www.frontiersin.org/files/Articles/527814/fbioe-08-00664-HTML/image_m/fbioe-08-00664-t001.jpg">
</a>
<p><strong>Table 1</strong>. Features extracted from each time window of each signal collected (from accelerometers and gyroscopes), and then used as input of the classification algorithm.</p>
</div>
<div class="clear"></div>
<div class="DottedLine"></div>
<p class="mt15 w100pc float_left">To set suitable ranges for window duration and decide the amount of window overlap, we analyzed the distribution of movement durations across the population (<a href="#F3">Figure 3</a>). Values between 300 and 600 ms (in increments of 100 ms) for window length, and of 0, 10, and 20% for the amount of overlap were chosen to study the sensitivity of the classification to the choice of windowing parameters. This allowed to have at least three time windows covering the execution of each movement or the transitions between subsequent movements. A [N &#x000D7; 540] feature matrix was generated for each participant, where N indicates the number of time windows in each session, and 540 is the overall number of features included in the analysis (5 sensors &#x000D7; 2 kinematic quantities per sensor &#x000D7; 3 directions per quantity &#x000D7;18 features per quantity).</p>
<div class="DottedLine"></div>
<div class="Imageheaders">FIGURE 3</div>
<div class="FigureDesc">
<a href="https://www.frontiersin.org/files/Articles/527814/fbioe-08-00664-HTML/image_m/fbioe-08-00664-g003.jpg" name="figure3" target="_blank">
<img id="F3" alt="www.frontiersin.org" class="lazy" data-src="https://www.frontiersin.org/files/Articles/527814/fbioe-08-00664-HTML/image_m/fbioe-08-00664-g003.jpg"></a>
<p><strong>Figure 3</strong>. Distribution of individual movement durations in the analyzed population. C&#x00026;J, Clean and Jerk; AS, American Swing; BJ, Box Jump; BP, Burpee.</p>
</div>
<div class="clear"></div>
<div class="DottedLine"></div>
<h4>Data Labeling</h4>
<p class="mb0">A supervised approach to automatic classification was adopted, with video-based classification used as the gold standard for labeling each data window as a transition phase or as a part of one of the four possible functional fitness movements (<a href="#F2">Figure 2</a>). Camera footage was used to identify the start and end of each movement and for their classification (i.e., labeling), as required by the supervised learning model. Movement recognition, timing and labeling were visually carried out by a single expert operator using freeware video editing software (VirtualDub, virtualdub.org). When a window spanned between a transition phase and one of the four movement tasks, a &#x0201C;majory&#x0201D; criterion was used. This implied assigning a movement label (i.e., &#x0201C;C&#x00026;J,&#x0201D; &#x0201C;AS,&#x0201D; &#x0201C;BJ,&#x0201D; or &#x0201C;BP&#x0201D;) to a window where the movement covered more than 66% of its length. Otherwise, the transition label (i.e., &#x0201C;TRANS&#x0201D;) was allocated.</p>
<h4>Classifier Training</h4>
<p class="mb0">After the extraction of the features and the labeling of associated windows, we trained different type of automatic classifiers using data Set 1 and 3. k-Nearest Neighbors (kNN), with different types of metrics (Euclidean, cosine, cubic or weighted distance) and number of neighbors (fine, k = 1; medium, k = 10; and coarse, k = 100), and Support Vector Machine (SVM) with several types of kernel functions (i.e., linear, quadratic, cubic and fine-medium-coarse Gaussian), were selected as the classifying algorithms to be tested. This choice was driven by the existing literature in the area of machine learning approaches addressing human motion (<a href="#B8">Camomilla et al., 2018</a>; <a href="#B13">Cust et al., 2019</a>) and sport (<a href="#B13">Cust et al., 2019</a>) classification. At this stage, all the reported features (<a href="#T1">Table 1</a>) were used to train the models.</p>
<h4>Classifier Assessment</h4>
<p class="mb15">Two levels of classifier evaluation were carried out. Firstly (Stage 1), we performed a 5-fold cross-validation on the classifier training dataset (N = 14 participants, Set 1 and 3); this approach was used to mitigate the risk of overfitting by partitioning the dataset into k-folds and estimating the accuracy of each fold (<a href="#B38">Taha et al., 2018</a>). We used this stage to select the most promising algorithm amongst the many tested. Finally (Stage 2), assessed the classifier performance on new data (i.e., the workout simulation dataset, Set 2) in a Leave-On-Subject-Out (LOSO) fashion (<a href="#B19">Hagenbuchner et al., 2015</a>; <a href="#B41">Willetts et al., 2018</a>). In this stage, the classifier was trained with data from Set 1 and 3 (including N-1 participants), and validated against data of the N-th participant, from Set 2; the N-th participant was iteratively changed, and results were reported averaging the multiple iterations. This approach guaranteed having independent data, in terms of both trials and individuals, between training and testing sets.</p>
<p class="mb0">Classification accuracy (Equation 1) was evaluated as follows (<a href="#B21">Hoettinger et al., 2016</a>; <a href="#B14">Davila et al., 2017</a>):</p>
<div class="equationImageholder"><math id="M1"><mtable class="eqnarray" columnalign="left"><mtr><mtd><mi mathsize="10.5pt" mathcolor="black">A</mi><mi mathsize="10.5pt" mathcolor="black">c</mi><mi mathsize="10.5pt" mathcolor="black">c</mi><mi mathsize="10.5pt" mathcolor="black">u</mi><mi mathsize="10.5pt" mathcolor="black">r</mi><mi mathsize="10.5pt" mathcolor="black">a</mi><mi mathsize="10.5pt" mathcolor="black">c</mi><mi mathsize="10.5pt" mathcolor="black">y</mi><mo mathsize="10.5pt" mathcolor="black">=</mo><mfrac><mrow><mi mathsize="10.5pt" mathcolor="black">T</mi><mi mathsize="10.5pt" mathcolor="black">P</mi><mo mathsize="10.5pt" mathcolor="black">&#x0002B;</mo><mi mathsize="10.5pt" mathcolor="black">T</mi><mi mathsize="10.5pt" mathcolor="black">N</mi></mrow><mrow><mi mathsize="10.5pt" mathcolor="black">T</mi><mi mathsize="10.5pt" mathcolor="black">P</mi><mo mathsize="10.5pt" mathcolor="black">&#x0002B;</mo><mi mathsize="10.5pt" mathcolor="black">T</mi><mi mathsize="10.5pt" mathcolor="black">N</mi><mo mathsize="10.5pt" mathcolor="black">&#x0002B;</mo><mi mathsize="10.5pt" mathcolor="black">F</mi><mi mathsize="10.5pt" mathcolor="black">P</mi><mo mathsize="10.5pt" mathcolor="black">&#x0002B;</mo><mi mathsize="10.5pt" mathcolor="black">F</mi><mi mathsize="10.5pt" mathcolor="black">N</mi></mrow></mfrac></mtd><mtd><mstyle class="text" mathsize="10.5pt" mathcolor="black"><mtext>&#x000A0;&#x000A0;&#x000A0;&#x000A0;</mtext></mstyle><mrow><mo mathsize="10.5pt" mathcolor="black" stretchy="false">(</mo><mn mathsize="10.5pt" mathcolor="black">1</mn><mo mathsize="10.5pt" mathcolor="black" stretchy="false">)</mo></mrow></mtd></mtr></mtable></math>
<div class="clear"></div>
</div>
<p class="mb15">where TP, TN, FP and FN represent True Positive, True Negative and False Positive, respectively.</p>
<p class="mb15">Once the optimal classifier was identified, the corresponding confusion matrix and Receiver Operating Characteristic (ROC) curves (in a multi-label &#x0201C;one-vs.-rest&#x0201D; assessment) were analyzed to assess the ability of the algorithm to recognize and correctly classify each functional fitness exercise. From the confusion matrix, for each exercise, we evaluated:</p>
<p style="margin-top:0em;margin-bottom:1em;margin-left:0.7em;text-indent:-0.7em;text-align:left">- the Positive Predictive Value (PPV), representing the precision of the classifier (Equation 2):</p>
<div class="equationImageholder"><math id="M2"><mtable class="eqnarray" columnalign="left"><mtr><mtd><mi mathsize="10.5pt" mathcolor="black">P</mi><mi mathsize="10.5pt" mathcolor="black">P</mi><mi mathsize="10.5pt" mathcolor="black">V</mi><mo mathsize="10.5pt" mathcolor="black">=</mo><mfrac><mrow><mi mathsize="10.5pt" mathcolor="black">T</mi><mi mathsize="10.5pt" mathcolor="black">P</mi></mrow><mrow><mi mathsize="10.5pt" mathcolor="black">T</mi><mi mathsize="10.5pt" mathcolor="black">P</mi><mo mathsize="10.5pt" mathcolor="black">&#x0002B;</mo><mi mathsize="10.5pt" mathcolor="black">F</mi><mi mathsize="10.5pt" mathcolor="black">P</mi></mrow></mfrac></mtd><mtd><mstyle class="text" mathsize="10.5pt" mathcolor="black"><mtext>&#x000A0;&#x000A0;&#x000A0;&#x000A0;</mtext></mstyle><mrow><mo mathsize="10.5pt" mathcolor="black" stretchy="false">(</mo><mn mathsize="10.5pt" mathcolor="black">2</mn><mo mathsize="10.5pt" mathcolor="black" stretchy="false">)</mo></mrow></mtd></mtr></mtable></math>
<div class="clear"></div>
</div>
<p style="margin-top:1em;margin-bottom:1em;margin-left:0.7em;text-indent:-0.7em;text-align:left">- the True Positive Rate (TPR), representing the sensitivity (also called recall) of the classifier (Equation 3):</p>
<div class="equationImageholder"><math id="M3"><mtable class="eqnarray" columnalign="left"><mtr><mtd><mi mathsize="10.5pt" mathcolor="black">T</mi><mi mathsize="10.5pt" mathcolor="black">P</mi><mi mathsize="10.5pt" mathcolor="black">R</mi><mo mathsize="10.5pt" mathcolor="black">=</mo><mfrac><mrow><mi mathsize="10.5pt" mathcolor="black">T</mi><mi mathsize="10.5pt" mathcolor="black">P</mi></mrow><mrow><mi mathsize="10.5pt" mathcolor="black">T</mi><mi mathsize="10.5pt" mathcolor="black">P</mi><mo mathsize="10.5pt" mathcolor="black">&#x0002B;</mo><mi mathsize="10.5pt" mathcolor="black">F</mi><mi mathsize="10.5pt" mathcolor="black">N</mi></mrow></mfrac></mtd><mtd><mstyle class="text" mathsize="10.5pt" mathcolor="black"><mtext>&#x000A0;&#x000A0;&#x000A0;&#x000A0;</mtext></mstyle><mrow><mo mathsize="10.5pt" mathcolor="black" stretchy="false">(</mo><mn mathsize="10.5pt" mathcolor="black">3</mn><mo mathsize="10.5pt" mathcolor="black" stretchy="false">)</mo></mrow></mtd></mtr></mtable></math>
<div class="clear"></div>
</div> <h4>Sensitivity Analysis</h4>
<p class="mb0">Two type of sensitivity analysis were carried out: (a) the effect of window length and overlapping, where all classifier types and all the features were included; and, (b) the effect of selecting a subset of the five available IMUs, which was analyzed starting from the classifier previously identified as giving the best outcome performance (as highlighted in validation Stage 1). For (b), the analysis was carried out starting from the data provided by each sensor in isolation and by considering data from pairs of sensors, as follows:</p>
<p style="margin-top:1em;margin-bottom:0em;margin-left:0.7em;text-indent:-0.7em;text-align:left">- wrist and ankle;</p>
<p style="margin-top:0em;margin-bottom:0em;margin-left:0.7em;text-indent:-0.7em;text-align:left">- wrist and lumbar area;</p>
<p style="margin-top:0em;margin-bottom:0em;margin-left:0.7em;text-indent:-0.7em;text-align:left">- wrist and thigh;</p>
<p style="margin-top:0em;margin-bottom:0em;margin-left:0.7em;text-indent:-0.7em;text-align:left">- wrist and upper arm;</p>
<p style="margin-top:0em;margin-bottom:0em;margin-left:0.7em;text-indent:-0.7em;text-align:left">- upper arm and ankle;</p>
<p style="margin-top:0em;margin-bottom:0em;margin-left:0.7em;text-indent:-0.7em;text-align:left">- upper arm and lumbar area;</p>
<p style="margin-top:0em;margin-bottom:1em;margin-left:0.7em;text-indent:-0.7em;text-align:left">- upper arm and thigh.</p>
<h4>Feature Selection Analysis</h4>
<p class="mb15">Once the best performing subset of the five measurement units was identified, an exploratory analysis of the most significant features extracted was carried out. We used the minimum Redundancy Maximum Relevance (mRMR) filter-based algorithm applied to the standardized feature matrix, due to its trade-off between performance and efficiency (<a href="#B30">Peng et al., 2005</a>; <a href="#B40">Wang et al., 2016</a>). To compare the overall accuracy, a fixed number of features was identified starting from the analysis of the predictor importance scores performed on the training dataset; these features were then used to train the models and to test them following Stage 2 validation.</p>
<p class="mb0">Training of the supervised learning models and analysis of classification performance were carried out through the Statistics and Machine Learning Toolbox and bespoke functions developed in Matlab (v R2019b, The Mathworks Inc.).</p>
<a id="h4" name="h4"></a><h2>Results</h2>
<h3 class="pt0">k-Fold Cross-Validation of Classifier Performance and Sensitivity Analysis: Time Window and Overlap Parameters</h3>
<p class="mb0">When data input included all the five available sensors, both SVM- and kNN-type classifiers achieved good level of overall accuracy (<a href="#T2">Tables 2</a>, <a href="#T3">3</a>, respectively). Accuracy ranged from 82.5% (SVM classifier with fine gaussian kernel, and 300 ms&#x02212;10% overlap windows) to 97.8% (cubic kernel SVM classifier, with 600 ms&#x02212;10% overlap windows).</p>
<div class="DottedLine"></div>
<div class="Imageheaders">TABLE 2</div>
<div class="FigureDesc">
<a href="https://www.frontiersin.org/files/Articles/527814/fbioe-08-00664-HTML/image_m/fbioe-08-00664-t002.jpg" name="Table2" target="_blank">
<img id="T2" alt="www.frontiersin.org" class="lazy" data-src="https://www.frontiersin.org/files/Articles/527814/fbioe-08-00664-HTML/image_m/fbioe-08-00664-t002.jpg">
</a>
<p><strong>Table 2</strong>. Overall classification performance (accuracy, in %) for Support Vector Machine (SVM) algorithms, as a factor of different kernel functions, window lengths, and percentage of window overlap.</p>
</div>
<div class="clear"></div>
<div class="DottedLine mb15"></div>
<div class="Imageheaders">TABLE 3</div>
<div class="FigureDesc">
<a href="https://www.frontiersin.org/files/Articles/527814/fbioe-08-00664-HTML/image_m/fbioe-08-00664-t003.jpg" name="Table3" target="_blank">
<img id="T3" alt="www.frontiersin.org" class="lazy" data-src="https://www.frontiersin.org/files/Articles/527814/fbioe-08-00664-HTML/image_m/fbioe-08-00664-t003.jpg">
</a>
<p><strong>Table 3</strong>. Overall classification performance (accuracy, in %) for k-Nearest Neighbors (kNN) algorithms, as a factor of different kernel functions, window lengths and percentage of window overlap.</p>
</div>
<div class="clear"></div>
<div class="DottedLine"></div>
<h3>Testing SVM Performance With Training and Test Datasets</h3>
<p class="mb0">Considering the overall accuracy, the training time (a ratio of more than 20 between the slowest and the fastest classifier) and the computational costs (a ratio of more than 80 between the fastest and slowest classifiers, in terms of prediction speed), the SVM with cubic kernel applied to 600 ms&#x02212;10% overlap windows appeared as the optimal learning model. The confusion matrix for this classifier (<a href="#T4">Table 4</a>) showed that the trained model yielded to almost no (validation Stage 1) or few (validation Stage 2) misclassifications between different functional fitness movements. Specific accuracy ranged from 99.7% for burpees in the 5-fold cross-validation to 94.3% for the transition phase when tested on new data. All but one erroneous classification in the 5-fold cross-validation were from movement tasks identified as transition phases (64, 1.6% of the total) and, less frequently, from transitions confused for functional fitness drills (19, 0.5%). We had up to 18.9% of false negative rates in the AS drill, which reported the lowest level of precision (93.0%) and sensitivity (81.1%) (<a href="#T5">Table 5</a>). Similar outcomes, but with lower percentage values, were reported by the LOSO validation on the test dataset. Precision and sensitivity values were always highest in the transition movements (94.9 and 97.8%, respectively), whereas the Clean &#x00026; Jerk (89.3 and 82.2%) and American Swing (93.0 and 79.3%) showed the lowest performance results (<a href="#T5">Table 5</a>).</p>
<div class="DottedLine"></div>
<div class="Imageheaders">TABLE 4</div>
<div class="FigureDesc">
<a href="https://www.frontiersin.org/files/Articles/527814/fbioe-08-00664-HTML/image_m/fbioe-08-00664-t004.jpg" name="Table4" target="_blank">
<img id="T4" alt="www.frontiersin.org" class="lazy" data-src="https://www.frontiersin.org/files/Articles/527814/fbioe-08-00664-HTML/image_m/fbioe-08-00664-t004.jpg">
</a>
<p><strong>Table 4</strong>. Confusion matrixes for the cubic kernel SVM algorithm with a 600 ms window length and 10% overlap.</p>
</div>
<div class="clear"></div>
<div class="DottedLine mb15"></div>
<div class="Imageheaders">TABLE 5</div>
<div class="FigureDesc">
<a href="https://www.frontiersin.org/files/Articles/527814/fbioe-08-00664-HTML/image_m/fbioe-08-00664-t005.jpg" name="Table5" target="_blank">
<img id="T5" alt="www.frontiersin.org" class="lazy" data-src="https://www.frontiersin.org/files/Articles/527814/fbioe-08-00664-HTML/image_m/fbioe-08-00664-t005.jpg">
</a>
<p><strong>Table 5</strong>. Accuracy (ACC), precision (PPV, Positive Predictive Value) and sensitivity (TPR, True Positive Rate) for each functional fitness movement, related to the cubic kernel SVM algorithm with a 600 ms length windows and 10% overlap.</p>
</div>
<div class="clear"></div>
<div class="DottedLine"></div>
<p class="mb0 w100pc float_left mt15">The analysis of ROC curves gives us the power of our classifier in a multi-label classification problem, as a function of the Type I error (i.e., 1&#x02014;specificity), as it was a binary predictor. Considering the validation stages, the selected SVM classifier showed an almost null value for FPR in each functional fitness movement (&#x0003C;1%), with the TPR ranging from 84% (BJ classification) to 96% (BP classification). The highest value of TPR was reached in the classification of transition phases (99%), although, in TRANS the classifier also reported the highest level of FPR (7%). Finally, the Area Under the Curve (AUC), which describes the capability of the supervised learning model to distinguish between one class and the others, ranged between 0.98 and 1, therefore showing good overall classification performances.</p>
<h3>Sensitivity Analysis: Number of Sensors</h3>
<p class="mb0">When considering the data coming from a single sensor, the selected SVM classifier achieved good values of recognition rates in most cases (<a href="#T6">Table 6</a>), with an overall accuracy between 83.2% (data from the ankle sensor, validation Stage 2) and 96.4% (data from the upper arm sensor, cross-fold validation). Using input data from pairs of IMUs generally improved the overall classification accuracy, pushing it up of several percentage point when testing on new data (Stage 2: from 83.2&#x02212;91.0% to 92.0&#x02212;93.0%). However, using two sensors did not match the performances obtained when data from all the sensors were utilized (93.0 vs. 97.8%).</p>
<div class="DottedLine"></div>
<div class="Imageheaders">TABLE 6</div>
<div class="FigureDesc">
<a href="https://www.frontiersin.org/files/Articles/527814/fbioe-08-00664-HTML/image_m/fbioe-08-00664-t006.jpg" name="Table6" target="_blank">
<img id="T6" alt="www.frontiersin.org" class="lazy" data-src="https://www.frontiersin.org/files/Articles/527814/fbioe-08-00664-HTML/image_m/fbioe-08-00664-t006.jpg">
</a>
<p><strong>Table 6</strong>. Overall classification performance (accuracy, in %) for Support Vector Machine (SVM) algorithms, with 600 ms&#x02212;10% overlap windows.</p>
</div>
<div class="clear"></div>
<div class="DottedLine"></div>
<p class="mt15 w100pc float_left">In relation to the contribution of each sensor to the correct classification of individual functional drills, including data from the sensor placed on the upper limb (upper arm or wrist), or from a combination of a sensor on the upper limb and a sensor on the lumbar area or thigh, seemed to improve classifier performance, in at least 3 out of 4 movements and in the transition phases (<a href="#T7">Tables 7</a>, <a href="#T8">8</a>). Only in the AS, the classifier seemed to perform relatively better when using data from the sensor placed on the lumbar spine (single sensor configuration). The worst overall performance was obtained when considering the data acquired by the only sensor placed on the ankle. Only for the AS, the algorithm did worse considering the data registered by the sensor placed on the wrist.</p>
<div class="DottedLine"></div>
<div class="Imageheaders">TABLE 7</div>
<div class="FigureDesc">
<a href="https://www.frontiersin.org/files/Articles/527814/fbioe-08-00664-HTML/image_m/fbioe-08-00664-t007.jpg" name="Table7" target="_blank">
<img id="T7" alt="www.frontiersin.org" class="lazy" data-src="https://www.frontiersin.org/files/Articles/527814/fbioe-08-00664-HTML/image_m/fbioe-08-00664-t007.jpg">
</a>
<p><strong>Table 7</strong>. Accuracy (ACC), precision (PPV, Positive Predictive Value) and sensitivity (TPR, True Positive Rate) for each functional fitness movement, related to the cubic kernel SVM algorithm with a 600 ms length windows and 10% overlap.</p>
</div>
<div class="clear"></div>
<div class="DottedLine mb15"></div>
<div class="Imageheaders">TABLE 8</div>
<div class="FigureDesc">
<a href="https://www.frontiersin.org/files/Articles/527814/fbioe-08-00664-HTML/image_m/fbioe-08-00664-t008.jpg" name="Table8" target="_blank">
<img id="T8" alt="www.frontiersin.org" class="lazy" data-src="https://www.frontiersin.org/files/Articles/527814/fbioe-08-00664-HTML/image_m/fbioe-08-00664-t008.jpg">
</a>
<p><strong>Table 8</strong>. Accuracy (ACC), precision (PPV, Positive Predictive Value) and sensitivity (TPR, True Positive Rate) for each functional fitness movement, related to the cubic kernel SVM algorithm with a 600 ms length windows and 10% overlap.</p>
</div>
<div class="clear"></div>
<div class="DottedLine"></div>
<h3>Feature Selection Analysis</h3>
<p class="mb15">From the sensitivity analysis we identified two configurations to be further tested by using the feature selection. We considered the data collected by the sensor on the upper arm (UA configuration, for a total of 108 features) and by the combination of sensors on the upper arm and thigh (UA&#x0002B;T configuration, for a total of 216 features). After a qualitative analysis of the trend in prediction scores, from the most important predictor to the less significant, we set the number of the features to keep to 20.</p>
<p class="mb0">The reduction of the number of the features did not compromise the overall accuracy of the classifier, thus underlining the reliability of the approach. In particular, the highest value of accuracy was maintained when considering the UA configuration (99.1% for BJ), whereas UA&#x0002B;T configuration reported a reduction of only 0.3% (99.1 vs. 98.8%) (<a href="#T9">Table 9</a>). Furthermore, in both configurations, all the values of accuracy were &#x0003E;89.5% (TRANS in UA). Larger differences concerned precision and recall in classifying the AS task, which decreased to 59.3% and 49.6% (UA), and 71.9% and 62.6% (UA&#x0002B;T), respectively. For AS alone, both PPV and TPR decreased by 15&#x02013;20%, showing risk of misclassification.</p>
<div class="DottedLine"></div>
<div class="Imageheaders">TABLE 9</div>
<div class="FigureDesc">
<a href="https://www.frontiersin.org/files/Articles/527814/fbioe-08-00664-HTML/image_m/fbioe-08-00664-t009.jpg" name="Table9" target="_blank">
<img id="T9" alt="www.frontiersin.org" class="lazy" data-src="https://www.frontiersin.org/files/Articles/527814/fbioe-08-00664-HTML/image_m/fbioe-08-00664-t009.jpg">
</a>
<p><strong>Table 9</strong>. Accuracy (ACC), precision (PPV, Positive Predictive Value) and sensitivity (TPR, True Positive Rate) for each functional fitness movement, related to the cubic kernel SVM algorithm with a 600 ms length windows and 10% overlap.</p>
</div>
<div class="clear"></div>
<div class="DottedLine"></div>
<p class="mb0 w100pc float_left mt15">Most of the identified features were time-domain features (15 out of 20 for the UA configuration and 16 out of 20 for the UA&#x0002B;T configuration), and was information coming from gyroscope data (13 out of 20 for the UA configuration and 12 out of 20 for the UA&#x0002B;T configuration). In UA&#x0002B;T, the identified features were equally spread between the sensor placed on the upper arm and on the thigh (10 out of 20, each).</p>
<a id="h5" name="h5"></a><h2>Discussion</h2>
<p class="mb15">We developed and tested a supervised learning approach to recognizing and classifying functional fitness movements within a continuous workout, combining four different drills. Accelerations and angular velocities from a set of wearable inertia sensors were used as input of the classifier. Different machine learning algorithms, time segmentation strategies and combination of sensors were assessed. Classification accuracy was generally high in both Support Vector Machine (SVM) and k-Nearest Neighbors approaches (&#x0003E;82.5% in the worst case); the SVM model with cubic kernel and applied to 600 ms&#x02212;10% overlap data windows gave the best performance overall (94.4&#x02013;97.8% accuracy, depending on the type of validation carried out). Information coming from sensors from the upper limb, alone or in combination with a wearable unit in the lumbar area or on the thigh, appeared to be key to achieve optimal classification performance.</p>
<p class="mb15">By using SVM on the whole dataset, misclassifications (as False Negative Rate&#x02014;NFR) were lower in the &#x0201C;Transition&#x0201D; phase (0.6&#x02013;2.2%) and higher in the other four drills, particularly in the &#x0201C;American Swing&#x0201D; (18.9&#x02013;20.7%). <i>Ex post</i> analysis highlighted that the higher percentages of errors could be typically related to three main factors. (1) The overall smaller number of windows associated with functional movements as opposed to transitions. (2) The choice made for the &#x0201C;majority&#x0201D; criterion, whereby up to 34% of a functional movement could still belong to a window labeled as TRANS. This may have an influence on the capability of the classifier to assign a window to one of the four drills instead of TRANS. (3) The difficulty in labeling windows as belonging to a movement or TRANS between repetitions of the same exercise, when the dynamics of the task makes it difficult to establish with certainty the start and end of the movement. Combining these three items, the problem appeared more evident for the &#x0201C;American Swing,&#x0201D; possibly for the inherent dynamics of the task.</p>
<p class="mb15">When analyzing the contribution of each sensor independently (1-sensor input) or in combination with another IMU (2-sensor input), the overall classification performance decreased of few percentage points, but still achieved an accuracy &#x0003E;83.2% in the worst case (i.e., IMU on the ankle, with the most stringent validation approach). Ankle kinematics may contain less information when feet are not moving; this situation may happen in a number of movement- and transition-related situations, such as during the &#x0201C;Clean and Jerk,&#x0201D; thus explaining the decreased performance of the classifier. In fact, collecting upper arm kinematics alone yielded 91.0&#x02013;96.4% accuracy (depending on the validation approach). Also, adding information from a second sensor generally improved the capability of the algorithm to identify classes correctly, narrowing the performance gap between using two IMUs or the whole sensor network. The best combinations resulted from adding one further IMU to one sensor on the upper arm, i.e., upper arm and lumbar area (93.0&#x02013;97.4%) or thigh (93.0&#x02013;97.6%), which further confirms the need for the system to cover the widest range of movement dynamics. Similarly to what observed for the whole sensor network, misclassifications were more common in the &#x0201C;American Swing&#x0201D; (31.8&#x02013;36.3% and 21.2&#x02013;26.3% FNR for the UA and UA&#x0002B;T configurations, respectively).</p>
<p class="mb15">To explore the translation of the selected algorithm into more easily applicable framework, a subset of features, consisting of the best 20 identified through a filter-based algorithm (mRMR), was used in a 1-sensor or 2-sensor configuration, and its classification ability tested (LOSO validation on the test dataset). The overall accuracy resulted better than 90% for all the performed task, although the confusion matrixes highlighted difficulties in distinguishing &#x0201C;similar&#x0201D; gestures (AS misclassified with TRANS). Further analysis of feature selection suggested that the most informative characteristics of the dataset were mainly related to time domain (i.e., kurtosis and skewness). Although these preliminary findings support the use of feature reduction in the pipeline of data processing, a more in-depth analysis of feature selection and outcomes derived thereof is advisable, especially for 1-sensor solutions with lower-end technology (<a href="#B16">Fan et al., 2019</a>).</p>
<p class="mb15">Supervised machine learning appeared a suitable tool for the automatic classification of different functional fitness exercises. Our study addressed a scenario that for number and type of movements involved appears more challenging than what has been assessed by other works in the field. Also, we located our sensors according to where existing consumer technologies would be placed, and not thinking of what the best configuration for motion capture would be. Despite these added complexities, our approach obtained similar performance to what reported by the literature as the current state of the art. <a href="#B17">Ghazali et al. (2018)</a> achieved 91.2% accuracy in tracking several common sporting activities such as walking, sporting, jogging sprinting and jumping. Using wearable sensors and SVM/kNN methods, <a href="#B26">Mannini and Sabatini (2010)</a> were able to distinguish between elementary physical activities such as standing, sitting, lying, walking, climbing and identify activities within sequences of sitting-standing-walking-standing-sitting with an accuracy between 97.8 and 98.3%.</p>
<p class="mb15">Within fitness activities, <a href="#B1">Adelsberger and Troster (2013)</a>, studied 16 participants performing a squat press, and via SVM managed to detect movements with 100% accuracy and differentiate between expert and beginner performance (94% accuracy). Research on weightlifting has used different approaches, mainly aiming at recognizing the type of exercise performed (<a href="#B31">Pernek et al., 2015</a>; <a href="#B20">Hausberger et al., 2016</a>; <a href="#B28">O&#x00027;Reilly et al., 2017</a>), or identifying performance metrics (e.g., quality of execution, intensity, deviation from a standard pattern) for each exercise (<a href="#B31">Pernek et al., 2015</a>; <a href="#B28">O&#x00027;Reilly et al., 2017</a>a,b,c). Approaches looking at performance metrics focus on the possibility of using personalized classifiers to monitor the quality of movement execution; they are more complex and demanding in terms of computational resources and sample sizes than what we presented in our study. On the other hand, the solutions presented in literature to address the movement recognition problem are very similar to what we have proposed. Different algorithms (such as the Random Forest&#x02014;RF&#x02014;and the Linear Discriminant Analysis&#x02014;LDA) have been explored in the existing literature, but the overall accuracies appear comparable to the values we obtained. For the size of our dataset, SVM resulted optimal in terms of both classification accuracy and training costs. RF represented an optimal solution in multi-class problem in terms of performance and computational costs, despite requiring larger datasets (<a href="#B28">O&#x00027;Reilly et al., 2017</a>), whereas LDA was reported to perform well in simple drills classification, even allowing real-time applications, when considering a single sensor (<a href="#B11">Crema et al., 2019</a>).</p>
<p class="mb0">One of the main limitations of the presented work was the reduced number of involved subjects, compared to some validated machine learning approaches found in scientific literature (<a href="#B28">O&#x00027;Reilly et al., 2017</a>,<a href="#B27">a</a>,<a href="#B29">b</a>). Our study was exploratory, and the observed sample was relatively homogeneous in terms of sporting abilities. Having access to a larger and more varied group of participants would allow covering a wider spectrum of individual characteristics and, possibly, making the classifier more robust to inherent intra- and inter-subject variability (<a href="#B33">Preatoni et al., 2013</a>) in movement execution. It could also allow to distinguish between expert and novice performance and/or between different level of movement intensity. Although our sample size was relatively small for typical machine learning studies, our method achieved a classification performance not inferior to equivalent approaches applied in different sports scenarios, including simple tasks, such as walking or running, and even more complex exercise including fitness training. Another potential limit lies in the labeling procedures, which relied in the use of footage from a single 50 Hz camera. A single plane of view for four distinguished movements could make establishing their exact start and finish time more difficult. Differences in sampling rates between different systems could also add minor discrepancy in time line reconstruction. Finally, a potential bias to the assessment of classification performance could be the disproportion between the periods of transition and of functional movement execution, with the former being an order of magnitude more numerous (&#x0003E;3,000 transition windows vs. &#x0007E;100&#x02013;200 windows per each functional movement). Arguably, in our application, transitions are not static, easily detectable situations, and rather contain a spectrum of movement features that are as or even more varied than the four movements of interest. Thus, high prevalence of transition intervals should not decrease the value of the solution proposed.</p>
<a id="h6" name="h6"></a><h2>Conclusions</h2>
<p class="mb0">Our study addressed a novel issue in the area of automatic activity tracking. We used wearable sensor data of the same kind of what could be provided by modern smart technologies and obtained from body locations similar to where those technologies could be secured. Classifying functional fitness movements within a continuous workout is a non-trivial task that, to the best of our knowledge, no other research had investigated. Despite the relatively small dataset used to train the algorithm, the accuracy achieved in detecting and recognizing four popular training drills was encouraging, even considering a simpler 1-sensor or 2-sensor configuration. Reducing input data to accelerations and angular velocities provided by a single sensor did not degrade excessively the classification ability of the algorithm, which still generated an overall level of accuracy similar to what obtained from the whole dataset available. These findings are particularly interesting as commercially available devices such as smart watches and/or phones contain inertial sensors and are typically worn in similar locations (i.e., upper arm and wrist) to where IMUs were attached in our study. This work perfectly fits the current technological trend on the combined use of wearable devices and artificial intelligence to track human activities automatically (<a href="#B5">Attal et al., 2015</a>) and support sports activities (<a href="#B13">Cust et al., 2019</a>). In the longer perspective, the proposed approach could drive the development of software and applications to aid on-field coaching and judging and provide a more objective, quantitative way to evaluate movement technique and correct/safe execution of specific drills.</p>
<a id="h7" name="h7"></a><h2>Data Availability Statement</h2>
<p class="mb0">The datasets generated for this study are available on request to the corresponding author.</p>
<a id="h8" name="h8"></a><h2>Ethics Statement</h2>
<p class="mb0">The studies involving human participants were reviewed and approved by the study was approved by the Research Ethics Approval Committee for Health (REACH) of the University of Bath, with reference number EP17/18 247. The patients/participants provided their written informed consent to participate in this study.</p>
<a id="h9" name="h9"></a><h2>Consent Statement</h2>
<p class="mb0">Written informed consent was obtained from the individuals for the publication of any potentially identifiable images or data included in this article.</p>
<a id="h10" name="h10"></a><h2>Author Contributions</h2>
<p class="mb0">Original conceptualization was designed by EP, SN, and NL. SN and EP defined the setup. SN performed the acquisition and data analysis. EP, SN, and NL contributed to the original draft preparation. EP and NL finalized the manuscript. All authors contributed to the article and approved the submitted version.</p>
<a id="h11" name="h11"></a><h2>Funding</h2>
<p class="mb0">This research was partly supported by the post-graduate dissertation scheme Borse di studio per la preparazione di tesi o relazioni finali all&#x00027;estero a.a. 2018/19&#x02014;Universit&#x000E3; degli Studi di Brescia, italy.</p>
<a id="h12" name="h12"></a><h2>Conflict of Interest</h2>
<p class="mb0">The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p>
<a id="h13" name="h13"></a><h2>References</h2>
<div class="References">
<p class="ReferencesCopy1"><a name="B1" id="B1"></a> Adelsberger, R., and Troster, G. (2013). &#x0201C;Experts lift differently: classification of weight-lifting athletes,&#x0201D; in <i>2013 IEEE International Conference on Body Sensor Networks</i> (Cambridge, MA: IEEE), 1&#x02013;6. doi: 10.1109/BSN.2013.6575458</p>
<p class="ReferencesCopy2"><a href="https://doi.org/10.1109/BSN.2013.6575458" target="_blank">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=R.+Adelsberger&#x00026;author=G.+Troster+&#x00026;publication_year=2013&#x00026;title=&#x0201C;Experts+lift+differently%3A+classification+of+weight-lifting+athletes,&#x0201D;&#x00026;journal=2013+IEEE+International+Conference+on+Body+Sensor+Networks&#x00026;pages=1-6" target="_blank">Google Scholar</a></p>
</div>
<div class="References">
<p class="ReferencesCopy1"><a name="B2" id="B2"></a> Adesida, Y., Papi, E., and McGregor, A. H. (2019). Exploring the role of wearable technology in sport kinematics and kinetics: a systematic review. <i>Sensors (Switzerland)</i> 19:1597. doi: 10.3390/s19071597</p>
<p class="ReferencesCopy2"><a href="https://pubmed.ncbi.nlm.nih.gov/30987014" target="_blank">PubMed Abstract</a> | <a href="https://doi.org/10.3390/s19071597" target="_blank">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=Y.+Adesida&#x00026;author=E.+Papi&#x00026;author=A.+H.+McGregor+&#x00026;publication_year=2019&#x00026;title=Exploring+the+role+of+wearable+technology+in+sport+kinematics+and+kinetics%3A+a+systematic+review&#x00026;journal=Sensors+(Switzerland)&#x00026;volume=19&#x00026;pages=1597" target="_blank">Google Scholar</a></p>
</div>
<div class="References">
<p class="ReferencesCopy1"><a name="B3" id="B3"></a> Ahmad, M., Khan, A., Mazzara, M., and Distefano, S. (2017). Seeking optimum system settings for physical activity recognition on smartwatches. <i>arXiv1706.01720</i> 944, 220&#x02013;233. doi: 10.1007/978-3-030-17798-0_19</p>
<p class="ReferencesCopy2"><a href="https://doi.org/10.1007/978-3-030-17798-0_19" target="_blank">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=M.+Ahmad&#x00026;author=A.+Khan&#x00026;author=M.+Mazzara&#x00026;author=S.+Distefano+&#x00026;publication_year=2017&#x00026;title=Seeking+optimum+system+settings+for+physical+activity+recognition+on+smartwatches&#x00026;journal=arXiv1706.01720&#x00026;volume=944&#x00026;pages=220-233" target="_blank">Google Scholar</a></p>
</div>
<div class="References">
<p class="ReferencesCopy1"><a name="B4" id="B4"></a> Arifoglu, D., and Bouchachia, A. (2017). Activity recognition and abnormal behaviour detection with recurrent neural networks. <i>Proc. Comput. Sci.</i> 110, 86&#x02013;93. doi: 10.1016/j.procs.2017.06.121</p>
<p class="ReferencesCopy2"><a href="https://pubmed.ncbi.nlm.nih.gov/30871686" target="_blank">PubMed Abstract</a> | <a href="https://doi.org/10.1016/j.procs.2017.06.121" target="_blank">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=D.+Arifoglu&#x00026;author=A.+Bouchachia+&#x00026;publication_year=2017&#x00026;title=Activity+recognition+and+abnormal+behaviour+detection+with+recurrent+neural+networks&#x00026;journal=Proc.+Comput.+Sci.&#x00026;volume=110&#x00026;pages=86-93" target="_blank">Google Scholar</a></p>
</div>
<div class="References">
<p class="ReferencesCopy1"><a name="B5" id="B5"></a> Attal, F., Mohammed, S., Dedabrishvili, M., Chamroukhi, F., Oukhellou, L., and Amirat, Y. (2015). Physical human activity recognition using wearable sensors. <i>Sensors (Switzerland)</i> 15, 31314&#x02013;31338. doi: 10.3390/s151229858</p>
<p class="ReferencesCopy2"><a href="https://pubmed.ncbi.nlm.nih.gov/26690450" target="_blank">PubMed Abstract</a> | <a href="https://doi.org/10.3390/s151229858" target="_blank">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=F.+Attal&#x00026;author=S.+Mohammed&#x00026;author=M.+Dedabrishvili&#x00026;author=F.+Chamroukhi&#x00026;author=L.+Oukhellou&#x00026;author=Y.+Amirat+&#x00026;publication_year=2015&#x00026;title=Physical+human+activity+recognition+using+wearable+sensors&#x00026;journal=Sensors+(Switzerland)&#x00026;volume=15&#x00026;pages=31314-31338" target="_blank">Google Scholar</a></p>
</div>
<div class="References">
<p class="ReferencesCopy1"><a name="B6" id="B6"></a> Bachmann, D., Weichert, F., and Rinkenauer, G. (2018). Review of three-dimensional human-computer interaction with focus on the leap motion controller. <i>Sensors</i> 18:2194. doi: 10.3390/s18072194</p>
<p class="ReferencesCopy2"><a href="https://pubmed.ncbi.nlm.nih.gov/29986517" target="_blank">PubMed Abstract</a> | <a href="https://doi.org/10.3390/s18072194" target="_blank">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=D.+Bachmann&#x00026;author=F.+Weichert&#x00026;author=G.+Rinkenauer+&#x00026;publication_year=2018&#x00026;title=Review+of+three-dimensional+human-computer+interaction+with+focus+on+the+leap+motion+controller&#x00026;journal=Sensors&#x00026;volume=18&#x00026;pages=2194" target="_blank">Google Scholar</a></p>
</div>
<div class="References">
<p class="ReferencesCopy1"><a name="B7" id="B7"></a> Barbieri, J. F., Figueiredo, G. T., da, C., Castano, L. A. A., Guimaraes, P., dos, S., et al. (2019). A comparison of cardiorespiratory responses between crossfit<sup>&#x000AE;</sup> practitioners and recreationally trained individual. <i>J. Phys. Educ. Sport</i> 19, 1606&#x02013;1611. doi: 10.7752/jpes.2019.03233</p>
<p class="ReferencesCopy2"><a href="https://doi.org/10.7752/jpes.2019.03233" target="_blank">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=J.+F.+Barbieri&#x00026;author=G.+T.+Figueiredo&#x00026;author=C.+da&#x00026;author=L.+A.+A.+Castano&#x00026;author=P.+Guimaraes&#x00026;author=S.+dos+&#x00026;publication_year=2019&#x00026;title=A+comparison+of+cardiorespiratory+responses+between+crossfit&#x000AE;+practitioners+and+recreationally+trained+individual&#x00026;journal=J.+Phys.+Educ.+Sport&#x00026;volume=19&#x00026;pages=1606-1611" target="_blank">Google Scholar</a></p>
</div>
<div class="References">
<p class="ReferencesCopy1"><a name="B8" id="B8"></a> Camomilla, V., Bergamini, E., Fantozzi, S., and Vannozzi, G. (2018). Trends supporting the in-field use of wearable inertial sensors for sport performance evaluation: a systematic review. <i>Sensors (Switzerland)</i> 18:873. doi: 10.3390/s18030873</p>
<p class="ReferencesCopy2"><a href="https://pubmed.ncbi.nlm.nih.gov/29543747" target="_blank">PubMed Abstract</a> | <a href="https://doi.org/10.3390/s18030873" target="_blank">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=V.+Camomilla&#x00026;author=E.+Bergamini&#x00026;author=S.+Fantozzi&#x00026;author=G.+Vannozzi+&#x00026;publication_year=2018&#x00026;title=Trends+supporting+the+in-field+use+of+wearable+inertial+sensors+for+sport+performance+evaluation%3A+a+systematic+review&#x00026;journal=Sensors+(Switzerland)&#x00026;volume=18&#x00026;pages=873" target="_blank">Google Scholar</a></p>
</div>
<div class="References">
<p class="ReferencesCopy1"><a name="B9" id="B9"></a> Cheng, G., Wan, Y., Saudagar, A. N., Namuduri, K., and Buckles, B. P. (2015). <i>Advances in Human Action Recognition: a Survey</i>. Available online at: <a href="http://arxiv.org/abs/1501.05964">http://arxiv.org/abs/1501.05964</a>.</p>
<p class="ReferencesCopy2"><a href="http://scholar.google.com/scholar_lookup?author=G.+Cheng&#x00026;author=Y.+Wan&#x00026;author=A.+N.+Saudagar&#x00026;author=K.+Namuduri&#x00026;author=B.+P.+Buckles+&#x00026;publication_year=2015&#x00026;title=Advances+in+Human+Action+Recognition%3A+a+Survey" target="_blank">Google Scholar</a></p>
</div>
<div class="References">
<p class="ReferencesCopy1"><a name="B10" id="B10"></a> Chetty, G., and White, M. (2016). &#x0201C;Body sensor networks for human activity recognition,&#x0201D; in <i>3rd International Conference on Signal Processing and Integrated Networks, SPIN 2016</i> (Noida, Delhi NCR: IEEE), 660&#x02013;665. doi: 10.1109/SPIN.2016.7566779</p>
<p class="ReferencesCopy2"><a href="https://doi.org/10.1109/SPIN.2016.7566779" target="_blank">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=G.+Chetty&#x00026;author=M.+White+&#x00026;publication_year=2016&#x00026;title=&#x0201C;Body+sensor+networks+for+human+activity+recognition,&#x0201D;&#x00026;journal=3rd+International+Conference+on+Signal+Processing+and+Integrated+Networks,+SPIN+2016&#x00026;pages=660-665" target="_blank">Google Scholar</a></p>
</div>
<div class="References">
<p class="ReferencesCopy1"><a name="B11" id="B11"></a> Crema, C., Depari, A., Flammini, A., Sisinni, E., Haslwanter, T., and Salzmann, S. (2019). Characterization of a wearable system for automatic supervision of fitness exercises. <i>Measurement</i> 147:106810. doi: 10.1016/j.measurement.2019.07.038</p>
<p class="ReferencesCopy2"><a href="https://doi.org/10.1016/j.measurement.2019.07.038" target="_blank">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=C.+Crema&#x00026;author=A.+Depari&#x00026;author=A.+Flammini&#x00026;author=E.+Sisinni&#x00026;author=T.+Haslwanter&#x00026;author=S.+Salzmann+&#x00026;publication_year=2019&#x00026;title=Characterization+of+a+wearable+system+for+automatic+supervision+of+fitness+exercises&#x00026;journal=Measurement&#x00026;volume=147&#x00026;pages=106810" target="_blank">Google Scholar</a></p>
</div>
<div class="References">
<p class="ReferencesCopy1"><a name="B12" id="B12"></a> CrossFit (2019). Exercise Demos. <i>Available online at: <a href="https://www.crossfit.com/exercisedemos/">https://www.crossfit.com/exercisedemos/</a> (accessed March</i> 1, 2019).</p>
<p class="ReferencesCopy2"><a href="http://scholar.google.com/scholar_lookup?publication_year=2019&#x00026;title=Exercise+Demos&#x00026;journal=Available+online+at%3A+https%3A%2F%2Fwww.crossfit.com%2Fexercisedemos%2F+(accessed+March&#x00026;volume=1&#x00026;pages=2019" target="_blank">Google Scholar</a></p>
</div>
<div class="References">
<p class="ReferencesCopy1"><a name="B13" id="B13"></a> Cust, E. E., Sweeting, A. J., Ball, K., and Robertson, S. (2019). Machine and deep learning for sport-specific movement recognition: a systematic review of model development and performance. <i>J. Sports Sci.</i> 37, 568&#x02013;600. doi: 10.1080/02640414.2018.1521769</p>
<p class="ReferencesCopy2"><a href="https://pubmed.ncbi.nlm.nih.gov/30307362" target="_blank">PubMed Abstract</a> | <a href="https://doi.org/10.1080/02640414.2018.1521769" target="_blank">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=E.+E.+Cust&#x00026;author=A.+J.+Sweeting&#x00026;author=K.+Ball&#x00026;author=S.+Robertson+&#x00026;publication_year=2019&#x00026;title=Machine+and+deep+learning+for+sport-specific+movement+recognition%3A+a+systematic+review+of+model+development+and+performance&#x00026;journal=J.+Sports+Sci.&#x00026;volume=37&#x00026;pages=568-600" target="_blank">Google Scholar</a></p>
</div>
<div class="References">
<p class="ReferencesCopy1"><a name="B14" id="B14"></a> Davila, J. C., Cretu, A. M., and Zaremba, M. (2017). Wearable sensor data classification for human activity recognition based on an iterative learning framework. <i>Sensors (Switzerland)</i> 17:1287. doi: 10.3390/s17061287</p>
<p class="ReferencesCopy2"><a href="https://pubmed.ncbi.nlm.nih.gov/28590422" target="_blank">PubMed Abstract</a> | <a href="https://doi.org/10.3390/s17061287" target="_blank">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=J.+C.+Davila&#x00026;author=A.+M.+Cretu&#x00026;author=M.+Zaremba+&#x00026;publication_year=2017&#x00026;title=Wearable+sensor+data+classification+for+human+activity+recognition+based+on+an+iterative+learning+framework&#x00026;journal=Sensors+(Switzerland)&#x00026;volume=17&#x00026;pages=1287" target="_blank">Google Scholar</a></p>
</div>
<div class="References">
<p class="ReferencesCopy1"><a name="B15" id="B15"></a> Endres, F., Hess, J., and Burgard, W. (2012). <i>Graph-Based Action Models for Human Motion Classification. <i>Robot. 2012</i>, 1&#x02013;6</i>. Available online at: <a href="http://ais.informatik.uni-freiburg.de/publications/papers/endres12robotik.pdf">http://ais.informatik.uni-freiburg.de/publications/papers/endres12robotik.pdf</a>; <a href="https://www.vde-verlag.de/proceedings-en/453418001.html">https://www.vde-verlag.de/proceedings-en/453418001.html</a></p>
<p class="ReferencesCopy2"><a href="http://scholar.google.com/scholar_lookup?author=F.+Endres&#x00026;author=J.+Hess&#x00026;author=W.+Burgard+&#x00026;publication_year=2012&#x00026;title=Graph-Based+Action+Models+for+Human+Motion+Classification.+Robot.+2012,+1&#x02013;6" target="_blank">Google Scholar</a></p>
</div>
<div class="References">
<p class="ReferencesCopy1"><a name="B16" id="B16"></a> Fan, S., Jia, Y., and Jia, C. (2019). A feature selection and classification method for activity recognition based on an inertial sensing unit. <i>Information</i> 10:290. doi: 10.3390/info10100290</p>
<p class="ReferencesCopy2"><a href="https://doi.org/10.3390/info10100290" target="_blank">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=S.+Fan&#x00026;author=Y.+Jia&#x00026;author=C.+Jia+&#x00026;publication_year=2019&#x00026;title=A+feature+selection+and+classification+method+for+activity+recognition+based+on+an+inertial+sensing+unit&#x00026;journal=Information&#x00026;volume=10&#x00026;pages=290" target="_blank">Google Scholar</a></p>
</div>
<div class="References">
<p class="ReferencesCopy1"><a name="B17" id="B17"></a> Ghazali, N. F., Shahar, N., Rahmad, N. A., Sufri, N. A. J., As&#x00027;ari, M. A., and Latif, H. F. M. (2018). &#x0201C;Common sport activity recognition using inertial sensor,&#x0201D; in <i>Proc. - 2018 IEEE 14th Int. Colloq. Signal Process. Its Appl. (CSPA)</i> (Batu Feringghi), 67&#x02013;71. doi: 10.1109/CSPA.2018.8368687</p>
<p class="ReferencesCopy2"><a href="https://doi.org/10.1109/CSPA.2018.8368687" target="_blank">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=N.+F.+Ghazali&#x00026;author=N.+Shahar&#x00026;author=N.+A.+Rahmad&#x00026;author=N.+A.+J.+Sufri&#x00026;author=M.+A.+As&#x00027;ari&#x00026;author=H.+F.+M.+Latif+&#x00026;publication_year=2018&#x00026;title=&#x0201C;Common+sport+activity+recognition+using+inertial+sensor,&#x0201D;&#x00026;journal=Proc.+-+2018+IEEE+14th+Int.+Colloq.+Signal+Process.+Its+Appl.+(CSPA)&#x00026;pages=67-71" target="_blank">Google Scholar</a></p>
</div>
<div class="References">
<p class="ReferencesCopy1"><a name="B18" id="B18"></a> Gianzina, E. A., and Kassotaki, O. A. (2019). The benefits and risks of the high-intensity crossfit training. <i>Sport Sci. Health</i> 15, 21&#x02013;33. doi: 10.1007/s11332-018-0521-7</p>
<p class="ReferencesCopy2"><a href="https://doi.org/10.1007/s11332-018-0521-7" target="_blank">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=E.+A.+Gianzina&#x00026;author=O.+A.+Kassotaki+&#x00026;publication_year=2019&#x00026;title=The+benefits+and+risks+of+the+high-intensity+crossfit+training&#x00026;journal=Sport+Sci.+Health&#x00026;volume=15&#x00026;pages=21-33" target="_blank">Google Scholar</a></p>
</div>
<div class="References">
<p class="ReferencesCopy1"><a name="B19" id="B19"></a> Hagenbuchner, M., Cliff, D. P., Trost, S. G., Van Tuc, N., and Peoples, G. E. (2015). Prediction of activity type in preschool children using machine learning techniques. <i>J. Sci. Med. Sport</i> 18, 426&#x02013;431. doi: 10.1016/j.jsams.2014.06.003</p>
<p class="ReferencesCopy2"><a href="https://pubmed.ncbi.nlm.nih.gov/25088983" target="_blank">PubMed Abstract</a> | <a href="https://doi.org/10.1016/j.jsams.2014.06.003" target="_blank">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=M.+Hagenbuchner&#x00026;author=D.+P.+Cliff&#x00026;author=S.+G.+Trost&#x00026;author=N.+Van+Tuc&#x00026;author=G.+E.+Peoples+&#x00026;publication_year=2015&#x00026;title=Prediction+of+activity+type+in+preschool+children+using+machine+learning+techniques&#x00026;journal=J.+Sci.+Med.+Sport&#x00026;volume=18&#x00026;pages=426-431" target="_blank">Google Scholar</a></p>
</div>
<div class="References">
<p class="ReferencesCopy1"><a name="B20" id="B20"></a> Hausberger, P., Fernbach, A., and Kastner, W. (2016). &#x0201C;IMU-based smart fitness devices for weight training,&#x0201D; in <i>IECON Proc. Industrial Electron. Conf.</i> (Florenz), 5182&#x02013;5189. doi: 10.1109/IECON.2016.7793510</p>
<p class="ReferencesCopy2"><a href="https://doi.org/10.1109/IECON.2016.7793510" target="_blank">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=P.+Hausberger&#x00026;author=A.+Fernbach&#x00026;author=W.+Kastner+&#x00026;publication_year=2016&#x00026;title=&#x0201C;IMU-based+smart+fitness+devices+for+weight+training,&#x0201D;&#x00026;journal=IECON+Proc.+Industrial+Electron.+Conf.&#x00026;pages=5182-5189" target="_blank">Google Scholar</a></p>
</div>
<div class="References">
<p class="ReferencesCopy1"><a name="B21" id="B21"></a> Hoettinger, H., Mally, F., and Sabo, A. (2016). Activity recognition in surfing-a comparative study between hidden markov model and support vector machine. <i>Procedia Eng.</i> 147, 912&#x02013;917. doi: 10.1016/j.proeng.2016.06.279</p>
<p class="ReferencesCopy2"><a href="https://doi.org/10.1016/j.proeng.2016.06.279" target="_blank">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=H.+Hoettinger&#x00026;author=F.+Mally&#x00026;author=A.+Sabo+&#x00026;publication_year=2016&#x00026;title=Activity+recognition+in+surfing-a+comparative+study+between+hidden+markov+model+and+support+vector+machine&#x00026;journal=Procedia+Eng.&#x00026;volume=147&#x00026;pages=912-917" target="_blank">Google Scholar</a></p>
</div>
<div class="References">
<p class="ReferencesCopy1"><a name="B22" id="B22"></a> Howell, D. R., Oldham, J. R., DiFabio, M., Vallabhajosula, S., Hall, E. E., Ketcham, C. J., et al. (2017). Single-task and dual-task gait among collegiate athletes of different sport classifications: implications for concussion management. <i>J. Appl. Biomech.</i> 33, 24&#x02013;31. doi: 10.1123/jab.2015-0323</p>
<p class="ReferencesCopy2"><a href="https://pubmed.ncbi.nlm.nih.gov/27705076" target="_blank">PubMed Abstract</a> | <a href="https://doi.org/10.1123/jab.2015-0323" target="_blank">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=D.+R.+Howell&#x00026;author=J.+R.+Oldham&#x00026;author=M.+DiFabio&#x00026;author=S.+Vallabhajosula&#x00026;author=E.+E.+Hall&#x00026;author=C.+J.+Ketcham+&#x00026;publication_year=2017&#x00026;title=Single-task+and+dual-task+gait+among+collegiate+athletes+of+different+sport+classifications%3A+implications+for+concussion+management&#x00026;journal=J.+Appl.+Biomech.&#x00026;volume=33&#x00026;pages=24-31" target="_blank">Google Scholar</a></p>
</div>
<div class="References">
<p class="ReferencesCopy1"><a name="B23" id="B23"></a> Hsu, Y. L., Yang, S. C., Chang, H. C., and Lai, H. C. (2018). Human daily and sport activity recognition using a wearable inertial sensor network. <i>IEEE Access</i> 6, 31715&#x02013;31728. doi: 10.1109/ACCESS.2018.2839766</p>
<p class="ReferencesCopy2"><a href="https://doi.org/10.1109/ACCESS.2018.2839766" target="_blank">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=Y.+L.+Hsu&#x00026;author=S.+C.+Yang&#x00026;author=H.+C.+Chang&#x00026;author=H.+C.+Lai+&#x00026;publication_year=2018&#x00026;title=Human+daily+and+sport+activity+recognition+using+a+wearable+inertial+sensor+network&#x00026;journal=IEEE+Access&#x00026;volume=6&#x00026;pages=31715-31728" target="_blank">Google Scholar</a></p>
</div>
<div class="References">
<p class="ReferencesCopy1"><a name="B24" id="B24"></a> Jones, R. L., and Wallace, M. (2005). Another bad day at the training ground: coping with ambiguity in the coaching context. <i>Sport Educ. Soc.</i> 10, 119&#x02013;134. doi: 10.1080/1357332052000308792</p>
<p class="ReferencesCopy2"><a href="https://doi.org/10.1080/1357332052000308792" target="_blank">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=R.+L.+Jones&#x00026;author=M.+Wallace+&#x00026;publication_year=2005&#x00026;title=Another+bad+day+at+the+training+ground%3A+coping+with+ambiguity+in+the+coaching+context&#x00026;journal=Sport+Educ.+Soc.&#x00026;volume=10&#x00026;pages=119-134" target="_blank">Google Scholar</a></p>
</div>
<div class="References">
<p class="ReferencesCopy1"><a name="B25" id="B25"></a> Liebenson, C. (2006). Functional training for performance enhancement&#x02014;Part 1: the basics. <i>J. Bodyw. Mov. Ther.</i> 10, 154&#x02013;158. doi: 10.1016/j.jbmt.2006.01.003</p>
<p class="ReferencesCopy2"><a href="https://doi.org/10.1016/j.jbmt.2006.01.003" target="_blank">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=C.+Liebenson+&#x00026;publication_year=2006&#x00026;title=Functional+training+for+performance+enhancement&#x02014;Part+1%3A+the+basics&#x00026;journal=J.+Bodyw.+Mov.+Ther.&#x00026;volume=10&#x00026;pages=154-158" target="_blank">Google Scholar</a></p>
</div>
<div class="References">
<p class="ReferencesCopy1"><a name="B26" id="B26"></a> Mannini, A., and Sabatini, A. M. (2010). Machine learning methods for classifying human physical activity from on-body accelerometers. <i>Sensors</i> 10, 1154&#x02013;1175. doi: 10.3390/s100201154</p>
<p class="ReferencesCopy2"><a href="https://pubmed.ncbi.nlm.nih.gov/22205862" target="_blank">PubMed Abstract</a> | <a href="https://doi.org/10.3390/s100201154" target="_blank">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=A.+Mannini&#x00026;author=A.+M.+Sabatini+&#x00026;publication_year=2010&#x00026;title=Machine+learning+methods+for+classifying+human+physical+activity+from+on-body+accelerometers&#x00026;journal=Sensors&#x00026;volume=10&#x00026;pages=1154-1175" target="_blank">Google Scholar</a></p>
</div>
<div class="References">
<p class="ReferencesCopy1"><a name="B27" id="B27"></a> O&#x00027;Reilly, M. A., Whelan, D. F., Ward, T. E., Delahunt, E., and Caulfield, B. (2017a). Classification of lunge biomechanics with multiple and individual inertial measurement units. <i>Sport. Biomech.</i> 16, 342&#x02013;360. doi: 10.1080/14763141.2017.1314544</p>
<p class="ReferencesCopy2"><a href="https://pubmed.ncbi.nlm.nih.gov/28523981" target="_blank">PubMed Abstract</a> | <a href="https://doi.org/10.1080/14763141.2017.1314544" target="_blank">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=M.+A.+O&#x00027;Reilly&#x00026;author=D.+F.+Whelan&#x00026;author=T.+E.+Ward&#x00026;author=E.+Delahunt&#x00026;author=B.+Caulfield+&#x00026;publication_year=2017a&#x00026;title=Classification+of+lunge+biomechanics+with+multiple+and+individual+inertial+measurement+units&#x00026;journal=Sport.+Biomech.&#x00026;volume=16&#x00026;pages=342-360" target="_blank">Google Scholar</a></p>
</div>
<div class="References">
<p class="ReferencesCopy1"><a name="B28" id="B28"></a> O&#x00027;Reilly, M. A., Whelan, D. F., Ward, T. E., Delahunt, E., and Caulfield, B. (2017c). Technology in strength and conditioning tracking lower-limb exercises with wearable sensors. <i>J. Strength Cond. Res.</i> 31, 1726&#x02013;1736. doi: 10.1519/JSC.0000000000001852</p>
<p class="ReferencesCopy2"><a href="https://doi.org/10.1519/JSC.0000000000001852" target="_blank">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=M.+A.+O&#x00027;Reilly&#x00026;author=D.+F.+Whelan&#x00026;author=T.+E.+Ward&#x00026;author=E.+Delahunt&#x00026;author=B.+Caulfield+&#x00026;publication_year=2017c&#x00026;title=Technology+in+strength+and+conditioning+tracking+lower-limb+exercises+with+wearable+sensors&#x00026;journal=J.+Strength+Cond.+Res.&#x00026;volume=31&#x00026;pages=1726-1736" target="_blank">Google Scholar</a></p>
</div>
<div class="References">
<p class="ReferencesCopy1"><a name="B29" id="B29"></a> O&#x00027;Reilly, M. A., Whelan, D. F., Ward, T. E., Delahunt, E., and Caulfield, B. M. (2017b). Classification of deadlift biomechanics with wearable inertial measurement units. <i>J. Biomech.</i> 58, 155&#x02013;161. doi: 10.1016/j.jbiomech.2017.04.028</p>
<p class="ReferencesCopy2"><a href="https://pubmed.ncbi.nlm.nih.gov/28545824" target="_blank">PubMed Abstract</a> | <a href="https://doi.org/10.1016/j.jbiomech.2017.04.028" target="_blank">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=M.+A.+O&#x00027;Reilly&#x00026;author=D.+F.+Whelan&#x00026;author=T.+E.+Ward&#x00026;author=E.+Delahunt&#x00026;author=B.+M.+Caulfield+&#x00026;publication_year=2017b&#x00026;title=Classification+of+deadlift+biomechanics+with+wearable+inertial+measurement+units&#x00026;journal=J.+Biomech.&#x00026;volume=58&#x00026;pages=155-161" target="_blank">Google Scholar</a></p>
</div>
<div class="References">
<p class="ReferencesCopy1"><a name="B30" id="B30"></a> Peng, H., Long, F., and Ding, C. (2005). Feature selection based on mutual information criteria of max-dependency, max-relevance, and min-redundancy. <i>IEEE Trans. Pattern Anal. Mach. Intell.</i> 27, 1226&#x02013;1238. doi: 10.1109/TPAMI.2005.159</p>
<p class="ReferencesCopy2"><a href="https://pubmed.ncbi.nlm.nih.gov/16119262" target="_blank">PubMed Abstract</a> | <a href="https://doi.org/10.1109/TPAMI.2005.159" target="_blank">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=H.+Peng&#x00026;author=F.+Long&#x00026;author=C.+Ding+&#x00026;publication_year=2005&#x00026;title=Feature+selection+based+on+mutual+information+criteria+of+max-dependency,+max-relevance,+and+min-redundancy&#x00026;journal=IEEE+Trans.+Pattern+Anal.+Mach.+Intell.&#x00026;volume=27&#x00026;pages=1226-1238" target="_blank">Google Scholar</a></p>
</div>
<div class="References">
<p class="ReferencesCopy1"><a name="B31" id="B31"></a> Pernek, I., Kurillo, G., Stiglic, G., and Bajcsy, R. (2015). Recognizing the intensity of strength training exercises with wearable sensors. <i>J. Biomed. Inform.</i> 58, 145&#x02013;155. doi: 10.1016/j.jbi.2015.09.020</p>
<p class="ReferencesCopy2"><a href="https://pubmed.ncbi.nlm.nih.gov/26453822" target="_blank">PubMed Abstract</a> | <a href="https://doi.org/10.1016/j.jbi.2015.09.020" target="_blank">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=I.+Pernek&#x00026;author=G.+Kurillo&#x00026;author=G.+Stiglic&#x00026;author=R.+Bajcsy+&#x00026;publication_year=2015&#x00026;title=Recognizing+the+intensity+of+strength+training+exercises+with+wearable+sensors&#x00026;journal=J.+Biomed.+Inform.&#x00026;volume=58&#x00026;pages=145-155" target="_blank">Google Scholar</a></p>
</div>
<div class="References">
<p class="ReferencesCopy1"><a name="B32" id="B32"></a> Popoola, O. P., and Wang, K. (2012). Video-based abnormal human behavior recognition&#x02014;a review. <i>IEEE Trans. Syst. Man, Cybern. C Appl. Rev.</i> 42, 865&#x02013;878. doi: 10.1109/TSMCC.2011.2178594</p>
<p class="ReferencesCopy2"><a href="https://doi.org/10.1109/TSMCC.2011.2178594" target="_blank">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=O.+P.+Popoola&#x00026;author=K.+Wang+&#x00026;publication_year=2012&#x00026;title=Video-based+abnormal+human+behavior+recognition&#x02014;a+review&#x00026;journal=IEEE+Trans.+Syst.+Man,+Cybern.+C+Appl.+Rev.&#x00026;volume=42&#x00026;pages=865-878" target="_blank">Google Scholar</a></p>
</div>
<div class="References">
<p class="ReferencesCopy1"><a name="B33" id="B33"></a> Preatoni, E., Hamill, J., Harrison, A. J., Hayes, K., Van Emmerik, R. E. A., Wilson, C., et al. (2013). Movement variability and skills monitoring in sports. <i>Sport. Biomech.</i> 12, 69&#x02013;92. doi: 10.1080/14763141.2012.738700</p>
<p class="ReferencesCopy2"><a href="https://pubmed.ncbi.nlm.nih.gov/23898682" target="_blank">PubMed Abstract</a> | <a href="https://doi.org/10.1080/14763141.2012.738700" target="_blank">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=E.+Preatoni&#x00026;author=J.+Hamill&#x00026;author=A.+J.+Harrison&#x00026;author=K.+Hayes&#x00026;author=R.+E.+A.+Van+Emmerik&#x00026;author=C.+Wilson+&#x00026;publication_year=2013&#x00026;title=Movement+variability+and+skills+monitoring+in+sports&#x00026;journal=Sport.+Biomech.&#x00026;volume=12&#x00026;pages=69-92" target="_blank">Google Scholar</a></p>
</div>
<div class="References">
<p class="ReferencesCopy1"><a name="B34" id="B34"></a> Quitadamo, L. R., Cavrini, F., Sbernini, L., Riillo, F., Bianchi, L., Seri, S., et al. (2017). Support vector machines to detect physiological patterns for EEG and EMG-based human&#x02013;computer interaction: a review. <i>J. Neural Eng.</i> 14:011001. doi: 10.1088/1741-2552/14/1/011001</p>
<p class="ReferencesCopy2"><a href="https://pubmed.ncbi.nlm.nih.gov/28068295" target="_blank">PubMed Abstract</a> | <a href="https://doi.org/10.1088/1741-2552/14/1/011001" target="_blank">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=L.+R.+Quitadamo&#x00026;author=F.+Cavrini&#x00026;author=L.+Sbernini&#x00026;author=F.+Riillo&#x00026;author=L.+Bianchi&#x00026;author=S.+Seri+&#x00026;publication_year=2017&#x00026;title=Support+vector+machines+to+detect+physiological+patterns+for+EEG+and+EMG-based+human&#x02013;computer+interaction%3A+a+review&#x00026;journal=J.+Neural+Eng.&#x00026;volume=14&#x00026;pages=011001" target="_blank">Google Scholar</a></p>
</div>
<div class="References">
<p class="ReferencesCopy1"><a name="B35" id="B35"></a> Rawashdeh, S. A., Rafeldt, D. A., and Uhl, T. L. (2016). Wearable IMU for shoulder injury prevention in overhead sports. <i>Sensors (Basel).</i> 16:1847. doi: 10.3390/s16111847</p>
<p class="ReferencesCopy2"><a href="https://pubmed.ncbi.nlm.nih.gov/27827880" target="_blank">PubMed Abstract</a> | <a href="https://doi.org/10.3390/s16111847" target="_blank">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=S.+A.+Rawashdeh&#x00026;author=D.+A.+Rafeldt&#x00026;author=T.+L.+Uhl+&#x00026;publication_year=2016&#x00026;title=Wearable+IMU+for+shoulder+injury+prevention+in+overhead+sports&#x00026;journal=Sensors+(Basel).&#x00026;volume=16&#x00026;pages=1847" target="_blank">Google Scholar</a></p>
</div>
<div class="References">
<p class="ReferencesCopy1"><a name="B36" id="B36"></a> Sarig Bahat, H., Takasaki, H., Chen, X., Bet-Or, Y., and Treleaven, J. (2015). Cervical kinematic training with and without interactive VR training for chronic neck pain - a randomized clinical trial. <i>Man. Ther.</i> 20, 68&#x02013;78. doi: 10.1016/j.math.2014.06.008</p>
<p class="ReferencesCopy2"><a href="https://pubmed.ncbi.nlm.nih.gov/25066503" target="_blank">PubMed Abstract</a> | <a href="https://doi.org/10.1016/j.math.2014.06.008" target="_blank">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=H.+Sarig+Bahat&#x00026;author=H.+Takasaki&#x00026;author=X.+Chen&#x00026;author=Y.+Bet-Or&#x00026;author=J.+Treleaven+&#x00026;publication_year=2015&#x00026;title=Cervical+kinematic+training+with+and+without+interactive+VR+training+for+chronic+neck+pain+-+a+randomized+clinical+trial&#x00026;journal=Man.+Ther.&#x00026;volume=20&#x00026;pages=68-78" target="_blank">Google Scholar</a></p>
</div>
<div class="References">
<p class="ReferencesCopy1"><a name="B37" id="B37"></a> Singh, S., and Saini, H. K. (2019). Effect of ten weeks of crossfit training on &#x02018;Yo-Yo test&#x00027; perfor-mance of cricketers. <i>Int. J. Recent Technol. Eng.</i> 8, 1458&#x02013;1460. doi: 10.35940/ijrte.B1269.0782S319</p>
<p class="ReferencesCopy2"><a href="https://doi.org/10.35940/ijrte.B1269.0782S319" target="_blank">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=S.+Singh&#x00026;author=H.+K.+Saini+&#x00026;publication_year=2019&#x00026;title=Effect+of+ten+weeks+of+crossfit+training+on+&#x02018;Yo-Yo+test&#x00027;+perfor-mance+of+cricketers&#x00026;journal=Int.+J.+Recent+Technol.+Eng.&#x00026;volume=8&#x00026;pages=1458-1460" target="_blank">Google Scholar</a></p>
</div>
<div class="References">
<p class="ReferencesCopy1"><a name="B38" id="B38"></a> Taha, Z., Musa, R. M., Abdul Majeed, A. P. P., Alim, M. M., and Abdullah, M. R. (2018). The identification of high potential archers based on fitness and motor ability variables: a Support vector machine approach. <i>Hum. Mov. Sci.</i> 57, 184&#x02013;193. doi: 10.1016/j.humov.2017.12.008</p>
<p class="ReferencesCopy2"><a href="https://pubmed.ncbi.nlm.nih.gov/29248809" target="_blank">PubMed Abstract</a> | <a href="https://doi.org/10.1016/j.humov.2017.12.008" target="_blank">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=Z.+Taha&#x00026;author=R.+M.+Musa&#x00026;author=A.+P.+P.+Abdul+Majeed&#x00026;author=M.+M.+Alim&#x00026;author=M.+R.+Abdullah+&#x00026;publication_year=2018&#x00026;title=The+identification+of+high+potential+archers+based+on+fitness+and+motor+ability+variables%3A+a+Support+vector+machine+approach&#x00026;journal=Hum.+Mov.+Sci.&#x00026;volume=57&#x00026;pages=184-193" target="_blank">Google Scholar</a></p>
</div>
<div class="References">
<p class="ReferencesCopy1"><a name="B39" id="B39"></a> van der Kruk, E., and Reijne, M. M. (2018). Accuracy of human motion capture systems for sport applications; state-of-the-art review. <i>Eur. J. Sport Sci.</i> 18, 806&#x02013;819. doi: 10.1080/17461391.2018.1463397</p>
<p class="ReferencesCopy2"><a href="https://pubmed.ncbi.nlm.nih.gov/29741985" target="_blank">PubMed Abstract</a> | <a href="https://doi.org/10.1080/17461391.2018.1463397" target="_blank">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=E.+van+der+Kruk&#x00026;author=M.+M.+Reijne+&#x00026;publication_year=2018&#x00026;title=Accuracy+of+human+motion+capture+systems+for+sport+applications;+state-of-the-art+review&#x00026;journal=Eur.+J.+Sport+Sci.&#x00026;volume=18&#x00026;pages=806-819" target="_blank">Google Scholar</a></p>
</div>
<div class="References">
<p class="ReferencesCopy1"><a name="B40" id="B40"></a> Wang, Y., Cang, S., and Yu, H. (2016). &#x0201C;Realization of wearable sensors-based human activity recognition with an augmented feature group,&#x0201D; in <i>22nd International Conference on Automation and Computing (ICAC)</i> (Colchester, UK), 473&#x02013;478. doi: 10.1109/IConAC.2016.7604965</p>
<p class="ReferencesCopy2"><a href="https://doi.org/10.1109/IConAC.2016.7604965" target="_blank">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=Y.+Wang&#x00026;author=S.+Cang&#x00026;author=H.+Yu+&#x00026;publication_year=2016&#x00026;title=&#x0201C;Realization+of+wearable+sensors-based+human+activity+recognition+with+an+augmented+feature+group,&#x0201D;&#x00026;journal=22nd+International+Conference+on+Automation+and+Computing+(ICAC)&#x00026;pages=473-478" target="_blank">Google Scholar</a></p>
</div>
<div class="References">
<p class="ReferencesCopy1"><a name="B41" id="B41"></a> Willetts, M., Hollowell, S., and Aslett, L. (2018). Statistical machine learning of sleep and physical activity phenotypes from sensor data in 96,220 UK Biobank participants. <i>Sci. Rep.</i> 8:7961. doi: 10.1038/s41598-018-26174-1</p>
<p class="ReferencesCopy2"><a href="https://pubmed.ncbi.nlm.nih.gov/29784928" target="_blank">PubMed Abstract</a> | <a href="https://doi.org/10.1038/s41598-018-26174-1" target="_blank">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=M.+Willetts&#x00026;author=S.+Hollowell&#x00026;author=L.+Aslett+&#x00026;publication_year=2018&#x00026;title=Statistical+machine+learning+of+sleep+and+physical+activity+phenotypes+from+sensor+data+in+96,220+UK+Biobank+participants&#x00026;journal=Sci.+Rep.&#x00026;volume=8&#x00026;pages=7961" target="_blank">Google Scholar</a></p>
</div>
<div class="References">
<p class="ReferencesCopy1"><a name="B42" id="B42"></a> WODstar (2019). WODstar Video Movement Index. Available online at: <a href="https://wodstar.com/">https://wodstar.com/</a> (accessed March 1, 2019).</p>
<p class="ReferencesCopy2"><a href="http://scholar.google.com/scholar_lookup?publication_year=2019" target="_blank">Google Scholar</a></p>
</div>
<div class="References">
<p class="ReferencesCopy1"><a name="B43" id="B43"></a> Zhang, M., and Sawchuk, A. A. (2013). Human daily activity recognition with sparse representation using wearable sensors. <i>IEEE J. Biomed. Heal. Informatics</i> 17, 553&#x02013;560. doi: 10.1109/JBHI.2013.2253613</p>
<p class="ReferencesCopy2"><a href="https://pubmed.ncbi.nlm.nih.gov/24592458" target="_blank">PubMed Abstract</a> | <a href="https://doi.org/10.1109/JBHI.2013.2253613" target="_blank">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?author=M.+Zhang&#x00026;author=A.+A.+Sawchuk+&#x00026;publication_year=2013&#x00026;title=Human+daily+activity+recognition+with+sparse+representation+using+wearable+sensors&#x00026;journal=IEEE+J.+Biomed.+Heal.+Informatics&#x00026;volume=17&#x00026;pages=553-560" target="_blank">Google Scholar</a></p>
</div>
</div>
<div class="thinLineM20"></div>
<div class="AbstractSummary">
<p><span>Keywords:</span> automatic classification, inertial measurement unit, sport, on-field testing, activity monitoring, machine learning, wearable sensors</p>
<p><span>Citation:</span> Preatoni E, Nodari S and Lopomo NF (2020) Supervised Machine Learning Applied to Wearable Sensor Data Can Accurately Classify Functional Fitness Exercises Within a Continuous Workout. <i>Front. Bioeng. Biotechnol.</i> 8:664. doi: 10.3389/fbioe.2020.00664</p>
<p id="timestamps"><span>Received:</span> 17 January 2020; <span>Accepted:</span> 28 May 2020;<br> <span>Published:</span> 07 July 2020.</p>
<div><p>Edited by:</p> <a href="https://loop.frontiersin.org/people/638591/overview">Matteo Zago</a>, Polytechnic of Milan, Italy</div>
<div><p>Reviewed by:</p> <a href="https://loop.frontiersin.org/people/886944/overview">Anwar P. P. Abdul Majeed</a>, Universiti Malaysia Pahang, Malaysia<br> <a href="https://loop.frontiersin.org/people/896765/overview">Rabiu Muazu Musa</a>, University of Malaysia Terengganu, Malaysia</div>
<p><span>Copyright</span> &#x000A9; 2020 Preatoni, Nodari and Lopomo. This is an open-access article distributed under the terms of the <a rel="license" href="http://creativecommons.org/licenses/by/4.0/" target="_blank">Creative Commons Attribution License (CC BY)</a>. The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.</p>
<p><span>&#x0002A;Correspondence:</span> Nicola Francesco Lopomo, <a href="mailto:nicola.lopomo&#x00040;unibs.it">nicola.lopomo&#x00040;unibs.it</a></p>
<div class="clear"></div>
</div>    <div class="research-topic-container" style="display: none;">
        <a href="/research-topics/10005#articles" data-test-id="relatedRT-link">
            <div class="research-topic-data">
                <h5 class="topic-title">
                    <span>This article is part of the Research Topic</span>
                </h5>
                <p style="margin-bottom:0;">

                    Machine Learning Approaches to Human Movement Analysis
                </p>
                    <div class="topic-link-trim" data-event="rt-link-click">
                        View all
27                        Articles <span style="position: relative;top: 1px;"></span>
                    </div>


            </div>
        </a>
    </div>


<!-- TODO:BEGIN IS PART OF RT-->
<script type="text/javascript">
      
            const isPartOfRTElements = document.getElementsByClassName('research-topic-container');

            if (isPartOfRTElements.length > 0) {
                const headerBar3Container = document.getElementsByClassName('header-bar-three-container');

                if (headerBar3Container.length > 0) {
                    const container = headerBar3Container[0];
                    container.appendChild(isPartOfRTElements[0]);
                    isPartOfRTElements[0].style.display = 'block';
                }
            }
       
</script>
<!-- TODO:END IS PART OF RT-->

                <div class="thin-line-dark"></div>

            </div>
        </div>
    </div>
</main>
            </div>

                <div class="side-article-subjects only-devices  widget-listing people-also-looked-at hidden">
                    <h5 class="like-h4">People also looked at</h5>
                </div>

        </div>
    </div>
</div>
<div id="divTemplates">
    <div class="modal fade modal-container impact-modal-container" data-backdrop="static" id="impactModal" tabindex="-1" role="dialog" aria-labelledby="myModalLabel" aria-hidden="true">
</div>






    <div class="modal fade modal-container supplementary-modal-container"
     data-backdrop="static"
     data-keyboard="false"
     id="supplementaryFilesModal"
     tabindex="-1" role="dialog"
     aria-labelledby="mySupplementaryModalLabel"
     aria-hidden="true"
     style="display: none; padding-right: 17px;">
</div>




<script type="text/template" id="template-supplementary-files-modal">
    <div class="supplementary-material-wrapper modal-dialog" role="document">
        <div class="modal-content">
            <div class="modal-body">
                <a class="close" data-dismiss="modal"  aria-label="Close"><span data-event="supplementaryclose-icon-click" aria-hidden="true">×</span></a>
                <h4>Supplementary Material</h4><br>
                <p class="supplementary-empty-message" style="display: none;">There is no supplementary material currently available for this article</p>
                <div class="loading-wrapper" id="loader" style="display: none;">
                    Loading supplemental data...
                    <img style="" src="/Areas/Articles/Images/Icon/loading.gif" alt="Loading..">
                </div>
                <br>
                <div class="table-responsive" id="localFiles">

                </div>
                <div id="figshare-widget-container">

                </div>
            </div>
            <br>
            <div class="modal-footer bottom-links">
                <a class="btn btn-default" data-dismiss="modal">Close</a>
            </div>
        </div>

    </div>
</script>





<script type="text/template" id="template-local-files-modal">
    <table class="table table-striped supplementary-content" id="localFilesTable">
        <thead>
            <tr>
                <th>&nbsp;</th>
                <th>File Name</th>
                <th>&nbsp;</th>
            </tr>
        </thead>
        <tbody class="file-content">
            <% _.each(supplimentalFileDetails.FileDetails, function(fileViewModel){   %>
            <tr>
                <th scope="row"><a data-test-id="<%= fileViewModel.TestId %>" href="<%= fileViewModel.FileDownloadUrl %>"><img class="img-figure" src="<%= fileViewModel.ImageUrl %>"></a></th>
                <td>
                    <a  href="<%= fileViewModel.FileDownloadUrl %>" data-event="supplementarydownload-button-click">
                        <%= fileViewModel.FileName %>
                    </a>
                </td>
                <td><a  href="<%= fileViewModel.FileDownloadUrl %>" data-event="supplementarydownload-button-click" class="btn-link"><i class="fa fa-download" aria-hidden="true"></i></a></td>
            </tr>
            <% }); %>
        </tbody>
    </table>
</script>







    <div class="modal fade modal-container notifyme-modal-container" data-backdrop="static" id="notifyModal" tabindex="-1" role="dialog" aria-labelledby="Notify on publication" aria-hidden="true">
</div>



<script type="text/template" id="template-notify-me-modal">
    <div class="modal-f page-container simple-modal" role="dialog" aria-labelledby="NotfifyMeModalLabel" id="notify_me_popup">
            <div class="modal-dialog" role="document">
                <div class="modal-content">
                    <form onsubmit="return false" id="modalnotifyme">
                        <div class="modal-header">
                            <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
                            <h3>Notify me on publication</h3>
                        </div>
                        <div class="modal-body">

                            <div class="form-group">
                                <label>Please enter your email address:</label>
                            <input type="email" required class="form-control" id="txt_notification_email_id" placeholder="Email">
                            </div>

                        <div class="g-recaptcha" id="recaptcha"></div><br />

                        <p class="small" style="color: #555;">If you already have an account, please <a href="<%=login.FrontiersLoginUrl%>?returnUrl=<%=currentPage%>" class="text-blue">login</a>.</p>
                        <p class="small" style="color: #555;">You don't have a Frontiers account ? You can <a href="<%=login.FrontiersRegistrationUrl%>" class="text-blue">register here</a>.</p>
                        </div>
                        <div class="modal-footer">
                        <button type="button" id="article_notify_non_registered_user" class="btn btn-default btn-progress" style="width: 118px; visibility:hidden;">Notify me</button>
                        </div>
                    </form>
                </div>
            </div>
        </div>
</script>

<script type="text/template" id="template-notifyme-mailselection-modal">
    <div class="modal-f page-container simple-modal" tabindex="-1" role="dialog" aria-labelledby="NotfifyMeModalLabel">
        <div class="modal-dialog" role="document">
            <div class="modal-content">
                <div class="modal-header">
                    <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
                    <h3>Select one of your emails</h3>
                </div>
                <div class="modal-body">
                    <form>
                        <div class="form-group">
                            <label>You have multiple emails registered with Frontiers:</label>
                        </div>
                                           
                        <% _.each(userEmailList, function(user){   %>
                        <input type="radio" value="<%=user%>" name="notify_Emailcollection"> <%=user%><br>
                        <% }); %>

                    </form>
                </div>
                <div class="modal-footer">
                    <button type="button" id="article_notify_loggedin_user" class="btn btn-default btn-progress" style="width: 118px;">Notify me</button>
                </div>
            </div>

        </div>

    </div>
</script>

<script type="text/template" id="template_notifyme_error_modal">
    <div class="modal-f page-container simple-modal" tabindex="-1" role="dialog" aria-labelledby="NotfifyMeModalLabel">
        <div class="modal-dialog" role="document">
            <div class="modal-content">
                <div class="modal-header">
                    <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
                    <h3>Notify me on publication</h3>
                </div>
                <div class="modal-body">
                    <form>
                        <div class="form-group">
                            <p class="text-red"><strong><%=message%></strong></p>
                        </div>
                    </form>
                </div>
                <div class="modal-footer">
                    <button type="button" data-dismiss="modal" class="btn btn-default btn-progress" style="width: 118px;">Cancel</button>
                </div>
            </div>
        </div>
    </div>
</script>







</div>

    </div>
        <a id="floating-download-pdf-btn" class="floating-button" data-event="downloadfloating-button-click" href="/articles/10.3389/fbioe.2020.00664/pdf">
            <span class="floating-button-text" data-event="downloadfloating-button-click">Download</span>
        </a>
        <script type="text/javascript">

            $(document).ready(function () {
                document.onload = observerDowloadFloatingBtn();
            });
            function downloadButtonVisibilityChange(entries) {
                const btn = entries[0];
                const floatingBtn = document.querySelector("#floating-download-pdf-btn");
                if (!btn.isIntersecting) {
                    floatingBtn.classList.add("floating-button--show");
                } else {
                    floatingBtn.classList.remove("floating-button--show");
                }
            }

            function observerDowloadFloatingBtn() {
                const options = {
                    root: document.getElementsByName('body')[0],
                    rootMargin: '0px',
                    threshold: 1.0
                }

                const observer = new IntersectionObserver(downloadButtonVisibilityChange, options);
                const target = document.querySelector('[data-test-id="download-button"]');
                observer.observe(target);
            }
        </script>


    <frontiers-footer main-domain="frontiersin.org"></frontiers-footer>

    <script src="https://crossmark.crossref.org/javascripts/v1.5/crossmark.min.js"></script>
        <script type="text/javascript">
            var $ = jQuery.noConflict(); //==>>ToDo: Avoid this..!!
        </script>

    <script src=" https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML" async></script>
 
    <script src=" https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js" async></script>
 
    <script src="https://static.frontiersin.org/areas/articles/js/frontiers?v=z1cL-Tf1-Qh97gw19mFq-3nre84X6laFLbhVr-YdIA81"></script>



    <script type="text/javascript">

        var FRArticle = (function() {
            return {
                ArticleId: '527814',
                DOI:'10.3389/fbioe.2020.00664',
                LoginUserId: '0',
                DomainId: '1',
                FieldId: '7',
                SpecialityId: '1298',
                IsPreview: FRSafe.boolean('False'),
                IsViewImpactFromLoop: FRSafe.boolean('False'),
                IsPublished: 'True',
                FigShareApiUrl: 'https://api.figshare.com/v2/collections/search',
                FigShareTimeOut: '3000',
                ProvisionallyAcceptedText: 'The final, formatted version of the article will be published soon.',
                NotifyMeButtonText: 'Notify me',
                NotifyMeButtonSecondaryText: 'We&#39;ll notify you',
                IsSimilarArticleVisible: FRSafe.boolean('True')
            };
        })();

        var FRSocial = (function() {
            return {
                itemId: 14,
                itemTypeId: 1,
                entityId: '527814',
                ownerId: '890020',
                sanPath: 'https://www.frontiersin.org/files/',
                subItemId: '8',
                loginUserId: '0',
                ownerNWDBId: '21',
                pageType: 1,
                loopUrl:'https://loop.frontiersin.org'
            };
        })();
    </script>

    <script>
        var FRAjaxSettings = (function () {

            function urlLowercase() {
                $.ajaxSetup({
                    beforeSend: function (jqXHR, settings) {
                        settings.url = settings.url.toLowerCase();
                    }
                });
            }

            return { urlLowercase: urlLowercase };
        })();

        FRAjaxSettings.urlLowercase();
    </script>

    <script>
        function setBodyOverflow(dropdownElement) {
            if (!dropdownElement) return;
            const windowWidth = window.innerWidth;
            if (windowWidth <= 1024) {
                const body = document.querySelector("body");
                if (dropdownElement.classList.contains("open")) {
                    body.style.overflow = "hidden";
                } else {
                    body.style.overflow = "auto";
                }
            }
        }
        $(document).ready(function () {
            $(document).on('click', '.dropdown-menu-wrapper', function (e) {
                e.stopPropagation();
            });

            $(document).on('click', '[data-toggle="dropdown"]', function (e) {
                const dropdownElement = e.target.closest(".dropdown")
                if (dropdownElement) {
                    setBodyOverflow(dropdownElement);
                }
            });
        });

        function supplementalDataToggle() {
            const el = document.getElementById("supplementaryFilesModal");
            if (el) {
                el.classList.toggle("in");
                if (el.classList.contains("in")) {
                    el.style.display = "block";
                } else {
                    el.style.display = "none";
                }
            }
        }

        function initToggleSupplementalData() {
            const btn = document.getElementsByClassName("btn-open-supplemental")[0];
            const btn2 = document.getElementsByClassName("btn-open-supplemental")[1];
            const closeButtons = document.getElementById("supplementaryFilesModal").querySelectorAll("[data-dismiss='modal']");

            if (btn) { btn.onclick = supplementalDataToggle; }
            if (btn2) { btn2.onclick = supplementalDataToggle; }
            if (closeButtons) { closeButtons.forEach(b => b.onclick = supplementalDataToggle); }
        }


        initToggleSupplementalData();
    </script>

    <script type="text/javascript">

        var Items = [{
            item_id: '527814',
            item_name: 'Supervised Machine Learning Applied to Wearable Sensor Data Can Accurately Classify Functional Fitness Exercises Within a Continuous Workout',
            item_list_name: 'Bioengineering and Biotechnology'
        }]

        function trackUserViewItemGA4() {
            const dataLayer = (window).dataLayer;
            if (dataLayer) {
                dataLayer.push({
                    'event': 'view_item',
                    'items': Items,
                });
            }
        }
        $(window).load(function () {
            trackUserViewItemGA4();
        });

    </script>

    <!-- BEGIN TODO UXD-1800: Inconsistent sizing of equations -->
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
          CommonHTML: {
            scale: 80
          },
          PreviewHTML: {
            scale: 80
          }
        });
    </script>
    <!-- END TODO UXD-1800: Inconsistent sizing of equations -->

    <script src="https://static.frontiersin.org/areas/articles/js/schema?v=13am1aL5JpCh_IIFOIFRdlFySxVv16JLaVykDEyHCu81"></script>


    
    <script src="https://static.frontiersin.org/areas/articles/js/app?v=GxYul3WB4OMUG8OugfHg8sHcgdpcpUHuB8ZaMamX-0M1"></script>

    <script src="https://static.frontiersin.org/areas/articles/js/webcomponents?v=LN_Z7vG7UaBnkltQzPqEgQsImefpmc8Zd4DwI8Axos41"></script>


    <script type="text/plain" class="optanon-category-C0004" src="https://widgets.figshare.com/static/figshare.js">
    </script>

</body>
</html>
