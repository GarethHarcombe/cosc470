'
@TechReport{He2015,
  author      = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  institution = {Microsoft Research},
  title       = {Deep {Residual} {Learning} for {Image} {Recognition}},
  year        = {2015},
  month       = dec,
  note        = {arXiv:1512.03385 [cs] type: article},
  abstract    = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
  annote      = {Comment: Tech report},
  doi         = {10.48550/arXiv.1512.03385},
  file        = {:He2015 - Deep Residual Learning for Image Recognition.pdf:PDF},
  keywords    = {Computer Science - Computer Vision and Pattern Recognition},
  school      = {arXiv},
  url         = {http://arxiv.org/abs/1512.03385},
  urldate     = {2023-04-19},
}

@Misc{garminGPSAccuracy,
  author       = {Garmin},
  howpublished = {Website},
  month        = apr,
  note         = {Accessed April 15, 2023},
  title        = {What Can Cause GPS Accuracy Issues on My Fitness Device?},
  year         = {2023},
  owner        = {Garmin},
  url          = {https://support.garmin.com/en-US/?faq=z0n0KE1XVF0Pe4Su8QiZgA},
}

'
@TechReport{Dumas2022,
  author      = {Dumas, Joe},
  institution = {University of Tennessee},
  title       = {Accuracy of {Garmin} {GPS} {Running} {Watches} over {Repetitive} {Trials} on the {Same} {Route}},
  year        = {2022},
  month       = mar,
  note        = {arXiv:2203.00491 [cs] type: article},
  abstract    = {Many runners use watches incorporating Global Positioning System technology to track their workouts. These devices can be valuable training aids, but they have limitations. For several reasons including variations in satellite position, environmental factors, and design decisions made by the manufacturer, GPS-enabled watches can produce position measurement errors. These can result in incorrect estimations of total distance covered as well as running pace. This study examined the accuracy of three Garmin running watches of different technological generations using repetitive trials, over several years, by the same runner over the same route. The older watches, a Forerunner 205 and a Forerunner 220, showed similar accuracy when traversing the route. The newer generation watch, a Forerunner 45S, was found to be significantly less accurate in terms of both the trueness and precision of its distance measurements. This may indicate that Garmin, in competition with other manufacturers of similar devices, has chosen in recent years to prioritize device miniaturization and battery life over accuracy.},
  doi         = {10.48550/arXiv.2203.00491},
  file        = {:Dumas2022 - Accuracy of Garmin GPS Running Watches Over Repetitive Trials on the Same Route.pdf:PDF},
  keywords    = {Computer Science - Other Computer Science},
  school      = {arXiv},
  url         = {http://arxiv.org/abs/2203.00491},
  urldate     = {2023-04-19},
}

@Misc{DepartmentofLocalGovernment2023,
  author       = {Department of Local Government, Sport and Cultural Industries},
  howpublished = {Website},
  month        = apr,
  note         = {Accessed on April 21, 2023},
  title        = {Athletics track events},
  year         = {2023},
  url          = {https://www.dlgsc.wa.gov.au/sport-and-recreation/sports-dimensions-guide/athletics-track-events},
}

@Misc{Triastcyn2019,
  author        = {Aleksei Triastcyn and Boi Faltings},
  title         = {Generating Artificial Data for Private Deep Learning},
  year          = {2019},
  archiveprefix = {arXiv},
  eprint        = {1803.03148},
  primaryclass  = {cs.LG},
}

@Misc{Goodfellow2014,
  author        = {Ian J. Goodfellow and Jean Pouget-Abadie and Mehdi Mirza and Bing Xu and David Warde-Farley and Sherjil Ozair and Aaron Courville and Yoshua Bengio},
  title         = {Generative Adversarial Networks},
  year          = {2014},
  archiveprefix = {arXiv},
  eprint        = {1406.2661},
  primaryclass  = {stat.ML},
}

'
@Article{Saxena2021,
  author     = {Saxena, Divya and Cao, Jiannong},
  journal    = {ACM Computing Surveys},
  title      = {Generative {Adversarial} {Networks} ({GANs}): {Challenges}, {Solutions}, and {Future} {Directions}},
  year       = {2021},
  issn       = {0360-0300},
  month      = may,
  number     = {3},
  pages      = {63:1--63:42},
  volume     = {54},
  abstract   = {Generative Adversarial Networks (GANs) is a novel class of deep generative models that has recently gained significant attention. GANs learn complex and high-dimensional distributions implicitly over images, audio, and data. However, there exist major challenges in training of GANs, i.e., mode collapse, non-convergence, and instability, due to inappropriate design of network architectre, use of objective function, and selection of optimization algorithm. Recently, to address these challenges, several solutions for better design and optimization of GANs have been investigated based on techniques of re-engineered network architectures, new objective functions, and alternative optimization algorithms. To the best of our knowledge, there is no existing survey that has particularly focused on the broad and systematic developments of these solutions. In this study, we perform a comprehensive survey of the advancements in GANs design and optimization solutions proposed to handle GANs challenges. We first identify key research issues within each design and optimization technique and then propose a new taxonomy to structure solutions by key research issues. In accordance with the taxonomy, we provide a detailed discussion on different GANs variants proposed within each solution and their relationships. Finally, based on the insights gained, we present promising research directions in this rapidly growing field.},
  doi        = {10.1145/3446374},
  keywords   = {GANs variants, computer vision, Image generation, mode collapse, GANs, deep Generative models, GANs applications, Deep learning, GANs Survey, GANs challenges, Generative Adversarial Networks},
  shorttitle = {Generative {Adversarial} {Networks} ({GANs})},
  url        = {https://doi.org/10.1145/3446374},
  urldate    = {2023-04-19},
}

'
@Article{Wu2018,
  author   = {Wu, Shaoen and Xu, Junhong and Zhu, Shangyue and Guo, Hanqing},
  journal  = {Signal Processing},
  title    = {A {Deep} {Residual} convolutional neural network for facial keypoint detection with missing labels},
  year     = {2018},
  issn     = {0165-1684},
  month    = mar,
  pages    = {384--391},
  volume   = {144},
  abstract = {Keypoint detection is critical in image recognitions. Deep learning such as convolutional neural network (CNN) has recently demonstrated its tremendous success in detecting image keypoints over conventional image processing methodologies. The deep learning solutions, however, heavily rely on labeling target images for their reliability and accuracy. Unfortunately, most image datasets do not have all labels marked. To address this problem, this paper presents an effective and novel deep learning solution, Masked Loss Residual Convolutional Neural Network (ML-ResNet), to facial keypoint detection on the datasets that have missing target labels. The core of ML-ResNet is a masked loss objective function that ignores the error in predicting the missing target keypoints in the output layer of a CNN. To compensate for the loss induced by the masked loss objective function that likely results in overfitting, ML-ResNet is designed of a data augmentation strategy to increase the number of training data. The performance of ML-ResNet has been evaluated on the image dataset from Kaggle Facial Keypoints Detection competition, which consists of 7049 training images, but with only 2140 images that have full target keypoints labeled. In the experiments, ML-ResNet is compared to a pioneer literature CNN facial keypoint detection work. The experiment results clearly show that the proposed ML-ResNet is robust and advantageous in training CNNs on datasets with missing target values. ML-ResNet can improve the learning time by 30\% during the training and the detection accuracy by eight times in facial keypoint detection.},
  doi      = {10.1016/j.sigpro.2017.11.003},
  file     = {ScienceDirect Full Text PDF:https\://www.sciencedirect.com/science/article/pii/S0165168417303924/pdfft?md5=9dca069542f86ff3914107b74ecfe858&pid=1-s2.0-S0165168417303924-main.pdf&isDTMRedir=Y:application/pdf},
  keywords = {Deep learning, Facial keypoint detection},
  language = {en},
  url      = {https://www.sciencedirect.com/science/article/pii/S0165168417303924},
  urldate  = {2023-04-25},
}

@InProceedings{Altwaijry2016,
  author    = {Altwaijry, Hani and Veit, Andreas and Belongie, Serge J and Tech, Cornell},
  booktitle = {BMVC},
  title     = {Learning to detect and match keypoints with deep architectures.},
  year      = {2016},
  url       = {https://www.researchgate.net/profile/Andreas-Veit/publication/317191594\_Learning\_to\_Detect\_and\_Match\_Keypoints
  \_with\_Deep\_Architectures/links /5a2f59a30f7e9bfe817035f7/Learning-to-Detect-and-Match-Keypoints-with-Deep-Architectures.pdf},
}

'
@Article{Lowe2004,
  author   = {Lowe, David G.},
  journal  = {International Journal of Computer Vision},
  title    = {Distinctive {Image} {Features} from {Scale}-{Invariant} {Keypoints}},
  year     = {2004},
  issn     = {1573-1405},
  month    = nov,
  number   = {2},
  pages    = {91--110},
  volume   = {60},
  abstract = {This paper presents a method for extracting distinctive invariant features from images that can be used to perform reliable matching between different views of an object or scene. The features are invariant to image scale and rotation, and are shown to provide robust matching across a substantial range of affine distortion, change in 3D viewpoint, addition of noise, and change in illumination. The features are highly distinctive, in the sense that a single feature can be correctly matched with high probability against a large database of features from many images. This paper also describes an approach to using these features for object recognition. The recognition proceeds by matching individual features to a database of features from known objects using a fast nearest-neighbor algorithm, followed by a Hough transform to identify clusters belonging to a single object, and finally performing verification through least-squares solution for consistent pose parameters. This approach to recognition can robustly identify objects among clutter and occlusion while achieving near real-time performance.},
  doi      = {10.1023/B:VISI.0000029664.99615.94},
  file     = {:Lowe2004 - Distinctive Image Features from Scale Invariant Keypoints.html:URL},
  keywords = {invariant features, object recognition, scale invariance, image matching},
  language = {en},
  url      = {https://doi.org/10.1023/B:VISI.0000029664.99615.94},
  urldate  = {2023-04-27},
}

'
@Article{Zhang2021,
  author   = {Zhang, Jing and Chen, Zhe and Tao, Dacheng},
  journal  = {International Journal of Computer Vision},
  title    = {Towards {High} {Performance} {Human} {Keypoint} {Detection}},
  year     = {2021},
  issn     = {1573-1405},
  month    = sep,
  number   = {9},
  pages    = {2639--2662},
  volume   = {129},
  abstract = {Human keypoint detection from a single image is very challenging due to occlusion, blur, illumination, and scale variance. In this paper, we address this problem from three aspects by devising an efficient network structure, proposing three effective training strategies, and exploiting four useful postprocessing techniques. First, we find that context information plays an important role in reasoning human body configuration and invisible keypoints. Inspired by this, we propose a cascaded context mixer (CCM), which efficiently integrates spatial and channel context information and progressively refines them. Then, to maximize CCM’s representation capability, we develop a hard-negative person detection mining strategy and a joint-training strategy by exploiting abundant unlabeled data. It enables CCM to learn discriminative features from massive diverse poses. Third, we present several sub-pixel refinement techniques for postprocessing keypoint predictions to improve detection accuracy. Extensive experiments on the MS COCO keypoint detection benchmark demonstrate the superiority of the proposed method over representative state-of-the-art (SOTA) methods. Our single model achieves comparable performance with the winner of the 2018 COCO Keypoint Detection Challenge. The final ensemble model sets a new SOTA on this benchmark. The source code will be released at https://github.com/chaimi2013/CCM.},
  doi      = {10.1007/s11263-021-01482-8},
  file     = {:Zhang2021 - Towards High Performance Human Keypoint Detection.html:URL},
  keywords = {Human Pose Estimation, Deep Nerual Networks, Sub-pixel Refinement, Context},
  language = {en},
  url      = {https://doi.org/10.1007/s11263-021-01482-8},
  urldate  = {2023-04-27},
}

'
@Misc{PyTorch2023,
  author   = {PyTorch},
  month    = apr,
  note     = {Accessed April 29, 2023},
  title    = {{PyTorch} Docs},
  year     = {2023},
  abstract = {An open source machine learning framework that accelerates the path from research prototyping to production deployment.},
  language = {en},
  url      = {https://www.pytorch.org},
  urldate  = {2023-04-29},
}

'
@TechReport{He2018,
  author   = {He, Kaiming and Gkioxari, Georgia and Dollár, Piotr and Girshick, Ross},
  title    = {Mask {R}-{CNN}},
  year     = {2018},
  month    = jan,
  note     = {arXiv:1703.06870 [cs] type: article},
  abstract = {We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Without bells and whistles, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code has been made available at: https://github.com/facebookresearch/Detectron},
  annote   = {Comment: open source; appendix on more results},
  doi      = {10.48550/arXiv.1703.06870},
  file     = {:He2018 - Mask R CNN.pdf:PDF},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  school   = {arXiv},
  url      = {http://arxiv.org/abs/1703.06870},
  urldate  = {2023-04-29},
}

@Misc{PyTorch2023a,
  author       = {PyTorch},
  howpublished = {Website},
  month        = apr,
  note         = {Accessed on April 29, 2023},
  title        = {Keypoint R-CNN ResNet-50-FPN},
  year         = {2023},
  url          = {https://pytorch.org/vision/0.12/generated/torchvision.models.detection. keypointrcnn\_resnet50\_fpn.html},
}

'
@InProceedings{Deng2009,
  author     = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle  = {2009 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
  title      = {{ImageNet}: {A} large-scale hierarchical image database},
  year       = {2009},
  month      = jun,
  note       = {ISSN: 1063-6919},
  pages      = {248--255},
  abstract   = {The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called “ImageNet”, a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500–1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.},
  doi        = {10.1109/CVPR.2009.5206848},
  issn       = {1063-6919},
  keywords   = {Large-scale systems, Image databases, Explosions, Internet, Robustness, Information retrieval, Image retrieval, Multimedia databases, Ontologies, Spine},
  shorttitle = {{ImageNet}},
}

@Misc{OpenCV2023,
  author       = {OpenCV},
  howpublished = {Website},
  month        = apr,
  note         = {Accessed on April 29, 2023},
  title        = {Morphological Transformations},
  year         = {2023},
  url          = {https://docs.opencv.org/4.x/d9/d61/tutorial\_py\_morphological\_ops.html},
}

'
@Misc{Derpanis2010,
  author     = {Konstantinos G. Derpanis},
  month      = may,
  title      = {Overview of the RANSAC Algorithm},
  year       = {2010},
  accessdate = {2023-04-30},
  file       = {:http\://rmozone.com/snapshots/2015/07/cdg-room-refs/ransac.pdf:PDF},
  url        = {http://rmozone.com/snapshots/2015/07/cdg-room-refs/ransac.pdf},
}

'
@Article{Canny1986,
  author   = {Canny, John},
  journal  = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  title    = {A {Computational} {Approach} to {Edge} {Detection}},
  year     = {1986},
  issn     = {1939-3539},
  month    = nov,
  number   = {6},
  pages    = {679--698},
  volume   = {PAMI-8},
  abstract = {This paper describes a computational approach to edge detection. The success of the approach depends on the definition of a comprehensive set of goals for the computation of edge points. These goals must be precise enough to delimit the desired behavior of the detector while making minimal assumptions about the form of the solution. We define detection and localization criteria for a class of edges, and present mathematical forms for these criteria as functionals on the operator impulse response. A third criterion is then added to ensure that the detector has only one response to a single edge. We use the criteria in numerical optimization to derive detectors for several common image features, including step edges. On specializing the analysis to step edges, we find that there is a natural uncertainty principle between detection and localization performance, which are the two main goals. With this principle we derive a single operator shape which is optimal at any scale. The optimal detector has a simple approximate implementation in which edges are marked at maxima in gradient magnitude of a Gaussian-smoothed image. We extend this simple detector using operators of several widths to cope with different signal-to-noise ratios in the image. We present a general method, called feature synthesis, for the fine-to-coarse integration of information from operators at different scales. Finally we show that step edge detector performance improves considerably as the operator point spread function is extended along the edge.},
  doi      = {10.1109/TPAMI.1986.4767851},
  file     = {:Canny1986 - A Computational Approach to Edge Detection.html:URL},
  keywords = {Image edge detection, Detectors, Machine vision, Shape measurement, Performance analysis, Uncertainty, Gaussian approximation, Signal to noise ratio, Signal synthesis, Feature extraction, Edge detection, feature extraction, image processing, machine vision, multiscale image analysis},
}

'
@Article{Chiang2014,
  author   = {Chiang, Yao-Yi and Leyk, Stefan and Knoblock, Craig A.},
  journal  = {ACM Computing Surveys},
  title    = {A {Survey} of {Digital} {Map} {Processing} {Techniques}},
  year     = {2014},
  issn     = {0360-0300},
  month    = may,
  number   = {1},
  pages    = {1:1--1:44},
  volume   = {47},
  abstract = {Maps depict natural and human-induced changes on earth at a fine resolution for large areas and over long periods of time. In addition, maps—especially historical maps—are often the only information source about the earth as surveyed using geodetic techniques. In order to preserve these unique documents, increasing numbers of digital map archives have been established, driven by advances in software and hardware technologies. Since the early 1980s, researchers from a variety of disciplines, including computer science and geography, have been working on computational methods for the extraction and recognition of geographic features from archived images of maps (digital map processing). The typical result from map processing is geographic information that can be used in spatial and spatiotemporal analyses in a Geographic Information System environment, which benefits numerous research fields in the spatial, social, environmental, and health sciences. However, map processing literature is spread across a broad range of disciplines in which maps are included as a special type of image. This article presents an overview of existing map processing techniques, with the goal of bringing together the past and current research efforts in this interdisciplinary field, to characterize the advances that have been made, and to identify future research directions and opportunities.},
  doi      = {10.1145/2557423},
  file     = {:Chiang2014 - A Survey of Digital Map Processing Techniques.html:URL},
  keywords = {pattern recognition, color segmentation, image processing, graphics recognition, Map processing, geographic information systems},
  url      = {https://dl.acm.org/doi/10.1145/2557423},
  urldate  = {2023-05-02},
}

'
@Article{Rosenfeld1977,
  author   = {A. Rosenfeld and G. J. VanderBrug},
  journal  = {IEEE Transactions on Systems, Man, and Cybernetics},
  title    = {Coarse-{Fine} {Template} {Matching}},
  year     = {1977},
  issn     = {2168-2909},
  month    = feb,
  number   = {2},
  pages    = {104--107},
  volume   = {7},
  doi      = {10.1109/TSMC.1977.4309663},
  file     = {:1977 - Coarse Fine Template Matching.pdf:PDF},
  keywords = {Computational efficiency, Pattern recognition, Spatial resolution, Costs, Clustering algorithms, Data analysis, Pattern classification, Writing, Reconnaissance, Taxonomy},
}

'
@Article{Yang2019,
  author   = {Yang, Hua and Huang, Chenghui and Wang, Feiyue and Song, Kaiyou and Zheng, Shijiao and Yin, Zhouping},
  journal  = {Pattern Recognition},
  title    = {Large-scale and rotation-invariant template matching using adaptive radial ring code histograms},
  year     = {2019},
  issn     = {0031-3203},
  month    = jul,
  pages    = {345--356},
  volume   = {91},
  abstract = {Although template matching has been widely studied in the fields of image processing and computer vision, current template matching methods still cannot address large-scale changes and rotation changes simultaneously. In this study, we propose a novel adaptive radial ring code histograms (ARRCH) image descriptor for large-scale and rotation-invariant template matching. The image descriptor is constructed by (1) identifying, inside the template, a set of concentric ring regions around a reference point, (2) detecting “stable” pixels based on the ASGO, which is tolerant with respect to large scale change, (3) extracting a rotation-invariant feature for each “stable” pixel, and (4) discretizing the features in a separate histogram for each concentric ring region in the scale space. Finally, an ARRCH image descriptor is obtained by chaining the histograms of all concentric ring regions for each scale. In matching mode, a sliding window approach is used to extract descriptors, which are compared with the template one, and a coarse-to-fine search strategy is employed to detect the scale of the target image. To demonstrate the performance of the ARRCH, several experiments are carried out, including a parameter experiment and a large-scale and rotation change matching experiment, and some applications are presented. The experimental results demonstrate that the proposed method is more resistant to large-scale and rotation differences than previous state-of-the-art matching methods.},
  doi      = {10.1016/j.patcog.2019.03.003},
  file     = {:Yang2019 - Large Scale and Rotation Invariant Template Matching Using Adaptive Radial Ring Code Histograms.html:URL},
  keywords = {Template matching, Adaptive radial ring code histograms, Large-scale and rotation-invariant features},
  language = {en},
  url      = {https://www.sciencedirect.com/science/article/pii/S0031320319301025},
  urldate  = {2023-05-02},
}

@Comment{jabref-meta: databaseType:bibtex;}
