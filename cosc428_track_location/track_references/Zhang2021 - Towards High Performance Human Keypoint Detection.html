
    

<!DOCTYPE html>
<html lang="en" class="no-js">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="applicable-device" content="pc,mobile">
    <meta name="access" content="No">

    
    
    <meta name="twitter:site" content="@SpringerLink"/>
    <meta name="twitter:card" content="summary_large_image"/>
    <meta name="twitter:image:alt" content="Content cover image"/>
    <meta name="twitter:title" content="Towards High Performance Human Keypoint Detection"/>
    <meta name="twitter:description" content="International Journal of Computer Vision - Human keypoint detection from a single image is very challenging due to occlusion, blur, illumination, and scale variance. In this paper, we address this..."/>
    <meta name="twitter:image" content="https://static-content.springer.com/cover/journal/11263/129/9.jpg"/>
    <meta name="journal_id" content="11263"/>
    <meta name="dc.title" content="Towards High Performance Human Keypoint Detection"/>
    <meta name="dc.source" content="International Journal of Computer Vision 2021 129:9"/>
    <meta name="dc.format" content="text/html"/>
    <meta name="dc.publisher" content="Springer"/>
    <meta name="dc.date" content="2021-07-01"/>
    <meta name="dc.type" content="OriginalPaper"/>
    <meta name="dc.language" content="En"/>
    <meta name="dc.copyright" content="2021 Crown"/>
    <meta name="dc.rights" content="2021 Crown"/>
    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>
    <meta name="dc.description" content="Human keypoint detection from a single image is very challenging due to occlusion, blur, illumination, and scale variance. In this paper, we address this problem from three aspects by devising an efficient network structure, proposing three effective training strategies, and exploiting four useful postprocessing techniques. First, we find that context information plays an important role in reasoning human body configuration and invisible keypoints. Inspired by this, we propose a cascaded context mixer (CCM), which efficiently integrates spatial and channel context information and progressively refines them. Then, to maximize CCM&#8217;s representation capability, we develop a hard-negative person detection mining strategy and a joint-training strategy by exploiting abundant unlabeled data. It enables CCM to learn discriminative features from massive diverse poses. Third, we present several sub-pixel refinement techniques for postprocessing keypoint predictions to improve detection accuracy. Extensive experiments on the MS COCO keypoint detection benchmark demonstrate the superiority of the proposed method over representative state-of-the-art (SOTA) methods. Our single model achieves comparable performance with the winner of the 2018 COCO Keypoint Detection Challenge. The final ensemble model sets a new SOTA on this benchmark. The source code will be released at https://github.com/chaimi2013/CCM ."/>
    <meta name="prism.issn" content="1573-1405"/>
    <meta name="prism.publicationName" content="International Journal of Computer Vision"/>
    <meta name="prism.publicationDate" content="2021-07-01"/>
    <meta name="prism.volume" content="129"/>
    <meta name="prism.number" content="9"/>
    <meta name="prism.section" content="OriginalPaper"/>
    <meta name="prism.startingPage" content="2639"/>
    <meta name="prism.endingPage" content="2662"/>
    <meta name="prism.copyright" content="2021 Crown"/>
    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>
    <meta name="prism.url" content="https://link.springer.com/article/10.1007/s11263-021-01482-8"/>
    <meta name="prism.doi" content="doi:10.1007/s11263-021-01482-8"/>
    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1007/s11263-021-01482-8.pdf"/>
    <meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1007/s11263-021-01482-8"/>
    <meta name="citation_journal_title" content="International Journal of Computer Vision"/>
    <meta name="citation_journal_abbrev" content="Int J Comput Vis"/>
    <meta name="citation_publisher" content="Springer US"/>
    <meta name="citation_issn" content="1573-1405"/>
    <meta name="citation_title" content="Towards High Performance Human Keypoint Detection"/>
    <meta name="citation_volume" content="129"/>
    <meta name="citation_issue" content="9"/>
    <meta name="citation_publication_date" content="2021/09"/>
    <meta name="citation_online_date" content="2021/07/01"/>
    <meta name="citation_firstpage" content="2639"/>
    <meta name="citation_lastpage" content="2662"/>
    <meta name="citation_article_type" content="Article"/>
    <meta name="citation_language" content="en"/>
    <meta name="dc.identifier" content="doi:10.1007/s11263-021-01482-8"/>
    <meta name="DOI" content="10.1007/s11263-021-01482-8"/>
    <meta name="size" content="805162"/>
    <meta name="citation_doi" content="10.1007/s11263-021-01482-8"/>
    <meta name="citation_springer_api_url" content="http://api.springer.com/xmldata/jats?q=doi:10.1007/s11263-021-01482-8&amp;api_key="/>
    <meta name="description" content="Human keypoint detection from a single image is very challenging due to occlusion, blur, illumination, and scale variance. In this paper, we address this p"/>
    <meta name="dc.creator" content="Zhang, Jing"/>
    <meta name="dc.creator" content="Chen, Zhe"/>
    <meta name="dc.creator" content="Tao, Dacheng"/>
    <meta name="dc.subject" content="Computer Imaging, Vision, Pattern Recognition and Graphics"/>
    <meta name="dc.subject" content="Artificial Intelligence"/>
    <meta name="dc.subject" content="Image Processing and Computer Vision"/>
    <meta name="dc.subject" content="Pattern Recognition"/>
    <meta name="citation_reference" content="Andriluka, M., Iqbal, U., Ensafutdinov, E., Pishchulin, L., Milan, A., &amp; Gall, J. B. S. (2018). PoseTrack: A benchmark for human pose estimation and tracking. In Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR)."/>
    <meta name="citation_reference" content="Baradel, F., Wolf, C., Mille, J., &amp; Taylor, G. W. (2018). Glimpse clouds: Human activity recognition from unstructured feature points. In Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR) (pp. 469&#8211;478)."/>
    <meta name="citation_reference" content="citation_journal_title=Psychological Review; citation_title=Recognition-by-components: A theory of human image understanding; citation_author=I Biederman; citation_volume=94; citation_issue=2; citation_publication_date=1987; citation_pages=115; citation_doi=10.1037/0033-295X.94.2.115; citation_id=CR3"/>
    <meta name="citation_reference" content="Cai, Y., Wang, Z., Luo, Z., Yin, B., Du, A., Wang, H., Zhou, X., Zhou, E., Zhang, X., &amp; Sun, J. (2020). Learning delicate local representations for multi-person pose estimation. In Proceedings of the European conference on computer vision (ECCV)"/>
    <meta name="citation_reference" content="Cao, Z., Simon, T., Wei, S. E., &amp; Sheikh, Y. (2017). Realtime multi-person 2d pose estimation using part affinity fields. In Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR) (pp. 7291&#8211;7299)."/>
    <meta name="citation_reference" content="citation_journal_title=IEEE Transactions on Pattern Analysis and Machine Intelligence; citation_title=Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFS; citation_author=LC Chen, G Papandreou, I Kokkinos, K Murphy, AL Yuille; citation_volume=40; citation_issue=4; citation_publication_date=2018; citation_pages=834-848; citation_doi=10.1109/TPAMI.2017.2699184; citation_id=CR6"/>
    <meta name="citation_reference" content="Chen, Y., Wang, Z., Peng, Y., Zhang, Z., Yu, G., &amp; Sun, J. (2018b) Cascaded pyramid network for multi-person pose estimation. In Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR) (pp. 7103&#8211;7112)."/>
    <meta name="citation_reference" content="citation_journal_title=International Journal of Computer Vision; citation_title=Recursive context routing for object detection; citation_author=Z Chen, J Zhang, D Tao; citation_volume=129; citation_publication_date=2020; citation_pages=142-160; citation_doi=10.1007/s11263-020-01370-7; citation_id=CR8"/>
    <meta name="citation_reference" content="Deng, J., Dong, W., Socher, R., Li, L. J., Li, K., &amp; Fei-Fei, L. (2009). Imagenet: A large-scale hierarchical image database. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 248&#8211;255)."/>
    <meta name="citation_reference" content="Fang, H. S., Xie, S., Tai, Y. W., &amp; Lu, C. (2017). Rmpe: Regional multi-person pose estimation. In Proceedings of the IEEE international conference on computer vision (ICCV) (pp. 2334&#8211;2343)."/>
    <meta name="citation_reference" content="Felzenszwalb, P., McAllester, D., &amp; Ramanan, D. (2008). A discriminatively trained, multiscale, deformable part model. In Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR) (pp. 1&#8211;8). IEEE."/>
    <meta name="citation_reference" content="Girdhar, R., Gkioxari, G., Torresani, L., Paluri, M., &amp; Tran, D. (2018). Detect-and-track: Efficient pose estimation in videos. In Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR) (pp. 350&#8211;359)."/>
    <meta name="citation_reference" content="citation_journal_title=International Journal of Computer Vision; citation_title=Synthesizing a scene-specific pedestrian detector and pose estimator for static video surveillance; citation_author=H Hattori, N Lee, VN Boddeti, F Beainy, KM Kitani, T Kanade; citation_volume=126; citation_issue=9; citation_publication_date=2018; citation_pages=1027-1044; citation_doi=10.1007/s11263-018-1077-3; citation_id=CR13"/>
    <meta name="citation_reference" content="He, K., Zhang, X., Ren, S., &amp; Sun, J. (2016). Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR) (pp. 770&#8211;778)."/>
    <meta name="citation_reference" content="He, K., Gkioxari, G., Doll&#225;r, P., &amp; Girshick, R. (2017). Mask r-cnn. In Proceedings of the IEEE international conference on computer vision (ICCV) (pp. 2961&#8211;2969)."/>
    <meta name="citation_reference" content="Holt, B., Ong, EJ., Cooper, H., &amp; Bowden, R. (2011). Putting the pieces together: Connected poselets for human pose estimation. In Proceedings of the IEEE international conference on computer vision workshops (ICCVW) (pp. 1196&#8211;1201). IEEE."/>
    <meta name="citation_reference" content="Hossain, M. R. I., &amp; Little, J. J. (2018). Exploiting temporal information for 3d human pose estimation. In Proceedings of the European conference on computer vision (ECCV) (pp. 69&#8211;86). Springer."/>
    <meta name="citation_reference" content="Hu, J., Shen, L., &amp; Sun, G. (2018). Squeeze-and-excitation networks. In Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR) (pp. 7132&#8211;7141)."/>
    <meta name="citation_reference" content="Huang, S., Gong, M., &amp; Tao, D. (2017). A coarse-fine network for keypoint localization. In Proceedings of the IEEE international conference on computer vision (ICCV) (pp. 3028&#8211;3037)."/>
    <meta name="citation_reference" content="Ioffe, S., &amp; Szegedy, C. (2015). Batch normalization: Accelerating deep network training by reducing internal covariate shift. In Proceedings of the international conference on machine learning (ICML) (pp. 448&#8211;456)."/>
    <meta name="citation_reference" content="Krizhevsky, A., Sutskever, I., &amp; Hinton, G. E. (2012). Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems (pp. 1097&#8211;1105)."/>
    <meta name="citation_reference" content="Lee, C. Y., Xie, S., Gallagher, P., Zhang, Z., &amp; Tu, Z. (2015). Deeply-supervised nets. In Artificial intelligence and statistics (pp. 562&#8211;570)."/>
    <meta name="citation_reference" content="Li, W., Wang, Z., Yin, B., Peng, Q., Du, Y., Xiao, T., Yu, G., Lu, H., Wei, Y., &amp; Sun, J. (2019). Rethinking on multi-stage networks for human pose estimation. arXiv preprint 
                  arXiv:1901.00148
                  
                "/>
    <meta name="citation_reference" content="Lin, T. Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Doll&#225;r, P., &amp; Zitnick, C. L. (2014) Microsoft coco: Common objects in context. In Proceedings of the European conference on computer vision (ECCV) (pp. 740&#8211;755)."/>
    <meta name="citation_reference" content="Lin TY, Doll&#225;r P, Girshick R, He K, Hariharan B, &amp; Belongie S (2017) Feature pyramid networks for object detection. In Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR) (pp. 2117&#8211;2125)."/>
    <meta name="citation_reference" content="citation_journal_title=IEEE transactions on pattern analysis and machine intelligence; citation_title=Skeleton-based action recognition using spatio-temporal lstm network with trust gates; citation_author=J Liu, A Shahroudy, D Xu, AC Kot, G Wang; citation_volume=40; citation_issue=12; citation_publication_date=2018; citation_pages=3007-3021; citation_doi=10.1109/TPAMI.2017.2771306; citation_id=CR26"/>
    <meta name="citation_reference" content="citation_journal_title=International Journal of Computer Vision; citation_title=Deep learning for generic object detection: A survey; citation_author=L Liu, W Ouyang, X Wang, P Fieguth, J Chen, X Liu, M Pietik&#228;inen; citation_volume=128; citation_issue=2; citation_publication_date=2020; citation_pages=261-318; citation_doi=10.1007/s11263-019-01247-4; citation_id=CR27"/>
    <meta name="citation_reference" content="Ma, B., Zhang, J., Xia, Y., &amp; Tao, D. (2020). Auto learning attention. In Advances in neural information processing systems (Vol. 33)."/>
    <meta name="citation_reference" content="Mazhar, O., Ramdani, S., Navarro, B., Passama, R., &amp; Cherubini, A. (2018). Towards real-time physical human-robot interaction using skeleton information and hand gestures. In Proceedings of the 2018 IEEE/RSJ international conference on intelligent robots and systems (IROS) (pp. 1&#8211;6). IEEE."/>
    <meta name="citation_reference" content="Newell, A., Yang, K., &amp; Deng, J. (2016). Stacked hourglass networks for human pose estimation. In Proceedings of the European conference on computer vision (ECCV) (pp. 483&#8211;499)."/>
    <meta name="citation_reference" content="Newell, A., Huang, Z., &amp; Deng, J. (2017). Associative embedding: End-to-end learning for joint detection and grouping. In Advances in neural information processing systems (pp. 2277&#8211;2287)."/>
    <meta name="citation_reference" content="citation_journal_title=IEEE Transactions on Neural Networks and Learning Systems; citation_title=Learning semantic-aligned action representation; citation_author=B Ni, T Li, X Yang; citation_volume=29; citation_issue=8; citation_publication_date=2017; citation_pages=3715-3725; citation_doi=10.1109/TNNLS.2017.2731775; citation_id=CR32"/>
    <meta name="citation_reference" content="citation_journal_title=International Journal of Computer Vision; citation_title=Learning mutual visibility relationship for pedestrian detection with a deep model; citation_author=W Ouyang, X Zeng, X Wang; citation_volume=120; citation_issue=1; citation_publication_date=2016; citation_pages=14-27; citation_doi=10.1007/s11263-016-0890-9; citation_id=CR33"/>
    <meta name="citation_reference" content="Papandreou, G., Zhu, T., Kanazawa, N., Toshev, A., Tompson, J., Bregler, C., &amp; Murphy, K. (2017). Towards accurate multi-person pose estimation in the wild. In Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR) (pp. 4903&#8211;4911)."/>
    <meta name="citation_reference" content="Papandreou, G., Zhu, T., Chen, LC., Gidaris, S., Tompson, J., &amp; Murphy, K. (2018) . Personlab: Person pose estimation and instance segmentation with a bottom-up, part-based, geometric embedding model. In Proceedings of the European conference on computer vision (ECCV) (pp. 269&#8211;286)."/>
    <meta name="citation_reference" content="Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E., DeVito, Z., Lin, Z., Desmaison, A., Antiga, L., &amp; Lerer, A. (2017). Automatic differentiation in pytorch. In Advances in neural information processing systems workshops."/>
    <meta name="citation_reference" content="Pavlakos, G., Zhou, X., &amp; Daniilidis, K. (2018a). Ordinal depth supervision for 3d human pose estimation. In Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR) (pp. 7307&#8211;7316)."/>
    <meta name="citation_reference" content="Pavlakos, G., Zhu, L., Zhou, X., &amp; Daniilidis, K. (2018b). Learning to estimate 3d human pose and shape from a single color image. In Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR) (pp. 459&#8211;468)."/>
    <meta name="citation_reference" content="Pishchulin, L., Insafutdinov, E., Tang, S., Andres, B., Andriluka, M., Gehler, PV., &amp; Schiele, B. (2016). Deepcut: Joint subset partition and labeling for multi person pose estimation. In Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR) (pp. 4929&#8211;4937)."/>
    <meta name="citation_reference" content="Ren, S., He, K., Girshick, R., &amp; Sun, J. (2015). Faster r-cnn: Towards real-time object detection with region proposal networks. In Advances in neural information processing systems (pp. 91&#8211;99)."/>
    <meta name="citation_reference" content="Rhodin, H., Salzmann, M., &amp; Fua, P. (2018). Unsupervised geometry-aware representation for 3d human pose estimation. In Proceedings of the European conference on computer vision (ECCV) (pp. 750&#8211;767)."/>
    <meta name="citation_reference" content="citation_journal_title=International Journal of Computer Vision; citation_title=Fast human pose detection using randomized hierarchical cascades of rejectors; citation_author=G Rogez, J Rihan, C Orrite-Uru&#241;uela, PH Torr; citation_volume=99; citation_issue=1; citation_publication_date=2012; citation_pages=25-52; citation_doi=10.1007/s11263-012-0516-9; citation_id=CR42"/>
    <meta name="citation_reference" content="Sun, K., Xiao, B., Liu, D., &amp; Wang, J. (2019). Deep high-resolution representation learning for human pose estimation. In Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR) (pp. 5693&#8211;5703)."/>
    <meta name="citation_reference" content="Sun, X., Xiao, B., Wei, F., Liang, S., &amp; Wei, Y. (2018). Integral human pose regression. In Proceedings of the European conference on computer vision (ECCV) (pp. 529&#8211;545)."/>
    <meta name="citation_reference" content="Toshev, A., &amp; Szegedy, C. (2014). Deeppose: Human pose estimation via deep neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR) (pp. 1653&#8211;1660)."/>
    <meta name="citation_reference" content="citation_journal_title=International Journal of Computer Vision; citation_title=Joint estimation of human pose and conversational groups from social scenes; citation_author=J Varadarajan, R Subramanian, SR Bul&#242;, N Ahuja, O Lanz, E Ricci; citation_volume=126; citation_issue=2&#8211;4; citation_publication_date=2018; citation_pages=410-429; citation_doi=10.1007/s11263-017-1026-6; citation_id=CR46"/>
    <meta name="citation_reference" content="Wagemans, J., Elder, JH., Kubovy, M., Palmer, SE., Peterson, MA., Singh, M., &amp; von&#160;der Heydt, R. (2012). A century of gestalt psychology in visual perception: I. perceptual grouping and figure&#8211;ground organization. Psychological bulletin 138(6):1172"/>
    <meta name="citation_reference" content="Wang, F., &amp; Li, Y. (2013). Beyond physical connections: Tree models in human pose estimation. In Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR) (pp. 596&#8211;603)."/>
    <meta name="citation_reference" content="Xiao, B., Wu, H., &amp; Wei, Y. (2018). Simple baselines for human pose estimation and tracking. In Proceedings of the European conference on computer vision (ECCV) (pp. 466&#8211;481)."/>
    <meta name="citation_reference" content="Yang, Q., Yang, R., Davis, J., &amp; Nist&#233;r, D. (2007). Spatial-depth super resolution for range images. In Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR), IEEE (pp. 1&#8211;8)."/>
    <meta name="citation_reference" content="Yang, W., Li, S., Ouyang, W., Li, H., &amp; Wang, X. (2017). Learning feature pyramids for human pose estimation. In Proceedings of the IEEE international conference on computer vision (ICCV) (pp. 1281&#8211;1290)."/>
    <meta name="citation_reference" content="Yang, W., Ouyang, W., Wang, X., Ren, J., Li, H., &amp; Wang, X. (2018). 3d human pose estimation in the wild by adversarial learning. In Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR) (pp. 5255&#8211;5264)."/>
    <meta name="citation_reference" content="citation_journal_title=IEEE Transactions on Pattern Analysis and Machine Intelligence; citation_title=Articulated human detection with flexible mixtures of parts; citation_author=Y Yang, D Ramanan; citation_volume=35; citation_issue=12; citation_publication_date=2013; citation_pages=2878-2890; citation_doi=10.1109/TPAMI.2012.261; citation_id=CR53"/>
    <meta name="citation_reference" content="Zhang, F., Zhu, X., Dai, H., Ye, M., &amp; Zhu, C. (2020). Distribution-aware coordinate representation for human pose estimation. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 7093&#8211;7102)."/>
    <meta name="citation_reference" content="Zhang, H., Ouyang, H., Liu, S., Qi, X., Shen, X., Yang, R., &amp; Jia, J. (2019a). Human pose estimation with spatial contextual information. arXiv preprint 
                  arXiv:1901.01760
                  
                "/>
    <meta name="citation_reference" content="Zhang, J., &amp; Tao, D. (2020). Empowering things with intelligence: A survey of the progress, challenges, and opportunities in artificial intelligence of things. IEEE Internet of Things Journal."/>
    <meta name="citation_reference" content="Zhang, SH., &amp; Li, R., et&#160;al (2019b). Pose2seg: Detection free human instance segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR)."/>
    <meta name="citation_author" content="Zhang, Jing"/>
    <meta name="citation_author_institution" content="School of Computer Science, Faculty of Engineering, The University of Sydney, Darlington, Australia"/>
    <meta name="citation_author" content="Chen, Zhe"/>
    <meta name="citation_author_email" content="zhe.chen1@sydney.edu.au"/>
    <meta name="citation_author_institution" content="School of Computer Science, Faculty of Engineering, The University of Sydney, Darlington, Australia"/>
    <meta name="citation_author" content="Tao, Dacheng"/>
    <meta name="citation_author_email" content="dacheng.tao@sydney.edu.au"/>
    <meta name="citation_author_institution" content="School of Computer Science, Faculty of Engineering, The University of Sydney, Darlington, Australia"/>
    <meta name="format-detection" content="telephone=no"/>
    <meta name="citation_cover_date" content="2021/09/01"/>
    

    
    
    <meta property="og:url" content="https://link.springer.com/article/10.1007/s11263-021-01482-8"/>
    <meta property="og:type" content="article"/>
    <meta property="og:site_name" content="SpringerLink"/>
    <meta property="og:title" content="Towards High Performance Human Keypoint Detection - International Journal of Computer Vision"/>
    <meta property="og:description" content="Human keypoint detection from a single image is very challenging due to occlusion, blur, illumination, and scale variance. In this paper, we address this problem from three aspects by devising an efficient network structure, proposing three effective training strategies, and exploiting four useful postprocessing techniques. First, we find that context information plays an important role in reasoning human body configuration and invisible keypoints. Inspired by this, we propose a cascaded context mixer (CCM), which efficiently integrates spatial and channel context information and progressively refines them. Then, to maximize CCM’s representation capability, we develop a hard-negative person detection mining strategy and a joint-training strategy by exploiting abundant unlabeled data. It enables CCM to learn discriminative features from massive diverse poses. Third, we present several sub-pixel refinement techniques for postprocessing keypoint predictions to improve detection accuracy. Extensive experiments on the MS COCO keypoint detection benchmark demonstrate the superiority of the proposed method over representative state-of-the-art (SOTA) methods. Our single model achieves comparable performance with the winner of the 2018 COCO Keypoint Detection Challenge. The final ensemble model sets a new SOTA on this benchmark. The source code will be released at https://github.com/chaimi2013/CCM ."/>
    <meta property="og:image" content="https://media.springernature.com/w200/springer-static/cover/journal/11263.jpg"/>
    


    <title>Towards High Performance Human Keypoint Detection | SpringerLink</title>

    <link rel="shortcut icon" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16 32x32 48x48" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png />
<link rel="icon" sizes="32x32" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png />
<link rel="icon" sizes="48x48" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png />
<link rel="apple-touch-icon" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />
<link rel="apple-touch-icon" sizes="72x72" href=/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png />
<link rel="apple-touch-icon" sizes="76x76" href=/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png />
<link rel="apple-touch-icon" sizes="114x114" href=/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png />
<link rel="apple-touch-icon" sizes="120x120" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png />
<link rel="apple-touch-icon" sizes="144x144" href=/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png />
<link rel="apple-touch-icon" sizes="152x152" href=/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png />
<link rel="apple-touch-icon" sizes="180x180" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />


    
    <script>(function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)</script>

    
    
        <style>@media only print, only all and (prefers-color-scheme: no-preference), only all and (prefers-color-scheme: light), only all and (prefers-color-scheme: dark) { html{text-size-adjust:100%;-webkit-font-smoothing:subpixel-antialiased;box-sizing:border-box;color:#333;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:100%;height:100%;line-height:1.61803;overflow-y:scroll}body,img{max-width:100%}body{background:#fcfcfc;font-size:1.125rem;line-height:1.5;min-height:100%}main{display:block}h1{font-family:Georgia,Palatino,serif;font-size:2.25rem;font-style:normal;font-weight:400;line-height:1.4}a{background-color:transparent;color:#004b83;overflow-wrap:break-word;text-decoration:underline;text-decoration-skip-ink:auto;word-break:break-word}b{font-weight:bolder}sub,sup{font-size:75%;line-height:0;position:relative;vertical-align:baseline}sub{bottom:-.25em}sup{top:-.5em}img{border:0;height:auto;vertical-align:middle}button,input{font-family:inherit;font-size:100%}input{line-height:1.15}button,input{overflow:visible}button{text-transform:none}[type=button],[type=submit],button{-webkit-appearance:button}[hidden]{display:none}button{border-radius:0;cursor:pointer;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;line-height:inherit}*{margin:0}h2{font-size:1.75rem}h2,h3{font-family:Georgia,Palatino,serif;font-weight:400;line-height:1.4}h3{font-size:1.5rem}h2,h3{font-style:normal}label{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif}*{box-sizing:inherit}body,button,div,form,input,p{margin:0;padding:0}h1,h2,h3{margin:0}h2+*{margin-block-start:1rem}h1+*{margin-block-start:3rem}[style*="display: none"]:first-child+*{margin-block-start:0}p{overflow-wrap:break-word;word-break:break-word}.c-ad{text-align:center}@media only screen and (min-width:320px){.c-ad{padding:8px}}.c-ad--728x90{background-color:#ccc;display:none}.c-ad--728x90 .c-ad__inner{min-height:calc(1.5em + 94px)}.c-ad--728x90 iframe{height:90px;max-width:970px}@media only screen and (min-width:768px){.js .c-ad--728x90{display:none}.js .u-show-following-ad+.c-ad--728x90{display:block}}.c-ad iframe{border:0;overflow:auto;vertical-align:top}.c-ad__label{color:#333;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:.875rem;font-weight:400;line-height:1.5;margin-bottom:4px}.c-breadcrumbs>li{display:inline}.c-card{background-color:transparent;border:0;box-shadow:none;flex-direction:column;font-size:14px;min-width:0;padding:0}.c-card,.c-card__image{display:flex;overflow:hidden;position:relative}.c-card__image{justify-content:center;padding-bottom:56.25%}@supports (aspect-ratio:1/1){.c-card__image{padding-bottom:0}}.c-card__image img{left:0;min-height:100%;min-width:100%;position:absolute}@supports ((-o-object-fit:cover) or (object-fit:cover)){.c-card__image img{height:100%;object-fit:cover;width:100%}}.c-card__body{flex:1 1 auto;padding:16px}.c-card__link:not(.c-card__link--no-block-link):before{bottom:0;content:"";left:0;position:absolute;right:0;top:0}.c-card--flush .c-card__body{padding:0}.c-skip-link{background:#f7fbfe;bottom:auto;color:#004b83;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:.875rem;padding:8px;position:absolute;text-align:center;transform:translateY(-100%);z-index:9999}@media (prefers-reduced-motion:reduce){.c-skip-link{transition:top .3s ease-in-out 0s}}@media print{.c-skip-link{display:none}}.c-skip-link:link{color:#004b83}.c-status-message{align-items:center;box-sizing:border-box;display:flex;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1rem;position:relative;width:100%}.c-status-message :last-child{margin-bottom:0}.c-status-message--boxed{background-color:#fff;border:1px solid #ccc;border-radius:2px;line-height:1.4;padding:16px}.c-status-message__icon{fill:currentcolor;display:inline-block;flex:0 0 auto;height:1.5em;margin-right:8px;transform:translate(0);vertical-align:text-top;width:1.5em}.c-status-message--info .c-status-message__icon{color:#003f8d}.c-status-message--boxed.c-status-message--info{border-bottom:4px solid #003f8d}.c-status-message--success .c-status-message__icon{color:#00b8b0}.c-pagination{align-items:center;display:flex;flex-wrap:wrap;font-size:.875rem;list-style:none;margin:0;padding:16px}@media only screen and (min-width:540px){.c-pagination{justify-content:center}}.c-pagination__item{margin-bottom:8px;margin-right:16px}.c-pagination__item:last-child{margin-right:0}.c-pagination__link{align-items:center;background-color:#f2f2f2;background-image:linear-gradient(#fff,#f2f2f2);border:1px solid #ccc;border-radius:2px;color:#004b83;cursor:pointer;display:inline-flex;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1rem;justify-content:center;line-height:1.3;margin:0;min-width:30px;padding:8px;position:relative;text-align:center;text-decoration:none;transition:all .25s ease 0s,color .25s ease 0s,border-color .25s ease 0s;width:auto}.c-pagination__link svg,.c-pagination__link--disabled svg{fill:currentcolor}.c-pagination__link:visited{color:#004b83}.c-pagination__link:focus,.c-pagination__link:hover{border:1px solid #666;text-decoration:none}.c-pagination__link:focus,.c-pagination__link:hover{background-color:#666;background-image:none;color:#fff}.c-pagination__link:focus svg path,.c-pagination__link:hover svg path{fill:#fff}.c-pagination__link--disabled{align-items:center;background-color:transparent;background-image:none;border-radius:2px;color:#333;cursor:default;display:inline-flex;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1rem;justify-content:center;line-height:1.3;margin:0;opacity:.67;padding:8px;position:relative;transition:all .25s ease 0s,color .25s ease 0s,border-color .25s ease 0s;width:auto}.c-pagination__link--disabled:visited{color:#333}.c-pagination__link--disabled,.c-pagination__link--disabled:focus,.c-pagination__link--disabled:hover{border:1px solid #ccc;text-decoration:none}.c-pagination__link--disabled:focus,.c-pagination__link--disabled:hover{background-color:transparent;background-image:none;color:#333}.c-pagination__link--disabled:focus svg path,.c-pagination__link--disabled:hover svg path{fill:#333}.c-pagination__link--active{background-color:#666;background-image:none;border-color:#666;color:#fff;cursor:default}.c-pagination__ellipsis{background:0 0;border:0;min-width:auto;padding-left:0;padding-right:0}.c-pagination__icon{fill:#999;height:12px;width:16px}.c-pagination__icon--active{fill:#004b83}.c-breadcrumbs{color:#000;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1rem;list-style:none;margin:0;padding:0}.c-breadcrumbs__link{color:#666}svg.c-breadcrumbs__chevron{fill:#666;height:10px;margin:4px 4px 0;width:10px}.c-header{background-color:#fff;border-bottom:4px solid #00285a;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1.125rem;padding:16px 0}.c-header__container,.c-header__menu{align-items:center;display:flex;flex-wrap:wrap}@supports (gap:2em){.c-header__container,.c-header__menu{gap:2em 2em}}.c-header__menu{list-style:none;margin:0;padding:0}.c-header__item{color:inherit}@supports not (gap:2em){.c-header__item{margin-left:24px}}.c-header__container{justify-content:space-between;margin:0 auto;max-width:1280px;padding:0 16px}@supports not (gap:2em){.c-header__brand{margin-right:32px}}.c-header__brand a{display:block;text-decoration:none}.c-header__link{color:inherit}.c-popup-search{background-color:#eee;box-shadow:0 3px 3px -3px rgba(0,0,0,.21);padding:16px 0;position:relative;z-index:10}@media only screen and (min-width:1024px){.js .c-popup-search{position:absolute;top:100%;width:100%}.c-popup-search__container{margin:auto;max-width:70%}}.c-article-header{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;margin-bottom:40px}.c-article-identifiers{color:#6f6f6f;display:flex;flex-wrap:wrap;font-size:1rem;line-height:1.3;list-style:none;margin:0 0 8px;padding:0}.c-article-identifiers__item{border-right:1px solid #6f6f6f;list-style:none;margin-right:8px;padding-right:8px}.c-article-identifiers__item:last-child{border-right:0;margin-right:0;padding-right:0}.c-article-title{font-size:1.5rem;line-height:1.25;margin-bottom:16px}@media only screen and (min-width:768px){.c-article-title{font-size:1.875rem;line-height:1.2}}.c-article-author-list{display:inline;font-size:1rem;list-style:none;margin:0 8px 0 0;padding:0;width:100%}.c-article-author-list__item{display:inline;padding-right:0}.c-article-author-list svg{margin-left:4px}.c-article-author-list__show-more{display:none;margin-right:4px}.c-article-author-list__button,.js .c-article-author-list__item--hide,.js .c-article-author-list__show-more{display:none}.js .c-article-author-list--long .c-article-author-list__show-more,.js .c-article-author-list--long+.c-article-author-list__button{display:inline}@media only screen and (max-width:539px){.js .c-article-author-list__item--hide-small-screen{display:none}.js .c-article-author-list--short .c-article-author-list__show-more,.js .c-article-author-list--short+.c-article-author-list__button{display:inline}}#uptodate-client,.js .c-article-author-list--expanded .c-article-author-list__show-more{display:none!important}.js .c-article-author-list--expanded .c-article-author-list__item--hide-small-screen{display:inline!important}.c-article-author-list__button,.c-button-author-list{background:#ebf1f5;border:4px solid #ebf1f5;border-radius:20px;color:#666;font-size:.875rem;line-height:1.4;padding:2px 11px 2px 8px;text-decoration:none}.c-article-author-list__button svg,.c-button-author-list svg{margin:1px 4px 0 0}.c-article-author-list__button:hover,.c-button-author-list:hover{background:#069;border-color:transparent;color:#fff}.c-article-info-details{font-size:1rem;margin-bottom:8px;margin-top:16px}.c-article-info-details__cite-as{border-left:1px solid #6f6f6f;margin-left:8px;padding-left:8px}.c-article-metrics-bar{display:flex;flex-wrap:wrap;font-size:1rem;line-height:1.3}.c-article-metrics-bar__wrapper{margin:0 0 16px}.c-article-metrics-bar__item{align-items:baseline;border-right:1px solid #6f6f6f;margin-right:8px}.c-article-metrics-bar__item:last-child{border-right:0}.c-article-metrics-bar__count{font-weight:700;margin:0}.c-article-metrics-bar__label{color:#626262;font-style:normal;font-weight:400;margin:0 10px 0 5px}.c-article-metrics-bar__details{margin:0}.c-article-main-column{font-family:Georgia,Palatino,serif;margin-right:8.6%;width:60.2%}@media only screen and (max-width:1023px){.c-article-main-column{margin-right:0;width:100%}}.c-article-extras{float:left;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;width:31.2%}@media only screen and (max-width:1023px){.c-article-extras{display:none}}.c-article-section__title{border-bottom:2px solid #d5d5d5;font-size:1.25rem;margin:0;padding-bottom:8px}@media only screen and (min-width:768px){.c-article-section__title{font-size:1.5rem;line-height:1.24}}.c-article-section{clear:both}.c-article-section__content{margin-bottom:40px;margin-top:0;padding-top:8px}@media only screen and (max-width:1023px){.c-article-section__content{padding-left:0}}.c-article-authors-search{margin-bottom:24px;margin-top:0}.c-article-authors-search__item,.c-article-authors-search__title{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif}.c-article-authors-search__title{color:#626262;font-size:1.05rem;font-weight:700;margin:0;padding:0}.c-article-authors-search__item{font-size:1rem}.c-article-authors-search__text{margin:0}.c-code-block{border:1px solid #f2f2f2;font-family:monospace;margin:0 0 24px;padding:20px}.c-code-block__heading{font-weight:400;margin-bottom:16px}.c-code-block__line{display:block;overflow-wrap:break-word;white-space:pre-wrap}.c-article-share-box__no-sharelink-info{font-size:.813rem;font-weight:700;margin-bottom:24px;padding-top:4px}.c-article-share-box__only-read-input{border:1px solid #d5d5d5;box-sizing:content-box;display:inline-block;font-size:.875rem;font-weight:700;height:24px;margin-bottom:8px;padding:8px 10px}.c-article-share-box__button--link-like{background-color:transparent;border:0;color:#069;cursor:pointer;font-size:.875rem;margin-bottom:8px;margin-left:10px}.c-article-associated-content__container .c-article-associated-content__collection-label{line-height:1.4}.c-article-associated-content__container .c-article-associated-content__collection-title{line-height:1.3}.c-context-bar{box-shadow:0 0 10px 0 rgba(51,51,51,.2);position:relative;width:100%}.c-context-bar__title{display:none}.c-breadcrumbs--truncated .c-breadcrumbs__link{display:inline-block;max-width:45%;overflow:hidden;text-overflow:ellipsis;vertical-align:bottom;white-space:nowrap}.c-reading-companion{clear:both;min-height:389px}.c-reading-companion__sticky{max-width:389px}.c-reading-companion__scroll-pane{margin:0;min-height:200px;overflow:hidden auto}.c-reading-companion__tabs{display:flex;flex-flow:row nowrap;font-size:1rem;list-style:none;margin:0 0 8px;padding:0}.c-reading-companion__tabs>li{flex-grow:1}.c-reading-companion__tab{background-color:#eee;border:1px solid #d5d5d5;border-image:initial;border-left-width:0;color:#069;font-size:1rem;padding:8px 8px 8px 15px;text-align:left;width:100%}.c-reading-companion__tabs li:first-child .c-reading-companion__tab{border-left-width:1px}.c-reading-companion__tab--active{background-color:#fcfcfc;border-bottom:1px solid #fcfcfc;color:#222;font-weight:700}.c-reading-companion__sections-list{list-style:none;padding:0}.c-reading-companion__figures-list,.c-reading-companion__references-list{list-style:none;min-height:389px;padding:0}.c-reading-companion__references-list--numeric{list-style:decimal inside}.c-reading-companion__sections-list{margin:0 0 8px;min-height:50px}.c-reading-companion__section-item{font-size:1rem;padding:0}.c-reading-companion__section-item a{display:block;line-height:1.5;overflow:hidden;padding:8px 0 8px 16px;text-overflow:ellipsis;white-space:nowrap}.c-reading-companion__figure-item{border-top:1px solid #d5d5d5;font-size:1rem;padding:16px 8px 16px 0}.c-reading-companion__figure-item:first-child{border-top:none;padding-top:8px}.c-reading-companion__reference-item{border-top:1px solid #d5d5d5;font-size:1rem;padding:8px 8px 8px 16px}.c-reading-companion__reference-item:first-child{border-top:none}.c-reading-companion__reference-item a{word-break:break-word}.c-reading-companion__reference-citation{display:inline}.c-reading-companion__reference-links{font-size:.813rem;font-weight:700;list-style:none;margin:8px 0 0;padding:0;text-align:right}.c-reading-companion__reference-links>a{display:inline-block;padding-left:8px}.c-reading-companion__reference-links>a:first-child{display:inline-block;padding-left:0}.c-reading-companion__figure-title{display:block;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1.25rem;font-style:normal;font-weight:700;line-height:1.4;margin:0 0 8px}.c-reading-companion__figure-links{display:flex;justify-content:space-between;margin:8px 0 0}.c-reading-companion__figure-links>a{align-items:center;display:flex}.c-reading-companion__figure-full-link svg{height:.8em;margin-left:2px}.c-reading-companion__panel{border-top:none;display:none;margin-top:0;padding-top:0}.c-reading-companion__panel--active{display:block}.c-article-section__figure-description{font-size:1rem}.c-article-section__figure-description>*{margin-bottom:0}.c-cod{display:block;font-size:1rem;width:100%}.c-cod__form{background:#ebf0f3}.c-cod__prompt{font-size:1.125rem;line-height:1.3;margin:0 0 24px}.c-cod__label{display:block;margin:0 0 4px}.c-cod__row{display:flex;margin:0 0 16px}.c-cod__row:last-child{margin:0}.c-cod__input{border:1px solid #d5d5d5;border-radius:2px;flex-basis:75%;flex-shrink:0;margin:0;padding:13px}.c-cod__input--submit{color:#fff;flex-shrink:1;margin-left:8px;transition:background-color .2s ease-out 0s,color .2s ease-out 0s}.c-cod__input--submit-single{flex-basis:100%;flex-shrink:0;margin:0}.c-cod__input--submit:focus,.c-cod__input--submit:hover{background-color:#fff}.c-pdf-download__link .u-icon{padding-top:2px}.save-data .c-article-author-institutional-author__sub-division,.save-data .c-article-equation__number,.save-data .c-article-figure-description,.save-data .c-article-fullwidth-content,.save-data .c-article-main-column,.save-data .c-article-satellite-article-link,.save-data .c-article-satellite-subtitle,.save-data .c-article-table-container,.save-data .c-blockquote__body,.save-data .c-code-block__heading,.save-data .c-reading-companion__figure-title,.save-data .c-reading-companion__reference-citation,.save-data .c-site-messages--nature-briefing-email-variant .serif,.save-data .c-site-messages--nature-briefing-email-variant.serif,.save-data .serif,.save-data .u-serif,.save-data h1,.save-data h2,.save-data h3{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif}.c-pdf-download{display:flex;margin-bottom:16px;max-height:48px}@media only screen and (min-width:540px){.c-pdf-download{max-height:none}}@media only screen and (min-width:1024px){.c-pdf-download{max-height:48px}}.c-pdf-download__link{display:flex;flex:1 1 0%;padding:13px 24px!important}.c-pdf-download__text{padding-right:4px}@media only screen and (max-width:539px){.c-pdf-download__text{text-transform:capitalize}}@media only screen and (min-width:540px){.c-pdf-download__text{padding-right:8px}}.c-pdf-container{display:flex;justify-content:flex-end}@media only screen and (max-width:539px){.c-pdf-container .c-pdf-download{display:flex;flex-basis:100%}}.c-article-extras .c-pdf-container{flex-wrap:wrap;width:100%}.c-article-extras .c-pdf-container .c-pdf-download{width:100%}.c-status-message--success{border-bottom:2px solid #00b8b0;margin-bottom:16px;padding-bottom:16px}.c-recommendations-header{border-bottom:1px solid #d5d5d5}.c-recommendations-title{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1.125rem;font-weight:700;line-height:1.24;margin:0;padding-bottom:16px}.c-recommendations-close{background-color:transparent;border:0;cursor:pointer;height:2em;margin-right:-10px;margin-top:-5px;width:2em}.c-recommendations-authors{line-height:1.24;margin-bottom:0}.c-recommendations-list-container{margin-top:0;position:relative}.c-recommendations-list{display:flex;flex-wrap:nowrap;justify-content:space-between;margin:0 auto;overflow-x:hidden;padding:16px 0;scroll-behavior:smooth;scroll-snap-type:x mandatory;width:calc(100% - 146px)}@media only screen and (max-width:539px){.c-recommendations-list{display:block;height:40vh;overflow-y:auto;width:100%}}.c-recommendations-list__item{display:flex;flex:0 0 calc(33.3333% - 16px);margin:0 8px;scroll-snap-align:center}@media only screen and (max-width:539px){.c-recommendations-list__item{margin:0;padding:0 0 16px}}.c-recommendations-list__item .c-card__image{align-items:baseline;flex:1 1 40%;margin:0 16px 0 0;max-width:150px}.c-recommendations-list__item .c-card__image img{border:1px solid #d5d5d5;height:auto;min-height:0;position:relative;transform:translateY(0)}@media only screen and (max-width:1023px){.c-recommendations-list__item .c-card__image{display:none}}.c-card__layout{display:flex;flex:1 1 auto;justify-content:space-between}.c-card__title-recommendation{-webkit-box-orient:vertical;-webkit-line-clamp:4;display:-webkit-box;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1.25rem;font-style:normal;font-weight:700;line-height:1.4rem;margin-bottom:0;max-height:5.6em;overflow:hidden!important;text-overflow:ellipsis}.c-card__title-recommendation .c-card__link{color:#004b83;font-size:1.125rem;text-decoration:none}@media only screen and (max-width:539px){.c-recommendations-column-switch{display:flex;flex-direction:column-reverse}}.js-greyout-page-background{background-color:rgba(34,34,34,.75);bottom:0;left:0;position:fixed;right:0;top:0}.app-search__content{display:flex}.app-search__label{color:#666;display:inline-block;font-size:.875rem;margin-bottom:8px}.app-search__input{border:1px solid #b3b3b3;border-bottom-left-radius:3px;border-top-left-radius:3px;box-shadow:inset 0 1px 3px 0 rgba(0,0,0,.21);flex:0 1 auto;font-size:.875rem;line-height:1.2;padding:.75em 1em;vertical-align:middle;width:100%}.app-search__button{align-items:center;background-color:#33629d;background-image:linear-gradient(#4d76a9,#33629d);border:1px solid rgba(0,59,132,.5);border-radius:0 2px 2px 0;color:#fff;cursor:pointer;display:inline-flex;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1rem;justify-content:center;line-height:1.3;margin:0;padding:8px;position:relative;text-align:center;text-decoration:none;transition:all .25s ease 0s,color .25s ease 0s,border-color .25s ease 0s;width:50px}.app-search__button svg,.u-button svg,.u-button--primary svg{fill:currentcolor}.app-checklist-banner{border:2px solid #ebf1f5;display:flex;flex:1 1 auto;font-size:1rem;justify-content:space-between;margin-bottom:16px;padding:16px}.app-checklist-banner--on-mobile{display:block;margin-bottom:32px}@media only screen and (min-width:1024px){.app-checklist-banner--on-mobile{display:none}}.app-checklist-banner__title{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1.125rem;font-weight:700;margin-bottom:0}.app-checklist-banner__icon-container{align-items:center;display:flex;flex:0 0 60px;justify-content:flex-end;width:60px}.app-checklist-banner__link{align-items:center;color:#004b83;display:inline-flex;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif}.app-checklist-banner__arrow-icon,.app-checklist-banner__paper-icon{fill:currentcolor;display:inline-block;transform:translate(0);vertical-align:text-top}.app-checklist-banner__paper-icon{height:36px!important;width:36px!important}.app-checklist-banner__arrow-icon{height:11px;margin:4px 0 0 8px;width:16px}.u-button{align-items:center;background-color:#f2f2f2;background-image:linear-gradient(#fff,#f2f2f2);border:1px solid #ccc;border-radius:2px;color:#004b83;cursor:pointer;display:inline-flex;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1rem;justify-content:center;line-height:1.3;margin:0;padding:8px;position:relative;text-decoration:none;transition:all .25s ease 0s,color .25s ease 0s,border-color .25s ease 0s;width:auto}.u-button--primary{background-color:#33629d;background-image:linear-gradient(#4d76a9,#33629d);border:1px solid rgba(0,59,132,.5);color:#fff}.u-button--full-width{display:flex;width:100%}.u-clearfix:after,.u-clearfix:before{content:"";display:table}.u-clearfix:after{clear:both}.u-display-block{display:block}.u-display-flex{display:flex;width:100%}.u-flex-direction-column{flex-direction:column}.u-align-items-center{align-items:center}.u-justify-content-space-between{justify-content:space-between}.u-flex-static{flex:0 0 auto}.u-display-none{display:none}.js .u-js-hide{display:none;visibility:hidden}@media print{.u-hide-print{display:none}}.u-icon{fill:currentcolor;display:inline-block;height:1em;transform:translate(0);vertical-align:text-top;width:1em}.u-list-reset{list-style:none;margin:0;padding:0}.u-button-reset{background-color:transparent;border:0;padding:0}.u-sans-serif{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif}.u-mbs-0{margin-block-start:0!important}.u-container{margin:0 auto;max-width:1280px;padding:0 16px}.u-position-relative{position:relative}.u-mt-0{margin-top:0}.u-mt-32{margin-top:32px}.u-mr-24{margin-right:24px}.u-mb-0{margin-bottom:0}.u-mb-8{margin-bottom:8px}.u-mb-16{margin-bottom:16px}.u-mb-24{margin-bottom:24px}.u-mb-32{margin-bottom:32px}.u-ml-8{margin-left:8px}.u-float-left{float:left}.u-hide{display:none;visibility:hidden}.u-hide:first-child+*{margin-block-start:0}@media only screen and (min-width:540px){.u-hide-at-sm{display:none;visibility:hidden}}@media only screen and (min-width:1024px){.u-hide-at-lg{display:none;visibility:hidden}}.u-visually-hidden{clip:rect(0,0,0,0);border:0;height:1px;margin:-100%;overflow:hidden;padding:0;position:absolute!important;width:1px}.u-text-sm{font-size:1rem}.u-h4{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1.25rem;font-style:normal;font-weight:700;line-height:1.4}.hide{display:none;visibility:hidden}.visually-hidden{clip:rect(1px,1px,1px,1px);height:1px;position:absolute!important;width:1px}.c-article-section__figure-description{font-family:Georgia,Palatino,serif}.c-article-section__content p{line-height:1.8}.c-pagination__input{border:1px solid #bfbfbf;border-radius:2px;box-shadow:inset 0 2px 6px 0 rgba(51,51,51,.2);box-sizing:initial;display:inline-block;height:28px;margin:0;max-width:64px;min-width:16px;padding:0 8px;text-align:center;transition:width .15s ease 0s}.c-pagination__input::-webkit-inner-spin-button,.c-pagination__input::-webkit-outer-spin-button{-webkit-appearance:none;margin:0}@media only screen and (min-width:1024px){.c-article-collection__container{display:none}}.c-article-associated-content__container .c-article-associated-content__collection-label{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1.063rem}.c-article-associated-content__container .c-article-associated-content__collection-title{font-size:1.063rem;font-weight:400}.c-reading-companion__sections-list{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif}.c-article-section__title,.c-article-title{font-weight:400}.c-header__cart-icon{margin-right:12px}.c-header__navigation{display:flex} }</style>



        <link rel="stylesheet" data-test="critical-css-handler" data-inline-css-source="critical-css" href="/oscar-static/app-springerlink/css/enhanced-article-6805b3feeb.css" media="print" onload="this.media='only print, only all and (prefers-color-scheme: no-preference), only all and (prefers-color-scheme: light), only all and (prefers-color-scheme: dark)';this.onload=null">
        
    



    
    <script>
        window.dataLayer = [{"GA Key":"UA-26408784-1","DOI":"10.1007/s11263-021-01482-8","Page":"article","springerJournal":true,"page":{"attributes":{"environment":"live"}},"Country":"NZ","japan":false,"doi":"10.1007-s11263-021-01482-8","Journal Title":"International Journal of Computer Vision","Journal Id":11263,"Keywords":"Human Pose Estimation, Deep Nerual Networks, Sub-pixel Refinement, Context","kwrd":["Human_Pose_Estimation","Deep_Nerual_Networks","Sub-pixel_Refinement","Context"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":[],"Open Access":"N","hasAccess":"N","bypassPaywall":"N","user":{"license":{"businessPartnerID":[],"businessPartnerIDString":""}},"Access Type":"no-access","Bpids":"","Bpnames":"","BPID":["1"],"VG Wort Identifier":"pw-vgzm.415900-10.1007-s11263-021-01482-8","Full HTML":"N","Subject Codes":["SCI","SCI22005","SCI21000","SCI22021","SCI2203X"],"pmc":["I","I22005","I21000","I22021","I2203X"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1573-1405","pissn":"0920-5691"},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"Computer Imaging, Vision, Pattern Recognition and Graphics","2":"Artificial Intelligence","3":"Image Processing and Computer Vision","4":"Pattern Recognition"},"secondarySubjectCodes":{"1":"I22005","2":"I21000","3":"I22021","4":"I2203X"}},"sucode":"SC6"},"attributes":{"deliveryPlatform":"oscar"}},"Event Category":"Article"}];
    </script>

    <script>
    window.dataLayer.push({
        ga4MeasurementId: 'G-B3E4QL2TPR',
        ga360TrackingId: 'UA-26408784-1',
        twitterId: 'o47a7',
        ga4ServerUrl: 'https://collect.springer.com',
        imprint: 'springerlink'
    });
</script>

    <script data-test="gtm-head">
    window.initGTM = function() {
        if (window.config.mustardcut) {
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://collect.springer.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-MRVXSHQ');
        }
    }
</script>
    <script>
    (function(w,d,t) {
        function cc() {
            var h = w.location.hostname;
            var e = d.createElement(t),
                    s = d.getElementsByTagName(t)[0];

            if (h.indexOf('springer.com') > -1) {
                e.src = 'https://cmp-static.springer.com/production_live/consent-bundle-17-28.js';
                e.setAttribute('onload', "initGTM(window,document,'script','dataLayer','GTM-MRVXSHQ')");
            } else {
                e.src = '/static/js/lib/cookie-consent.min.js';
                e.setAttribute('data-consent', h);
            }
            s.insertAdjacentElement('afterend', e);
        }

        cc();
    })(window,document,'script');
</script>

    <script>
    (function(w, d) {
        w.config = w.config || {};
        w.config.mustardcut = false;

        
        if (w.matchMedia && w.matchMedia('only print, only all and (prefers-color-scheme: no-preference), only all and (prefers-color-scheme: light), only all and (prefers-color-scheme: dark)').matches) {
            w.config.mustardcut = true;
            d.classList.add('js');
            d.classList.remove('grade-c');
        }
    })(window, document.documentElement);
</script>


    
<script>
    (function () {
        if ( typeof window.CustomEvent === "function" ) return false;
        function CustomEvent ( event, params ) {
            params = params || { bubbles: false, cancelable: false, detail: null };
            var evt = document.createEvent( 'CustomEvent' );
            evt.initCustomEvent( event, params.bubbles, params.cancelable, params.detail );
            return evt;
        }

        CustomEvent.prototype = window.Event.prototype;

        window.CustomEvent = CustomEvent;
    })();
</script>



    <script class="js-entry">
    if (window.config.mustardcut) {
        (function(w, d) {
            
            
            
                window.Component = {};
                window.suppressShareButton = false;
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                {'src': '/oscar-static/js/polyfill-es5-bundle-51eb718839.js', 'async': false},
                {'src': '/oscar-static/js/airbrake-es5-bundle-edfc643aa1.js', 'async': false},
            ];

            var bodyScripts = [
                
                    {'src': '/oscar-static/js/app-es5-bundle-55e31c31db.js', 'async': false, 'module': false},
                    {'src': '/oscar-static/js/app-es6-bundle-c4bf80b786.js', 'async': false, 'module': true}
                
                
                
                    , {'src': '/oscar-static/js/global-article-es5-bundle-4799bd8c8d.js', 'async': false, 'module': false},
                    {'src': '/oscar-static/js/global-article-es6-bundle-199faa8a7e.js', 'async': false, 'module': true}
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i = 0; i < headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i = 0; i < bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        })(window, document);
    }
</script>

    
    
    <link rel="canonical" href="https://link.springer.com/article/10.1007/s11263-021-01482-8"/>
    

    
    <script type="application/ld+json">{"mainEntity":{"headline":"Towards High Performance Human Keypoint Detection","description":"Human keypoint detection from a single image is very challenging due to occlusion, blur, illumination, and scale variance. In this paper, we address this problem from three aspects by devising an efficient network structure, proposing three effective training strategies, and exploiting four useful postprocessing techniques. First, we find that context information plays an important role in reasoning human body configuration and invisible keypoints. Inspired by this, we propose a cascaded context mixer (CCM), which efficiently integrates spatial and channel context information and progressively refines them. Then, to maximize CCM’s representation capability, we develop a hard-negative person detection mining strategy and a joint-training strategy by exploiting abundant unlabeled data. It enables CCM to learn discriminative features from massive diverse poses. Third, we present several sub-pixel refinement techniques for postprocessing keypoint predictions to improve detection accuracy. Extensive experiments on the MS COCO keypoint detection benchmark demonstrate the superiority of the proposed method over representative state-of-the-art (SOTA) methods. Our single model achieves comparable performance with the winner of the 2018 COCO Keypoint Detection Challenge. The final ensemble model sets a new SOTA on this benchmark. The source code will be released at \n                https://github.com/chaimi2013/CCM\n                \n              .","datePublished":"2021-07-01","dateModified":"2021-07-01","pageStart":"2639","pageEnd":"2662","sameAs":"https://doi.org/10.1007/s11263-021-01482-8","keywords":"Computer Imaging,Vision,Pattern Recognition and Graphics,Artificial Intelligence,Image Processing and Computer Vision,Pattern Recognition","image":"https://static-content.springer.com/image/art%3A10.1007%2Fs11263-021-01482-8/MediaObjects/11263_2021_1482_Fig1_HTML.png","isPartOf":{"name":"International Journal of Computer Vision","issn":["1573-1405","0920-5691"],"volumeNumber":"129","@type":["Periodical","PublicationVolume"]},"publisher":{"name":"Springer US","logo":{"url":"https://www.springernature.com/app-sn/public/images/logo-springernature.png","@type":"ImageObject"},"@type":"Organization"},"author":[{"name":"Zhang, Jing","affiliation":[{"name":"The University of Sydney","address":{"name":"School of Computer Science, Faculty of Engineering, The University of Sydney, Darlington, Australia","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"},{"name":"Chen, Zhe","affiliation":[{"name":"The University of Sydney","address":{"name":"School of Computer Science, Faculty of Engineering, The University of Sydney, Darlington, Australia","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"},{"name":"Tao, Dacheng","url":"http://orcid.org/0000-0001-7225-5449","affiliation":[{"name":"The University of Sydney","address":{"name":"School of Computer Science, Faculty of Engineering, The University of Sydney, Darlington, Australia","@type":"PostalAddress"},"@type":"Organization"}],"email":"dacheng.tao@sydney.edu.au","@type":"Person"}],"isAccessibleForFree":false,"hasPart":{"isAccessibleForFree":false,"cssSelector":".main-content","@type":"WebPageElement"},"@type":"ScholarlyArticle"},"@context":"https://schema.org","@type":"WebPage"}</script>

</head>
<body class="shared-article-renderer">
    
    
    
        
            <!-- Google Tag Manager (noscript) -->
            <noscript data-test="gtm-body">
                <iframe src="https://collect.springer.com/ns.html?id=GTM-MRVXSHQ"
                height="0" width="0" style="display:none;visibility:hidden"></iframe>
            </noscript>
            <!-- End Google Tag Manager (noscript) -->
        
    


    <div class="u-vh-full">
        <a class="c-skip-link" href="#main-content">Skip to main content</a>
        
            <div class="u-hide u-show-following-ad"></div>
            <aside class="c-ad c-ad--728x90" data-test="springer-doubleclick-ad">
                <div class="c-ad__inner">
                    <p class="c-ad__label">Advertisement</p>
                    <div id="div-gpt-ad-LB1" data-pa11y-ignore data-gpt data-test="LB1-ad"
                         data-gpt-unitpath="/270604982/springerlink/11263/article" data-gpt-sizes="728x90"
                         style="min-width:728px;min-height:90px" data-gpt-targeting="pos=LB1;articleid=s11263-021-01482-8;"></div>
                </div>
            </aside>


<div class="u-position-relative u-mbs-0">
        <header class="c-header u-mb-24" data-test="publisher-header">
    
        <div class="c-header__container">
            <div class="c-header__brand">
                
    <a id="logo" class="u-display-block" href="/" title="Go to homepage" data-test="springerlink-logo">
        <picture>
            <source type="image/svg+xml" srcset=/oscar-static/images/springerlink/svg/springerlink-6c9a864b59.svg>
            <img src=/oscar-static/images/springerlink/png/springerlink-1db8a5b8b1.png alt="SpringerLink" width="148" height="30" data-test="header-academic">
        </picture>
        
        
    </a>


            </div>
            <div class="c-header__navigation">
                
    
        <button type="button"
                class="c-header__link u-button-reset u-mr-24"
                data-expander
                data-expander-target="#popup-search"
                data-expander-autofocus="firstTabbable"
                data-test="header-search-button">
            <span class="u-display-flex u-align-items-center">
                <span>Search</span>
                <svg class="u-icon u-flex-static u-ml-8" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </span>
        </button>

        <div class="c-header__cart-icon">
            <div id="ecommerce-header-cart-icon-link" class="c-header__item ecommerce-cart" style="display:inline-block;margin-right:10px">
 <form action="https://order.springer.com/public/precheckout" method="post">
  <button class="c-header__link" type="submit" style="appearance:none;border:none;background:none;color:inherit;position:relative">
   <svg aria-hidden="true" focusable="false" height="18" viewbox="0 0 18 18" width="18" xmlns="http://www.w3.org/2000/svg" style="vertical-align:text-bottom">
    <path d="m5 14c1.1045695 0 2 .8954305 2 2s-.8954305 2-2 2-2-.8954305-2-2 .8954305-2 2-2zm10 0c1.1045695 0 2 .8954305 2 2s-.8954305 2-2 2-2-.8954305-2-2 .8954305-2 2-2zm-10 1c-.55228475 0-1 .4477153-1 1s.44771525 1 1 1 1-.4477153 1-1-.44771525-1-1-1zm10 0c-.5522847 0-1 .4477153-1 1s.4477153 1 1 1 1-.4477153 1-1-.4477153-1-1-1zm-12.82032249-15c.47691417 0 .88746157.33678127.98070211.80449199l.23823144 1.19501025 13.36277974.00045554c.5522847.00001882.9999659.44774934.9999659 1.00004222 0 .07084994-.0075361.14150708-.022474.2107727l-1.2908094 5.98534344c-.1007861.46742419-.5432548.80388386-1.0571651.80388386h-10.24805106c-.59173366 0-1.07142857.4477153-1.07142857 1 0 .5128358.41361449.9355072.94647737.9932723l.1249512.0067277h10.35933776c.2749512 0 .4979349.2228539.4979349.4978051 0 .2749417-.2227336.4978951-.4976753.4980063l-10.35959736.0041886c-1.18346732 0-2.14285714-.8954305-2.14285714-2 0-.6625717.34520317-1.24989198.87690425-1.61383592l-1.63768102-8.19004794c-.01312273-.06561364-.01950005-.131011-.0196107-.19547395l-1.71961253-.00064219c-.27614237 0-.5-.22385762-.5-.5 0-.27614237.22385763-.5.5-.5zm14.53193359 2.99950224h-13.11300004l1.20580469 6.02530174c.11024034-.0163252.22327998-.02480398.33844139-.02480398h10.27064786z" fill="#333"></path>
   </svg><span class="u-screenreader-only visually-hidden">Go to cart</span><span class="cart-info" style="display:none;position:absolute;top:-4px;right:-10px;background-color:#C40606;color:#fff;width:18px;height:18px;font-size:11px;border-radius:50%;line-height:17.5px;text-align:center"></span></button>
 </form>
 <script>(function () { var exports = {}; if (window.fetch) {
            
            "use strict";
Object.defineProperty(exports, "__esModule", { value: true });
exports.headerWidgetClientInit = void 0;
var headerWidgetClientInit = function (getCartInfo) {
    console.log("listen to updatedCart event");
    document.body.addEventListener("updatedCart", function () {
        console.log("updatedCart happened");
        updateCartIcon().then(function () { return console.log("Cart state update upon event"); });
    }, false);
    return updateCartIcon().then(function () { return console.log("Initial cart state update"); });
    function updateCartIcon() {
        return getCartInfo()
            .then(function (res) { return res.json(); })
            .then(refreshCartState)
            .catch(function () { return console.log("Could not fetch cart info"); });
    }
    function refreshCartState(json) {
        var indicator = document.querySelector("#ecommerce-header-cart-icon-link .cart-info");
        /* istanbul ignore else */
        if (indicator && json.itemCount) {
            indicator.style.display = 'block';
            indicator.textContent = json.itemCount > 9 ? '9+' : json.itemCount.toString();
            var moreThanOneItem = json.itemCount > 1;
            indicator.setAttribute('title', "there ".concat(moreThanOneItem ? "are" : "is", " ").concat(json.itemCount, " item").concat(moreThanOneItem ? "s" : "", " in your cart"));
        }
        return json;
    }
};
exports.headerWidgetClientInit = headerWidgetClientInit;

            
            headerWidgetClientInit(
              function () {
                return window.fetch("https://cart.springer.com/cart-info", {
                  credentials: "include",
                  headers: { Accept: "application/json" }
                })
              }
            )
        }})()</script>
</div>
        </div>

        <nav class="u-position-relative">
            <ul class="c-header__menu">
                
        
            <li class="c-header__item">
                <a
                    data-test="login-link"
                    class="c-header__link"
                    href="https://link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs11263-021-01482-8"
                    data-track="click"
                    data-track-category="header"
                    data-track-action="login header"
                    data-track-label="link">Log in</a>
            </li>
        

        


            </ul>
        </nav>
    

    



            </div>
        </div>
    </header>

    
        
            <div id="popup-search" class="c-popup-search u-mb-16 js-header-search u-js-hide">
                <div class="c-popup-search__content">
                    <div class="u-container">
                        <div class="c-popup-search__container" data-test="springerlink-popup-search">
                            <div class="app-search">
    <form role="search" method="GET" action="/search" >
        <label for="search" class="app-search__label">Search SpringerLink</label>
        <div class="app-search__content">
            <input id="search" class="app-search__input" data-search-input autocomplete="off" name="query" type="text" value="">
            <button class="app-search__button" type="submit">
                <span class="u-visually-hidden">Search</span>
                <svg class="u-icon" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </button>
            
                <input type="hidden" name="searchType" value="publisherSearch">
            
            
        </div>
    </form>
</div>

                        </div>
                    </div>
                </div>
            </div>
        
    
</div>
        
                
    
        <nav class="u-container" aria-label="breadcrumbs" data-test="article-breadcrumbs">
            <ol class="c-breadcrumbs c-breadcrumbs--truncated" itemscope itemtype="https://schema.org/BreadcrumbList">
                
                    <li class="c-breadcrumbs__item" id="breadcrumb0" itemprop="itemListElement" itemscope="" itemtype="https://schema.org/ListItem">
                        <a href="/" class="c-breadcrumbs__link" itemprop="item" data-track="click" data-track-category="article" data-track-action="breadcrumbs" data-track-label="breadcrumb1"><span itemprop="name">Home</span></a><meta itemprop="position" content="1">
                            <svg class="c-breadcrumbs__chevron" role="img" aria-hidden="true" focusable="false" height="10" viewBox="0 0 10 10" width="10" xmlns="http://www.w3.org/2000/svg">
                                <path d="m5.96738168 4.70639573 2.39518594-2.41447274c.37913917-.38219212.98637524-.38972225 1.35419292-.01894278.37750606.38054586.37784436.99719163-.00013556 1.37821513l-4.03074001 4.06319683c-.37758093.38062133-.98937525.38100976-1.367372-.00003075l-4.03091981-4.06337806c-.37759778-.38063832-.38381821-.99150444-.01600053-1.3622839.37750607-.38054587.98772445-.38240057 1.37006824.00302197l2.39538588 2.4146743.96295325.98624457z" fill="#666" fill-rule="evenodd" transform="matrix(0 -1 1 0 0 10)"/>
                            </svg>
                    </li>
                
                    <li class="c-breadcrumbs__item" id="breadcrumb1" itemprop="itemListElement" itemscope="" itemtype="https://schema.org/ListItem">
                        <a href="/journal/11263" class="c-breadcrumbs__link" itemprop="item" data-track="click" data-track-category="article" data-track-action="breadcrumbs" data-track-label="breadcrumb2"><span itemprop="name">International Journal of Computer Vision</span></a><meta itemprop="position" content="2">
                            <svg class="c-breadcrumbs__chevron" role="img" aria-hidden="true" focusable="false" height="10" viewBox="0 0 10 10" width="10" xmlns="http://www.w3.org/2000/svg">
                                <path d="m5.96738168 4.70639573 2.39518594-2.41447274c.37913917-.38219212.98637524-.38972225 1.35419292-.01894278.37750606.38054586.37784436.99719163-.00013556 1.37821513l-4.03074001 4.06319683c-.37758093.38062133-.98937525.38100976-1.367372-.00003075l-4.03091981-4.06337806c-.37759778-.38063832-.38381821-.99150444-.01600053-1.3622839.37750607-.38054587.98772445-.38240057 1.37006824.00302197l2.39538588 2.4146743.96295325.98624457z" fill="#666" fill-rule="evenodd" transform="matrix(0 -1 1 0 0 10)"/>
                            </svg>
                    </li>
                
                    <li class="c-breadcrumbs__item" id="breadcrumb2" itemprop="itemListElement" itemscope="" itemtype="https://schema.org/ListItem">
                        <span itemprop="name">Article</span><meta itemprop="position" content="3">
                    </li>
                
            </ol>
        </nav>
    

        
        
    <div class="u-container u-mt-32 u-mb-32 u-clearfix" id="main-content" data-component="article-container">
        <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
            

            
                <div class="c-pdf-button__container u-hide-at-lg js-context-bar-sticky-point-mobile">
                    
                </div>
            

            <div class="c-article-collection__container">
                
    

            </div>


            <article lang="en">
                <div class="c-article-header">
                    <header>
                        <ul class="c-article-identifiers" data-test="article-identifier">
                            
    
    
    

                            <li class="c-article-identifiers__item"><a href="#article-info" data-track="click" data-track-action="publication date" data-track-label="link">Published: <time datetime="2021-07-01">01 July 2021</time></a></li>
                        </ul>

                        
                        <h1 class="c-article-title" data-test="article-title" data-article-title="">Towards High Performance Human Keypoint Detection</h1>
                        <ul class="c-article-author-list c-article-author-list--short" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-article-author-list__item"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Jing-Zhang" data-author-popup="auth-Jing-Zhang">Jing Zhang</a><sup class="u-js-hide"><a href="#Aff1">1</a></sup>, </li><li class="c-article-author-list__item"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Zhe-Chen" data-author-popup="auth-Zhe-Chen">Zhe Chen</a><sup class="u-js-hide"><a href="#Aff1">1</a></sup> &amp; </li><li class="c-article-author-list__item"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Dacheng-Tao" data-author-popup="auth-Dacheng-Tao" data-corresp-id="c1">Dacheng Tao<svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-email"></use></svg></a><span class="u-js-hide"> 
            <a class="js-orcid" href="http://orcid.org/0000-0001-7225-5449"><span class="u-visually-hidden">ORCID: </span>orcid.org/0000-0001-7225-5449</a></span><sup class="u-js-hide"><a href="#Aff1">1</a></sup> </li></ul>
                        <p class="c-article-info-details" data-container-section="info">
                            
    <a data-test="journal-link" href="/journal/11263" data-track="click" data-track-action="journal homepage" data-track-category="article body" data-track-label="link"><i data-test="journal-title">International Journal of Computer Vision</i></a>

                            <b data-test="journal-volume"><span class="u-visually-hidden">volume</span> 129</b>, <span class="u-visually-hidden">pages </span>2639–2662 (<span data-test="article-publication-year">2021</span>)<a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                        </p>
                        
    

                        <div data-test="article-metrics">
                            <div id="altmetric-container">
    <div class="c-article-metrics-bar__wrapper u-clear-both">
        <ul class="c-article-metrics-bar u-list-reset">
            
                <li class=" c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__count">1399 <span class="c-article-metrics-bar__label">Accesses</span></p>
                </li>
            
            
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__count">21 <span class="c-article-metrics-bar__label">Citations</span></p>
                </li>
            
            
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">1 <span class="c-article-metrics-bar__label">Altmetric</span></p>
                    </li>
                
            
            <li class="c-article-metrics-bar__item">
                <p class="c-article-metrics-bar__details"><a href="/article/10.1007/s11263-021-01482-8/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
            </li>
        </ul>
    </div>
</div>

                        </div>
                        
    

    

                        
                    </header>
                </div>

                <div data-article-body="true" data-track-component="article body" class="c-article-body">
                    <section aria-labelledby="Abs1" data-title="Abstract" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>Human keypoint detection from a single image is very challenging due to occlusion, blur, illumination, and scale variance. In this paper, we address this problem from three aspects by devising an efficient network structure, proposing three effective training strategies, and exploiting four useful postprocessing techniques. First, we find that context information plays an important role in reasoning human body configuration and invisible keypoints. Inspired by this, we propose a cascaded context mixer (CCM), which efficiently integrates spatial and channel context information and progressively refines them. Then, to maximize CCM’s representation capability, we develop a hard-negative person detection mining strategy and a joint-training strategy by exploiting abundant unlabeled data. It enables CCM to learn discriminative features from massive diverse poses. Third, we present several sub-pixel refinement techniques for postprocessing keypoint predictions to improve detection accuracy. Extensive experiments on the MS COCO keypoint detection benchmark demonstrate the superiority of the proposed method over representative state-of-the-art (SOTA) methods. Our single model achieves comparable performance with the winner of the 2018 COCO Keypoint Detection Challenge. The final ensemble model sets a new SOTA on this benchmark. The source code will be released at <a href="https://github.com/chaimi2013/CCM">https://github.com/chaimi2013/CCM</a>.</p></div></div></section>
                    
    


                    
                        
                            <div class="c-notes">
                                <p class="c-notes__text">This is a preview of subscription content, <a id="test-login-banner-link" href="//wayf.springernature.com?redirect_uri&#x3D;https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs11263-021-01482-8" data-track="click" data-track-action="login" data-track-label="link">access via your institution</a>.</p>
                            </div>
                        
                        
                            
                            
                                <div data-test="buy-box-mobile" class="c-article-buy-box c-article-buy-box--article">
                                    <div class="sprcom-buybox-articleSidebar" id="sprcom-buybox-articleSidebar">
 <!-- rendered: 2023-04-27T09:30:32.337671 -->
 <h2 class="c-box__heading">Access options</h2>
 <article class="c-box buying-option" data-test-id="buy-article">
  <h3 class="c-box__heading">Buy single article</h3>
  <div class="c-box__body">
   <div class="buybox__info">
    <p>Instant access to the full article PDF.</p>
   </div>
   <div class="buybox__buy">
    <p class="buybox__price">39,95 €</p>
    <p class="buybox__price-info">Price includes VAT (New Zealand)<br></p>
    <form action="https://order.springer.com/public/cart" method="post">
     <input type="hidden" name="type" value="article"><input type="hidden" name="doi" value="10.1007/s11263-021-01482-8"><input type="hidden" name="isxn" value="1573-1405"><input type="hidden" name="contenttitle" value="Towards High Performance Human Keypoint Detection"><input type="hidden" name="copyrightyear" value="2021"><input type="hidden" name="year" value="2021"><input type="hidden" name="authors" value="Jing Zhang, Zhe Chen, Dacheng Tao"><input type="hidden" name="title" value="International Journal of Computer Vision"><input type="hidden" name="mac" value="13FE6EA62CF0EDCF43FF37668A2C96BF"><input type="submit" class="c-box__button" onclick="dataLayer.push({&quot;event&quot;:&quot;addToCart&quot;,&quot;ecommerce&quot;:{&quot;currencyCode&quot;:&quot;EUR&quot;,&quot;add&quot;:{&quot;products&quot;:[{&quot;name&quot;:&quot;Towards High Performance Human Keypoint Detection&quot;,&quot;id&quot;:&quot;1573-1405&quot;,&quot;price&quot;:39.95,&quot;brand&quot;:&quot;Springer US&quot;,&quot;category&quot;:&quot;Computer Science&quot;,&quot;variant&quot;:&quot;ppv-article&quot;,&quot;quantity&quot;:1}]}}});" value="Buy article PDF">
    </form>
   </div>
  </div>
  <script>dataLayer.push({"ecommerce":{"currency":"EUR","impressions":[{"name":"Towards High Performance Human Keypoint Detection","id":"1573-1405","price":39.95,"brand":"Springer US","category":"Computer Science","variant":"ppv-article","quantity":1}]}});</script>
 </article>
 <article class="c-box buybox__rent-article" id="deepdyve" style="display: none" data-test-id="journal-subscription">
  <div class="c-box__body">
   <div class="buybox__info">
    <p><a class="deepdyve-link" target="deepdyve" rel="nofollow" data-track="click" data-track-action="rent article" data-track-label="rent action, new buybox">Rent this article via DeepDyve.</a></p>
   </div>
  </div>
  <script>
            function deepDyveResponse(data) {
                if (data.status === 'ok') {
                    [].slice.call(document.querySelectorAll('.c-box.buybox__rent-article')).forEach(function (article) {
                        article.style.display = 'flex'
                        var link = article.querySelector('.deepdyve-link')
                        if (link) {
                            link.setAttribute('href', data.url)
                        }
                    })
                }
            }

            var script = document.createElement('script')
            script.src = '//www.deepdyve.com/rental-link?docId=10.1007/s11263-021-01482-8&journal=1573-1405&fieldName=journal_doi&affiliateId=springer&format=jsonp&callback=deepDyveResponse'
            document.body.appendChild(script)
          </script>
 </article>
 <aside class="buybox__institutional-sub">
  <div class="c-box__body">
   <div class="buybox__info">
    <p><a href="https://www.springernature.com/gp/librarians/licensing/license-options?&amp;abtest=v2" data-track="click" data-track-action="institutional link" data-track-label="institutional subscriptions, new buybox">Learn more about Institutional subscriptions</a></p>
   </div>
  </div>
 </aside>
 <style>.sprcom-buybox-articleSidebar{
  box-shadow: 0px 0px 5px rgba(51,51,51,0.101);
  display: flex;
  flex-wrap: wrap;
  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen-Sans, Ubuntu, Cantarell, 'Helvetica Neue', sans-serif;
  text-align: center;
}
.sprcom-buybox-articleSidebar *{
  box-sizing: border-box;
  line-height: calc(100% + 4px);
  margin: 0px;
}
.sprcom-buybox-articleSidebar > *{
  display: flex;
  flex-basis: 240px;
  flex-direction: column;
  flex-grow: 1;
  flex-shrink: 1;
  margin: 0.5px;
}
.sprcom-buybox-articleSidebar > *{
  box-shadow: 0 0 0 1px rgba(204,204,204,0.494);
}
.sprcom-buybox-articleSidebar .c-box__body{
  display: flex;
  flex-direction: column-reverse;
  flex-grow: 1;
  justify-content: space-between;
  padding: 6%;
}
.sprcom-buybox-articleSidebar .c-box__body .buybox__buy{
  display: flex;
  flex-direction: column-reverse;
}
.sprcom-buybox-articleSidebar p{
  color: #333;
  font-size: 15px;
}
.sprcom-buybox-articleSidebar .buybox__price{
  font-size: 24px;
  font-weight: 500;
  line-height: calc(100% + 8px);
  margin: 20px 0;
  order: 1;
}
.sprcom-buybox-articleSidebar form{
  order: 1;
}
.sprcom-buybox-articleSidebar .buybox__price-info{
  margin-bottom: 20px;
}
.sprcom-buybox-articleSidebar .c-box__heading{
  background-color: #f0f0f0;
  color: #333;
  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen-Sans, Ubuntu, Cantarell, 'Helvetica Neue', sans-serif;
  font-size: 16px;
  margin: 0px;
  padding: 10px 12px;
  text-align: center;
}
.sprcom-buybox-articleSidebar .c-box__button{
  background-color: #3365A4;
  border: 1px solid transparent;
  border-radius: 2px;
  color: #fff;
  cursor: pointer;
  display: inline-block;
  font-family: inherit;
  font-size: 16px;
  max-width: 222px;
  padding: 10px 12px;
  text-decoration: none;
  width: 100%;
}
.sprcom-buybox-articleSidebar h3{
  clip: rect(1px, 1px, 1px, 1px);
  height: 1px;
  overflow: hidden;
  position: absolute;
  width: 1px;
}
.sprcom-buybox-articleSidebar h2{
  flex-basis: 100%;
  margin-bottom: 16px;
  text-align: left;
}
.sprcom-buybox-articleSidebar .buybox__institutional-sub, .buybox__rent-article .c-box__body{
  flex-direction: row;
}
.sprcom-buybox-articleSidebar .buybox__institutional-sub, .buybox__rent-article .buybox__info{
  text-align: left;
}
.sprcom-buybox-articleSidebar .buybox__institutional-sub{
  background-color: #f0f0f0;
}
.sprcom-buybox-articleSidebar .visually-hidden{
  clip: rect(1px, 1px, 1px, 1px);
  height: 1px;
  overflow: hidden;
  position: absolute;
  width: 1px;
}
.sprcom-buybox-articleSidebar style{
  display: none;
}
</style>
 <script style="display: none">
                ;(function () {
                    var timestamp = Date.now()
                    document.write('<div data-id="id_'+ timestamp +'"></div>')

                    var head = document.getElementsByTagName("head")[0]
                    var script = document.createElement("script")
                    script.type = "text/javascript"
                    script.src = "https://buy.springer.com/assets/js/buybox-bundle-abe5f44a67.js"
                    script.id = "ecommerce-scripts-" + timestamp
                    head.appendChild(script)

                    var buybox = document.querySelector("[data-id=id_"+ timestamp +"]").parentNode

                    ;[].slice.call(buybox.querySelectorAll(".buying-option")).forEach(init)

                    function init(buyingOption, index) {
                        var form = buyingOption.querySelector("form")

                        if (form) {
                            var formAction = form.getAttribute("action")
                            document.querySelector("#ecommerce-scripts-" + timestamp).addEventListener("load", bindModal(form, formAction, timestamp, index), false)
                        }
                    }

                    function bindModal(form, formAction, timestamp, index) {
                        var weHasBrowserSupport = window.fetch && Array.from

                        return function() {
                            console.log("ecommerce-scripts loaded, attempting to init modal …")
                            var Buybox = EcommScripts ? EcommScripts.Buybox : null
                            var Modal = EcommScripts ? EcommScripts.Modal : null
                            
                            if (weHasBrowserSupport && Buybox && Modal) {
                                var modalID = "ecomm-modal_" + timestamp + "_" + index
                                
                                var modal = new Modal(modalID)
                                modal.domEl.addEventListener("close", close)
                                function close() {
                                    form.querySelector("button[type=submit]").focus()
                                }

                                var cartURL = "/cart"
                                var cartModalURL = "/cart?messageOnly=1"

                                form.setAttribute(
                                    "action",
                                    formAction.replace(cartURL, cartModalURL)
                                )
                                
                                var formSubmit = Buybox.interceptFormSubmit(
                                    Buybox.fetchFormAction(window.fetch),
                                    function(responseBody) {
                                        document.body.dispatchEvent(new Event("updatedCart"))
                                        Buybox.triggerModalAfterAddToCartSuccess(modal)(responseBody)
                                    },
                                    function() {
                                        form.removeEventListener("submit", formSubmit, false)
                                        form.setAttribute(
                                            "action",
                                            formAction.replace(cartModalURL, cartURL)
                                        )
                                        form.submit()
                                    }
                                )

                                form.addEventListener("submit", formSubmit, false)
                                
                                document.body.appendChild(modal.domEl)
                            } else {
                                console.log("binding failed:", weHasBrowserSupport, EcommScripts)
                            }
                        }
                    }
                })()
              </script>
</div>
                                </div>
                            
                        
                        <div class="u-display-none">
                            <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><picture><source type="image/webp" srcset="//media.springernature.com/m312/springer-static/image/art%3A10.1007%2Fs11263-021-01482-8/MediaObjects/11263_2021_1482_Fig1_HTML.png?as=webp"><img src="//media.springernature.com/m312/springer-static/image/art%3A10.1007%2Fs11263-021-01482-8/MediaObjects/11263_2021_1482_Fig1_HTML.png" alt="" loading="lazy" width="312" height="312"></picture></div></div></figure></div><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><picture><source type="image/webp" srcset="//media.springernature.com/m312/springer-static/image/art%3A10.1007%2Fs11263-021-01482-8/MediaObjects/11263_2021_1482_Fig2_HTML.png?as=webp"><img src="//media.springernature.com/m312/springer-static/image/art%3A10.1007%2Fs11263-021-01482-8/MediaObjects/11263_2021_1482_Fig2_HTML.png" alt="" loading="lazy" width="312" height="312"></picture></div></div></figure></div><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3"><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><picture><source type="image/webp" srcset="//media.springernature.com/m312/springer-static/image/art%3A10.1007%2Fs11263-021-01482-8/MediaObjects/11263_2021_1482_Fig3_HTML.jpg?as=webp"><img src="//media.springernature.com/m312/springer-static/image/art%3A10.1007%2Fs11263-021-01482-8/MediaObjects/11263_2021_1482_Fig3_HTML.jpg" alt="" loading="lazy" width="312" height="260"></picture></div></div></figure></div><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4"><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 4</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><picture><source type="image/webp" srcset="//media.springernature.com/m312/springer-static/image/art%3A10.1007%2Fs11263-021-01482-8/MediaObjects/11263_2021_1482_Fig4_HTML.png?as=webp"><img src="//media.springernature.com/m312/springer-static/image/art%3A10.1007%2Fs11263-021-01482-8/MediaObjects/11263_2021_1482_Fig4_HTML.png" alt="" loading="lazy" width="312" height="103"></picture></div></div></figure></div><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-5"><figure><figcaption><b id="Fig5" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 5</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><picture><source type="image/webp" srcset="//media.springernature.com/m312/springer-static/image/art%3A10.1007%2Fs11263-021-01482-8/MediaObjects/11263_2021_1482_Fig5_HTML.jpg?as=webp"><img src="//media.springernature.com/m312/springer-static/image/art%3A10.1007%2Fs11263-021-01482-8/MediaObjects/11263_2021_1482_Fig5_HTML.jpg" alt="" loading="lazy" width="312" height="218"></picture></div></div></figure></div><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-6"><figure><figcaption><b id="Fig6" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 6</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><picture><source type="image/webp" srcset="//media.springernature.com/m312/springer-static/image/art%3A10.1007%2Fs11263-021-01482-8/MediaObjects/11263_2021_1482_Fig6_HTML.png?as=webp"><img src="//media.springernature.com/m312/springer-static/image/art%3A10.1007%2Fs11263-021-01482-8/MediaObjects/11263_2021_1482_Fig6_HTML.png" alt="" loading="lazy" width="273" height="312"></picture></div></div></figure></div><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-7"><figure><figcaption><b id="Fig7" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 7</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><picture><source type="image/webp" srcset="//media.springernature.com/m312/springer-static/image/art%3A10.1007%2Fs11263-021-01482-8/MediaObjects/11263_2021_1482_Fig7_HTML.png?as=webp"><img src="//media.springernature.com/m312/springer-static/image/art%3A10.1007%2Fs11263-021-01482-8/MediaObjects/11263_2021_1482_Fig7_HTML.png" alt="" loading="lazy" width="271" height="312"></picture></div></div></figure></div><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-8"><figure><figcaption><b id="Fig8" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 8</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><picture><source type="image/webp" srcset="//media.springernature.com/m312/springer-static/image/art%3A10.1007%2Fs11263-021-01482-8/MediaObjects/11263_2021_1482_Fig8_HTML.png?as=webp"><img src="//media.springernature.com/m312/springer-static/image/art%3A10.1007%2Fs11263-021-01482-8/MediaObjects/11263_2021_1482_Fig8_HTML.png" alt="" loading="lazy" width="312" height="146"></picture></div></div></figure></div><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-9"><figure><figcaption><b id="Fig9" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 9</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><picture><source type="image/webp" srcset="//media.springernature.com/m312/springer-static/image/art%3A10.1007%2Fs11263-021-01482-8/MediaObjects/11263_2021_1482_Fig9_HTML.png?as=webp"><img src="//media.springernature.com/m312/springer-static/image/art%3A10.1007%2Fs11263-021-01482-8/MediaObjects/11263_2021_1482_Fig9_HTML.png" alt="" loading="lazy" width="312" height="141"></picture></div></div></figure></div><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-10"><figure><figcaption><b id="Fig10" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 10</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><picture><source type="image/webp" srcset="//media.springernature.com/m312/springer-static/image/art%3A10.1007%2Fs11263-021-01482-8/MediaObjects/11263_2021_1482_Fig10_HTML.jpg?as=webp"><img src="//media.springernature.com/m312/springer-static/image/art%3A10.1007%2Fs11263-021-01482-8/MediaObjects/11263_2021_1482_Fig10_HTML.jpg" alt="" loading="lazy" width="312" height="218"></picture></div></div></figure></div><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-11"><figure><figcaption><b id="Fig11" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 11</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><picture><source type="image/webp" srcset="//media.springernature.com/m312/springer-static/image/art%3A10.1007%2Fs11263-021-01482-8/MediaObjects/11263_2021_1482_Fig11_HTML.jpg?as=webp"><img src="//media.springernature.com/m312/springer-static/image/art%3A10.1007%2Fs11263-021-01482-8/MediaObjects/11263_2021_1482_Fig11_HTML.jpg" alt="" loading="lazy" width="312" height="192"></picture></div></div></figure></div><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-12"><figure><figcaption><b id="Fig12" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 12</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><picture><source type="image/webp" srcset="//media.springernature.com/m312/springer-static/image/art%3A10.1007%2Fs11263-021-01482-8/MediaObjects/11263_2021_1482_Fig12_HTML.png?as=webp"><img src="//media.springernature.com/m312/springer-static/image/art%3A10.1007%2Fs11263-021-01482-8/MediaObjects/11263_2021_1482_Fig12_HTML.png" alt="" loading="lazy" width="312" height="174"></picture></div></div></figure></div><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-13"><figure><figcaption><b id="Fig13" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 13</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><picture><source type="image/webp" srcset="//media.springernature.com/m312/springer-static/image/art%3A10.1007%2Fs11263-021-01482-8/MediaObjects/11263_2021_1482_Fig13_HTML.png?as=webp"><img src="//media.springernature.com/m312/springer-static/image/art%3A10.1007%2Fs11263-021-01482-8/MediaObjects/11263_2021_1482_Fig13_HTML.png" alt="" loading="lazy" width="312" height="263"></picture></div></div></figure></div><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-14"><figure><figcaption><b id="Fig14" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 14</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><picture><source type="image/webp" srcset="//media.springernature.com/m312/springer-static/image/art%3A10.1007%2Fs11263-021-01482-8/MediaObjects/11263_2021_1482_Fig14_HTML.png?as=webp"><img src="//media.springernature.com/m312/springer-static/image/art%3A10.1007%2Fs11263-021-01482-8/MediaObjects/11263_2021_1482_Fig14_HTML.png" alt="" loading="lazy" width="312" height="287"></picture></div></div></figure></div>
                        </div>
                    

                    <div data-test="cobranding-download">
                        
                    </div>

                    <div class="app-checklist-banner--on-mobile">
                        
                    </div>

                    

                    <section data-title="Notes"><div class="c-article-section" id="notes-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="notes">Notes</h2><div class="c-article-section__content" id="notes-content"><ol class="c-article-footnote c-article-footnote--listed"><li class="c-article-footnote--listed__item" id="Fn1" data-counter="1."><div class="c-article-footnote--listed__content"><p>The instances with at least one annotated keypoint are counted.</p></div></li><li class="c-article-footnote--listed__item" id="Fn2" data-counter="2."><div class="c-article-footnote--listed__content"><p><a href="https://challenger.ai/competition/keypoint/">https://challenger.ai/competition/keypoint/</a>.</p></div></li><li class="c-article-footnote--listed__item" id="Fn3" data-counter="3."><div class="c-article-footnote--listed__content"><p>A video demo can be found in <a href="https://github.com/chaimi2013/CCM/video">https://github.com/chaimi2013/CCM/video</a>.</p></div></li><li class="c-article-footnote--listed__item" id="Fn4" data-counter="4."><div class="c-article-footnote--listed__content"><p><a href="http://cocodataset.org/index.htm#keypoints-leaderboard">http://cocodataset.org/index.htm#keypoints-leaderboard</a>.</p></div></li></ol></div></div></section><div id="MagazineFulltextArticleBodySuffix"><section aria-labelledby="Bib1" data-title="References"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ul class="c-article-references" data-track-component="outbound reference"><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR1">Andriluka, M., Iqbal, U., Ensafutdinov, E., Pishchulin, L., Milan, A., &amp; Gall, J. B. S. (2018). PoseTrack: A benchmark for human pose estimation and tracking. In <i>Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR)</i>.</p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR2">Baradel, F., Wolf, C., Mille, J., &amp; Taylor, G. W. (2018). Glimpse clouds: Human activity recognition from unstructured feature points. In <i>Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR)</i> (pp. 469–478).</p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR3">Biederman, I. (1987). Recognition-by-components: A theory of human image understanding. <i>Psychological Review</i>, <i>94</i>(2), 115.</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1037/0033-295X.94.2.115" data-track-action="article reference" href="https://doi.org/10.1037%2F0033-295X.94.2.115" aria-label="Article reference 3" data-doi="10.1037/0033-295X.94.2.115">Article</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 3" href="http://scholar.google.com/scholar_lookup?&amp;title=Recognition-by-components%3A%20A%20theory%20of%20human%20image%20understanding&amp;journal=Psychological%20Review&amp;doi=10.1037%2F0033-295X.94.2.115&amp;volume=94&amp;issue=2&amp;publication_year=1987&amp;author=Biederman%2CI">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR4">Cai, Y., Wang, Z., Luo, Z., Yin, B., Du, A., Wang, H., Zhou, X., Zhou, E., Zhang, X., &amp; Sun, J. (2020). Learning delicate local representations for multi-person pose estimation. In <i>Proceedings of the European conference on computer vision (ECCV)</i></p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR5">Cao, Z., Simon, T., Wei, S. E., &amp; Sheikh, Y. (2017). Realtime multi-person 2d pose estimation using part affinity fields. In <i>Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR)</i> (pp. 7291–7299).</p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR6">Chen, L. C., Papandreou, G., Kokkinos, I., Murphy, K., &amp; Yuille, A. L. (2018). Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFS. <i>IEEE Transactions on Pattern Analysis and Machine Intelligence</i>, <i>40</i>(4), 834–848.</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1109/TPAMI.2017.2699184" data-track-action="article reference" href="https://doi.org/10.1109%2FTPAMI.2017.2699184" aria-label="Article reference 6" data-doi="10.1109/TPAMI.2017.2699184">Article</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 6" href="http://scholar.google.com/scholar_lookup?&amp;title=Deeplab%3A%20Semantic%20image%20segmentation%20with%20deep%20convolutional%20nets%2C%20atrous%20convolution%2C%20and%20fully%20connected%20CRFS&amp;journal=IEEE%20Transactions%20on%20Pattern%20Analysis%20and%20Machine%20Intelligence&amp;doi=10.1109%2FTPAMI.2017.2699184&amp;volume=40&amp;issue=4&amp;pages=834-848&amp;publication_year=2018&amp;author=Chen%2CLC&amp;author=Papandreou%2CG&amp;author=Kokkinos%2CI&amp;author=Murphy%2CK&amp;author=Yuille%2CAL">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR7">Chen, Y., Wang, Z., Peng, Y., Zhang, Z., Yu, G., &amp; Sun, J. (2018b) Cascaded pyramid network for multi-person pose estimation. In <i>Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR)</i> (pp. 7103–7112).</p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR8">Chen, Z., Zhang, J., &amp; Tao, D. (2020). Recursive context routing for object detection. <i>International Journal of Computer Vision</i>, <i>129</i>, 142–160.</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1007/s11263-020-01370-7" data-track-action="article reference" href="https://doi.org/10.1007%2Fs11263-020-01370-7" aria-label="Article reference 8" data-doi="10.1007/s11263-020-01370-7">Article</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 8" href="http://scholar.google.com/scholar_lookup?&amp;title=Recursive%20context%20routing%20for%20object%20detection&amp;journal=International%20Journal%20of%20Computer%20Vision&amp;doi=10.1007%2Fs11263-020-01370-7&amp;volume=129&amp;pages=142-160&amp;publication_year=2020&amp;author=Chen%2CZ&amp;author=Zhang%2CJ&amp;author=Tao%2CD">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR9">Deng, J., Dong, W., Socher, R., Li, L. J., Li, K., &amp; Fei-Fei, L. (2009). Imagenet: A large-scale hierarchical image database. In <i>Proceedings of the IEEE conference on computer vision and pattern recognition</i> (pp. 248–255).</p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR10">Fang, H. S., Xie, S., Tai, Y. W., &amp; Lu, C. (2017). Rmpe: Regional multi-person pose estimation. In <i>Proceedings of the IEEE international conference on computer vision (ICCV)</i> (pp. 2334–2343).</p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR11">Felzenszwalb, P., McAllester, D., &amp; Ramanan, D. (2008). A discriminatively trained, multiscale, deformable part model. In <i>Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR)</i> (pp. 1–8). IEEE.</p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR12">Girdhar, R., Gkioxari, G., Torresani, L., Paluri, M., &amp; Tran, D. (2018). Detect-and-track: Efficient pose estimation in videos. In <i>Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR)</i> (pp. 350–359).</p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR13">Hattori, H., Lee, N., Boddeti, V. N., Beainy, F., Kitani, K. M., &amp; Kanade, T. (2018). Synthesizing a scene-specific pedestrian detector and pose estimator for static video surveillance. <i>International Journal of Computer Vision</i>, <i>126</i>(9), 1027–1044.</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1007/s11263-018-1077-3" data-track-action="article reference" href="https://doi.org/10.1007%2Fs11263-018-1077-3" aria-label="Article reference 13" data-doi="10.1007/s11263-018-1077-3">Article</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 13" href="http://scholar.google.com/scholar_lookup?&amp;title=Synthesizing%20a%20scene-specific%20pedestrian%20detector%20and%20pose%20estimator%20for%20static%20video%20surveillance&amp;journal=International%20Journal%20of%20Computer%20Vision&amp;doi=10.1007%2Fs11263-018-1077-3&amp;volume=126&amp;issue=9&amp;pages=1027-1044&amp;publication_year=2018&amp;author=Hattori%2CH&amp;author=Lee%2CN&amp;author=Boddeti%2CVN&amp;author=Beainy%2CF&amp;author=Kitani%2CKM&amp;author=Kanade%2CT">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR14">He, K., Zhang, X., Ren, S., &amp; Sun, J. (2016). Deep residual learning for image recognition. In <i>Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR)</i> (pp. 770–778).</p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR15">He, K., Gkioxari, G., Dollár, P., &amp; Girshick, R. (2017). Mask r-cnn. In <i>Proceedings of the IEEE international conference on computer vision (ICCV)</i> (pp. 2961–2969).</p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR16">Holt, B., Ong, EJ., Cooper, H., &amp; Bowden, R. (2011). Putting the pieces together: Connected poselets for human pose estimation. In <i>Proceedings of the IEEE international conference on computer vision workshops (ICCVW)</i> (pp. 1196–1201). IEEE.</p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR17">Hossain, M. R. I., &amp; Little, J. J. (2018). Exploiting temporal information for 3d human pose estimation. In <i>Proceedings of the European conference on computer vision (ECCV)</i> (pp. 69–86). Springer.</p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR18">Hu, J., Shen, L., &amp; Sun, G. (2018). Squeeze-and-excitation networks. In <i>Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR)</i> (pp. 7132–7141).</p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR19">Huang, S., Gong, M., &amp; Tao, D. (2017). A coarse-fine network for keypoint localization. In <i>Proceedings of the IEEE international conference on computer vision (ICCV)</i> (pp. 3028–3037).</p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR20">Ioffe, S., &amp; Szegedy, C. (2015). Batch normalization: Accelerating deep network training by reducing internal covariate shift. In <i>Proceedings of the international conference on machine learning (ICML)</i> (pp. 448–456).</p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR21">Krizhevsky, A., Sutskever, I., &amp; Hinton, G. E. (2012). Imagenet classification with deep convolutional neural networks. In <i>Advances in neural information processing systems</i> (pp. 1097–1105).</p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR22">Lee, C. Y., Xie, S., Gallagher, P., Zhang, Z., &amp; Tu, Z. (2015). Deeply-supervised nets. In <i>Artificial intelligence and statistics</i> (pp. 562–570).</p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR23">Li, W., Wang, Z., Yin, B., Peng, Q., Du, Y., Xiao, T., Yu, G., Lu, H., Wei, Y., &amp; Sun, J. (2019). Rethinking on multi-stage networks for human pose estimation. arXiv preprint <a href="http://arxiv.org/abs/1901.00148">arXiv:1901.00148</a></p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR24">Lin, T. Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollár, P., &amp; Zitnick, C. L. (2014) Microsoft coco: Common objects in context. In <i>Proceedings of the European conference on computer vision (ECCV)</i> (pp. 740–755).</p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR25">Lin TY, Dollár P, Girshick R, He K, Hariharan B, &amp; Belongie S (2017) Feature pyramid networks for object detection. In <i>Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR)</i> (pp. 2117–2125).</p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR26">Liu, J., Shahroudy, A., Xu, D., Kot, A. C., &amp; Wang, G. (2018). Skeleton-based action recognition using spatio-temporal lstm network with trust gates. <i>IEEE transactions on pattern analysis and machine intelligence</i>, <i>40</i>(12), 3007–3021.</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1109/TPAMI.2017.2771306" data-track-action="article reference" href="https://doi.org/10.1109%2FTPAMI.2017.2771306" aria-label="Article reference 26" data-doi="10.1109/TPAMI.2017.2771306">Article</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 26" href="http://scholar.google.com/scholar_lookup?&amp;title=Skeleton-based%20action%20recognition%20using%20spatio-temporal%20lstm%20network%20with%20trust%20gates&amp;journal=IEEE%20transactions%20on%20pattern%20analysis%20and%20machine%20intelligence&amp;doi=10.1109%2FTPAMI.2017.2771306&amp;volume=40&amp;issue=12&amp;pages=3007-3021&amp;publication_year=2018&amp;author=Liu%2CJ&amp;author=Shahroudy%2CA&amp;author=Xu%2CD&amp;author=Kot%2CAC&amp;author=Wang%2CG">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR27">Liu, L., Ouyang, W., Wang, X., Fieguth, P., Chen, J., Liu, X., et al. (2020). Deep learning for generic object detection: A survey. <i>International Journal of Computer Vision</i>, <i>128</i>(2), 261–318.</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1007/s11263-019-01247-4" data-track-action="article reference" href="https://doi.org/10.1007%2Fs11263-019-01247-4" aria-label="Article reference 27" data-doi="10.1007/s11263-019-01247-4">Article</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 27" href="http://scholar.google.com/scholar_lookup?&amp;title=Deep%20learning%20for%20generic%20object%20detection%3A%20A%20survey&amp;journal=International%20Journal%20of%20Computer%20Vision&amp;doi=10.1007%2Fs11263-019-01247-4&amp;volume=128&amp;issue=2&amp;pages=261-318&amp;publication_year=2020&amp;author=Liu%2CL&amp;author=Ouyang%2CW&amp;author=Wang%2CX&amp;author=Fieguth%2CP&amp;author=Chen%2CJ&amp;author=Liu%2CX&amp;author=Pietik%C3%A4inen%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR28">Ma, B., Zhang, J., Xia, Y., &amp; Tao, D. (2020). Auto learning attention. In <i>Advances in neural information processing systems</i> (Vol. 33).</p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR29">Mazhar, O., Ramdani, S., Navarro, B., Passama, R., &amp; Cherubini, A. (2018). Towards real-time physical human-robot interaction using skeleton information and hand gestures. In <i>Proceedings of the 2018 IEEE/RSJ international conference on intelligent robots and systems (IROS)</i> (pp. 1–6). IEEE.</p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR30">Newell, A., Yang, K., &amp; Deng, J. (2016). Stacked hourglass networks for human pose estimation. In <i>Proceedings of the European conference on computer vision (ECCV)</i> (pp. 483–499).</p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR31">Newell, A., Huang, Z., &amp; Deng, J. (2017). Associative embedding: End-to-end learning for joint detection and grouping. In <i>Advances in neural information processing systems</i> (pp. 2277–2287).</p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR32">Ni, B., Li, T., &amp; Yang, X. (2017). Learning semantic-aligned action representation. <i>IEEE Transactions on Neural Networks and Learning Systems</i>, <i>29</i>(8), 3715–3725.</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1109/TNNLS.2017.2731775" data-track-action="article reference" href="https://doi.org/10.1109%2FTNNLS.2017.2731775" aria-label="Article reference 32" data-doi="10.1109/TNNLS.2017.2731775">Article</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 32" href="http://scholar.google.com/scholar_lookup?&amp;title=Learning%20semantic-aligned%20action%20representation&amp;journal=IEEE%20Transactions%20on%20Neural%20Networks%20and%20Learning%20Systems&amp;doi=10.1109%2FTNNLS.2017.2731775&amp;volume=29&amp;issue=8&amp;pages=3715-3725&amp;publication_year=2017&amp;author=Ni%2CB&amp;author=Li%2CT&amp;author=Yang%2CX">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR33">Ouyang, W., Zeng, X., &amp; Wang, X. (2016). Learning mutual visibility relationship for pedestrian detection with a deep model. <i>International Journal of Computer Vision</i>, <i>120</i>(1), 14–27.</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1007/s11263-016-0890-9" data-track-action="article reference" href="https://doi.org/10.1007%2Fs11263-016-0890-9" aria-label="Article reference 33" data-doi="10.1007/s11263-016-0890-9">Article</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="mathscinet reference" href="http://www.ams.org/mathscinet-getitem?mr=3532298" aria-label="MathSciNet reference 33">MathSciNet</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 33" href="http://scholar.google.com/scholar_lookup?&amp;title=Learning%20mutual%20visibility%20relationship%20for%20pedestrian%20detection%20with%20a%20deep%20model&amp;journal=International%20Journal%20of%20Computer%20Vision&amp;doi=10.1007%2Fs11263-016-0890-9&amp;volume=120&amp;issue=1&amp;pages=14-27&amp;publication_year=2016&amp;author=Ouyang%2CW&amp;author=Zeng%2CX&amp;author=Wang%2CX">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR34">Papandreou, G., Zhu, T., Kanazawa, N., Toshev, A., Tompson, J., Bregler, C., &amp; Murphy, K. (2017). Towards accurate multi-person pose estimation in the wild. In <i>Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR)</i> (pp. 4903–4911).</p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR35">Papandreou, G., Zhu, T., Chen, LC., Gidaris, S., Tompson, J., &amp; Murphy, K. (2018) . Personlab: Person pose estimation and instance segmentation with a bottom-up, part-based, geometric embedding model. In <i>Proceedings of the European conference on computer vision (ECCV)</i> (pp. 269–286).</p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR36">Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E., DeVito, Z., Lin, Z., Desmaison, A., Antiga, L., &amp; Lerer, A. (2017). Automatic differentiation in pytorch. In <i>Advances in neural information processing systems workshops</i>.</p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR37">Pavlakos, G., Zhou, X., &amp; Daniilidis, K. (2018a). Ordinal depth supervision for 3d human pose estimation. In <i>Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR)</i> (pp. 7307–7316).</p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR38">Pavlakos, G., Zhu, L., Zhou, X., &amp; Daniilidis, K. (2018b). Learning to estimate 3d human pose and shape from a single color image. In <i>Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR)</i> (pp. 459–468).</p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR39">Pishchulin, L., Insafutdinov, E., Tang, S., Andres, B., Andriluka, M., Gehler, PV., &amp; Schiele, B. (2016). Deepcut: Joint subset partition and labeling for multi person pose estimation. In <i>Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR)</i> (pp. 4929–4937).</p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR40">Ren, S., He, K., Girshick, R., &amp; Sun, J. (2015). Faster r-cnn: Towards real-time object detection with region proposal networks. In <i>Advances in neural information processing systems</i> (pp. 91–99).</p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR41">Rhodin, H., Salzmann, M., &amp; Fua, P. (2018). Unsupervised geometry-aware representation for 3d human pose estimation. In <i>Proceedings of the European conference on computer vision (ECCV)</i> (pp. 750–767).</p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR42">Rogez, G., Rihan, J., Orrite-Uruñuela, C., &amp; Torr, P. H. (2012). Fast human pose detection using randomized hierarchical cascades of rejectors. <i>International Journal of Computer Vision</i>, <i>99</i>(1), 25–52.</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1007/s11263-012-0516-9" data-track-action="article reference" href="https://doi.org/10.1007%2Fs11263-012-0516-9" aria-label="Article reference 42" data-doi="10.1007/s11263-012-0516-9">Article</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="mathscinet reference" href="http://www.ams.org/mathscinet-getitem?mr=2917018" aria-label="MathSciNet reference 42">MathSciNet</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 42" href="http://scholar.google.com/scholar_lookup?&amp;title=Fast%20human%20pose%20detection%20using%20randomized%20hierarchical%20cascades%20of%20rejectors&amp;journal=International%20Journal%20of%20Computer%20Vision&amp;doi=10.1007%2Fs11263-012-0516-9&amp;volume=99&amp;issue=1&amp;pages=25-52&amp;publication_year=2012&amp;author=Rogez%2CG&amp;author=Rihan%2CJ&amp;author=Orrite-Uru%C3%B1uela%2CC&amp;author=Torr%2CPH">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR43">Sun, K., Xiao, B., Liu, D., &amp; Wang, J. (2019). Deep high-resolution representation learning for human pose estimation. In <i>Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR)</i> (pp. 5693–5703).</p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR44">Sun, X., Xiao, B., Wei, F., Liang, S., &amp; Wei, Y. (2018). Integral human pose regression. In <i>Proceedings of the European conference on computer vision (ECCV)</i> (pp. 529–545).</p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR45">Toshev, A., &amp; Szegedy, C. (2014). Deeppose: Human pose estimation via deep neural networks. In <i>Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR)</i> (pp. 1653–1660).</p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR46">Varadarajan, J., Subramanian, R., Bulò, S. R., Ahuja, N., Lanz, O., &amp; Ricci, E. (2018). Joint estimation of human pose and conversational groups from social scenes. <i>International Journal of Computer Vision</i>, <i>126</i>(2–4), 410–429.</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1007/s11263-017-1026-6" data-track-action="article reference" href="https://doi.org/10.1007%2Fs11263-017-1026-6" aria-label="Article reference 46" data-doi="10.1007/s11263-017-1026-6">Article</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="mathscinet reference" href="http://www.ams.org/mathscinet-getitem?mr=3766627" aria-label="MathSciNet reference 46">MathSciNet</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 46" href="http://scholar.google.com/scholar_lookup?&amp;title=Joint%20estimation%20of%20human%20pose%20and%20conversational%20groups%20from%20social%20scenes&amp;journal=International%20Journal%20of%20Computer%20Vision&amp;doi=10.1007%2Fs11263-017-1026-6&amp;volume=126&amp;issue=2%E2%80%934&amp;pages=410-429&amp;publication_year=2018&amp;author=Varadarajan%2CJ&amp;author=Subramanian%2CR&amp;author=Bul%C3%B2%2CSR&amp;author=Ahuja%2CN&amp;author=Lanz%2CO&amp;author=Ricci%2CE">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR47">Wagemans, J., Elder, JH., Kubovy, M., Palmer, SE., Peterson, MA., Singh, M., &amp; von der Heydt, R. (2012). A century of gestalt psychology in visual perception: I. perceptual grouping and figure–ground organization. Psychological bulletin 138(6):1172</p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR48">Wang, F., &amp; Li, Y. (2013). Beyond physical connections: Tree models in human pose estimation. In <i>Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR)</i> (pp. 596–603).</p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR49">Xiao, B., Wu, H., &amp; Wei, Y. (2018). Simple baselines for human pose estimation and tracking. In <i>Proceedings of the European conference on computer vision (ECCV)</i> (pp. 466–481).</p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR50">Yang, Q., Yang, R., Davis, J., &amp; Nistér, D. (2007). Spatial-depth super resolution for range images. In <i>Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR), IEEE</i> (pp. 1–8).</p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR51">Yang, W., Li, S., Ouyang, W., Li, H., &amp; Wang, X. (2017). Learning feature pyramids for human pose estimation. In <i>Proceedings of the IEEE international conference on computer vision (ICCV)</i> (pp. 1281–1290).</p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR52">Yang, W., Ouyang, W., Wang, X., Ren, J., Li, H., &amp; Wang, X. (2018). 3d human pose estimation in the wild by adversarial learning. In <i>Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR)</i> (pp. 5255–5264).</p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR53">Yang, Y., &amp; Ramanan, D. (2013). Articulated human detection with flexible mixtures of parts. <i>IEEE Transactions on Pattern Analysis and Machine Intelligence</i>, <i>35</i>(12), 2878–2890.</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1109/TPAMI.2012.261" data-track-action="article reference" href="https://doi.org/10.1109%2FTPAMI.2012.261" aria-label="Article reference 53" data-doi="10.1109/TPAMI.2012.261">Article</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 53" href="http://scholar.google.com/scholar_lookup?&amp;title=Articulated%20human%20detection%20with%20flexible%20mixtures%20of%20parts&amp;journal=IEEE%20Transactions%20on%20Pattern%20Analysis%20and%20Machine%20Intelligence&amp;doi=10.1109%2FTPAMI.2012.261&amp;volume=35&amp;issue=12&amp;pages=2878-2890&amp;publication_year=2013&amp;author=Yang%2CY&amp;author=Ramanan%2CD">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR54">Zhang, F., Zhu, X., Dai, H., Ye, M., &amp; Zhu, C. (2020). Distribution-aware coordinate representation for human pose estimation. In <i>Proceedings of the IEEE conference on computer vision and pattern recognition</i> (pp. 7093–7102).</p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR55">Zhang, H., Ouyang, H., Liu, S., Qi, X., Shen, X., Yang, R., &amp; Jia, J. (2019a). Human pose estimation with spatial contextual information. arXiv preprint <a href="http://arxiv.org/abs/1901.01760">arXiv:1901.01760</a></p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR56">Zhang, J., &amp; Tao, D. (2020). Empowering things with intelligence: A survey of the progress, challenges, and opportunities in artificial intelligence of things. <i>IEEE Internet of Things Journal</i>.</p></li><li class="c-article-references__item js-c-reading-companion-references-item"><p class="c-article-references__text" id="ref-CR57">Zhang, SH., &amp; Li, R., et al (2019b). Pose2seg: Detection free human instance segmentation. In <i>Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR)</i>.</p></li></ul><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" rel="nofollow" href="https://citation-needed.springer.com/v2/references/10.1007/s11263-021-01482-8?format=refman&amp;flavour=references">Download references<svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section></div><section aria-labelledby="author-information" data-title="Author information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Authors and Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">School of Computer Science, Faculty of Engineering, The University of Sydney, Darlington, NSW, 2008, Australia</p><p class="c-article-author-affiliation__authors-list">Jing Zhang, Zhe Chen &amp; Dacheng Tao</p></li></ol><div class="u-js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-Jing-Zhang"><span class="c-article-authors-search__title u-h3 js-search-name">Jing Zhang</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=Jing%20Zhang" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" rel="nofollow">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Jing%20Zhang" data-track="click" data-track-action="author link - pubmed" data-track-label="link" rel="nofollow">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Jing%20Zhang%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link" rel="nofollow">Google Scholar</a></span></p></div></div></li><li id="auth-Zhe-Chen"><span class="c-article-authors-search__title u-h3 js-search-name">Zhe Chen</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=Zhe%20Chen" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" rel="nofollow">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Zhe%20Chen" data-track="click" data-track-action="author link - pubmed" data-track-label="link" rel="nofollow">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Zhe%20Chen%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link" rel="nofollow">Google Scholar</a></span></p></div></div></li><li id="auth-Dacheng-Tao"><span class="c-article-authors-search__title u-h3 js-search-name">Dacheng Tao</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=Dacheng%20Tao" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" rel="nofollow">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Dacheng%20Tao" data-track="click" data-track-action="author link - pubmed" data-track-label="link" rel="nofollow">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Dacheng%20Tao%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link" rel="nofollow">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" href="mailto:dacheng.tao@sydney.edu.au">Dacheng Tao</a>.</p></div></div></section><section data-title="Additional information"><div class="c-article-section" id="additional-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="additional-information">Additional information</h2><div class="c-article-section__content" id="additional-information-content"><p>Communicated by Karteek Alahari.</p><h3 class="c-article__sub-heading">Publisher's Note</h3><p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p><p>This work was supported by Australian Research Council Projects FL-170100117, DP-180103424, IH-180100002.</p></div></div></section><section data-title="Rights and permissions"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=Towards%20High%20Performance%20Human%20Keypoint%20Detection&amp;author=Jing%20Zhang%20et%20al&amp;contentID=10.1007%2Fs11263-021-01482-8&amp;copyright=Crown&amp;publication=0920-5691&amp;publicationDate=2021-07-01&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info" data-title="About this article"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="u-hide-print c-bibliographic-information__column c-bibliographic-information__column--border"><a data-crossmark="10.1007/s11263-021-01482-8" target="_blank" rel="noopener" href="https://crossmark.crossref.org/dialog/?doi=10.1007/s11263-021-01482-8" data-track="click" data-track-action="Click Crossmark" data-track-label="link" data-test="crossmark"><img width="57" height="81" alt="Verify currency and authenticity via CrossMark" src="data:image/svg+xml;base64,<svg height="81" width="57" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd"><path d="m17.35 35.45 21.3-14.2v-17.03h-21.3" fill="#989898"/><path d="m38.65 35.45-21.3-14.2v-17.03h21.3" fill="#747474"/><path d="m28 .5c-12.98 0-23.5 10.52-23.5 23.5s10.52 23.5 23.5 23.5 23.5-10.52 23.5-23.5c0-6.23-2.48-12.21-6.88-16.62-4.41-4.4-10.39-6.88-16.62-6.88zm0 41.25c-9.8 0-17.75-7.95-17.75-17.75s7.95-17.75 17.75-17.75 17.75 7.95 17.75 17.75c0 4.71-1.87 9.22-5.2 12.55s-7.84 5.2-12.55 5.2z" fill="#535353"/><path d="m41 36c-5.81 6.23-15.23 7.45-22.43 2.9-7.21-4.55-10.16-13.57-7.03-21.5l-4.92-3.11c-4.95 10.7-1.19 23.42 8.78 29.71 9.97 6.3 23.07 4.22 30.6-4.86z" fill="#9c9c9c"/><path d="m.2 58.45c0-.75.11-1.42.33-2.01s.52-1.09.91-1.5c.38-.41.83-.73 1.34-.94.51-.22 1.06-.32 1.65-.32.56 0 1.06.11 1.51.35.44.23.81.5 1.1.81l-.91 1.01c-.24-.24-.49-.42-.75-.56-.27-.13-.58-.2-.93-.2-.39 0-.73.08-1.05.23-.31.16-.58.37-.81.66-.23.28-.41.63-.53 1.04-.13.41-.19.88-.19 1.39 0 1.04.23 1.86.68 2.46.45.59 1.06.88 1.84.88.41 0 .77-.07 1.07-.23s.59-.39.85-.68l.91 1c-.38.43-.8.76-1.28.99-.47.22-1 .34-1.58.34-.59 0-1.13-.1-1.64-.31-.5-.2-.94-.51-1.31-.91-.38-.4-.67-.9-.88-1.48-.22-.59-.33-1.26-.33-2.02zm8.4-5.33h1.61v2.54l-.05 1.33c.29-.27.61-.51.96-.72s.76-.31 1.24-.31c.73 0 1.27.23 1.61.71.33.47.5 1.14.5 2.02v4.31h-1.61v-4.1c0-.57-.08-.97-.25-1.21-.17-.23-.45-.35-.83-.35-.3 0-.56.08-.79.22-.23.15-.49.36-.78.64v4.8h-1.61zm7.37 6.45c0-.56.09-1.06.26-1.51.18-.45.42-.83.71-1.14.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.36c.07.62.29 1.1.65 1.44.36.33.82.5 1.38.5.29 0 .57-.04.83-.13s.51-.21.76-.37l.55 1.01c-.33.21-.69.39-1.09.53-.41.14-.83.21-1.26.21-.48 0-.92-.08-1.34-.25-.41-.16-.76-.4-1.07-.7-.31-.31-.55-.69-.72-1.13-.18-.44-.26-.95-.26-1.52zm4.6-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.07.45-.31.29-.5.73-.58 1.3zm2.5.62c0-.57.09-1.08.28-1.53.18-.44.43-.82.75-1.13s.69-.54 1.1-.71c.42-.16.85-.24 1.31-.24.45 0 .84.08 1.17.23s.61.34.85.57l-.77 1.02c-.19-.16-.38-.28-.56-.37-.19-.09-.39-.14-.61-.14-.56 0-1.01.21-1.35.63-.35.41-.52.97-.52 1.67 0 .69.17 1.24.51 1.66.34.41.78.62 1.32.62.28 0 .54-.06.78-.17.24-.12.45-.26.64-.42l.67 1.03c-.33.29-.69.51-1.08.65-.39.15-.78.23-1.18.23-.46 0-.9-.08-1.31-.24-.4-.16-.75-.39-1.05-.7s-.53-.69-.7-1.13c-.17-.45-.25-.96-.25-1.53zm6.91-6.45h1.58v6.17h.05l2.54-3.16h1.77l-2.35 2.8 2.59 4.07h-1.75l-1.77-2.98-1.08 1.23v1.75h-1.58zm13.69 1.27c-.25-.11-.5-.17-.75-.17-.58 0-.87.39-.87 1.16v.75h1.34v1.27h-1.34v5.6h-1.61v-5.6h-.92v-1.2l.92-.07v-.72c0-.35.04-.68.13-.98.08-.31.21-.57.4-.79s.42-.39.71-.51c.28-.12.63-.18 1.04-.18.24 0 .48.02.69.07.22.05.41.1.57.17zm.48 5.18c0-.57.09-1.08.27-1.53.17-.44.41-.82.72-1.13.3-.31.65-.54 1.04-.71.39-.16.8-.24 1.23-.24s.84.08 1.24.24c.4.17.74.4 1.04.71s.54.69.72 1.13c.19.45.28.96.28 1.53s-.09 1.08-.28 1.53c-.18.44-.42.82-.72 1.13s-.64.54-1.04.7-.81.24-1.24.24-.84-.08-1.23-.24-.74-.39-1.04-.7c-.31-.31-.55-.69-.72-1.13-.18-.45-.27-.96-.27-1.53zm1.65 0c0 .69.14 1.24.43 1.66.28.41.68.62 1.18.62.51 0 .9-.21 1.19-.62.29-.42.44-.97.44-1.66 0-.7-.15-1.26-.44-1.67-.29-.42-.68-.63-1.19-.63-.5 0-.9.21-1.18.63-.29.41-.43.97-.43 1.67zm6.48-3.44h1.33l.12 1.21h.05c.24-.44.54-.79.88-1.02.35-.24.7-.36 1.07-.36.32 0 .59.05.78.14l-.28 1.4-.33-.09c-.11-.01-.23-.02-.38-.02-.27 0-.56.1-.86.31s-.55.58-.77 1.1v4.2h-1.61zm-47.87 15h1.61v4.1c0 .57.08.97.25 1.2.17.24.44.35.81.35.3 0 .57-.07.8-.22.22-.15.47-.39.73-.73v-4.7h1.61v6.87h-1.32l-.12-1.01h-.04c-.3.36-.63.64-.98.86-.35.21-.76.32-1.24.32-.73 0-1.27-.24-1.61-.71-.33-.47-.5-1.14-.5-2.02zm9.46 7.43v2.16h-1.61v-9.59h1.33l.12.72h.05c.29-.24.61-.45.97-.63.35-.17.72-.26 1.1-.26.43 0 .81.08 1.15.24.33.17.61.4.84.71.24.31.41.68.53 1.11.13.42.19.91.19 1.44 0 .59-.09 1.11-.25 1.57-.16.47-.38.85-.65 1.16-.27.32-.58.56-.94.73-.35.16-.72.25-1.1.25-.3 0-.6-.07-.9-.2s-.59-.31-.87-.56zm0-2.3c.26.22.5.37.73.45.24.09.46.13.66.13.46 0 .84-.2 1.15-.6.31-.39.46-.98.46-1.77 0-.69-.12-1.22-.35-1.61-.23-.38-.61-.57-1.13-.57-.49 0-.99.26-1.52.77zm5.87-1.69c0-.56.08-1.06.25-1.51.16-.45.37-.83.65-1.14.27-.3.58-.54.93-.71s.71-.25 1.08-.25c.39 0 .73.07 1 .2.27.14.54.32.81.55l-.06-1.1v-2.49h1.61v9.88h-1.33l-.11-.74h-.06c-.25.25-.54.46-.88.64-.33.18-.69.27-1.06.27-.87 0-1.56-.32-2.07-.95s-.76-1.51-.76-2.65zm1.67-.01c0 .74.13 1.31.4 1.7.26.38.65.58 1.15.58.51 0 .99-.26 1.44-.77v-3.21c-.24-.21-.48-.36-.7-.45-.23-.08-.46-.12-.7-.12-.45 0-.82.19-1.13.59-.31.39-.46.95-.46 1.68zm6.35 1.59c0-.73.32-1.3.97-1.71.64-.4 1.67-.68 3.08-.84 0-.17-.02-.34-.07-.51-.05-.16-.12-.3-.22-.43s-.22-.22-.38-.3c-.15-.06-.34-.1-.58-.1-.34 0-.68.07-1 .2s-.63.29-.93.47l-.59-1.08c.39-.24.81-.45 1.28-.63.47-.17.99-.26 1.54-.26.86 0 1.51.25 1.93.76s.63 1.25.63 2.21v4.07h-1.32l-.12-.76h-.05c-.3.27-.63.48-.98.66s-.73.27-1.14.27c-.61 0-1.1-.19-1.48-.56-.38-.36-.57-.85-.57-1.46zm1.57-.12c0 .3.09.53.27.67.19.14.42.21.71.21.28 0 .54-.07.77-.2s.48-.31.73-.56v-1.54c-.47.06-.86.13-1.18.23-.31.09-.57.19-.76.31s-.33.25-.41.4c-.09.15-.13.31-.13.48zm6.29-3.63h-.98v-1.2l1.06-.07.2-1.88h1.34v1.88h1.75v1.27h-1.75v3.28c0 .8.32 1.2.97 1.2.12 0 .24-.01.37-.04.12-.03.24-.07.34-.11l.28 1.19c-.19.06-.4.12-.64.17-.23.05-.49.08-.76.08-.4 0-.74-.06-1.02-.18-.27-.13-.49-.3-.67-.52-.17-.21-.3-.48-.37-.78-.08-.3-.12-.64-.12-1.01zm4.36 2.17c0-.56.09-1.06.27-1.51s.41-.83.71-1.14c.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.37c.08.62.29 1.1.65 1.44.36.33.82.5 1.38.5.3 0 .58-.04.84-.13.25-.09.51-.21.76-.37l.54 1.01c-.32.21-.69.39-1.09.53s-.82.21-1.26.21c-.47 0-.92-.08-1.33-.25-.41-.16-.77-.4-1.08-.7-.3-.31-.54-.69-.72-1.13-.17-.44-.26-.95-.26-1.52zm4.61-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.08.45-.31.29-.5.73-.57 1.3zm3.01 2.23c.31.24.61.43.92.57.3.13.63.2.98.2.38 0 .65-.08.83-.23s.27-.35.27-.6c0-.14-.05-.26-.13-.37-.08-.1-.2-.2-.34-.28-.14-.09-.29-.16-.47-.23l-.53-.22c-.23-.09-.46-.18-.69-.3-.23-.11-.44-.24-.62-.4s-.33-.35-.45-.55c-.12-.21-.18-.46-.18-.75 0-.61.23-1.1.68-1.49.44-.38 1.06-.57 1.83-.57.48 0 .91.08 1.29.25s.71.36.99.57l-.74.98c-.24-.17-.49-.32-.73-.42-.25-.11-.51-.16-.78-.16-.35 0-.6.07-.76.21-.17.15-.25.33-.25.54 0 .14.04.26.12.36s.18.18.31.26c.14.07.29.14.46.21l.54.19c.23.09.47.18.7.29s.44.24.64.4c.19.16.34.35.46.58.11.23.17.5.17.82 0 .3-.06.58-.17.83-.12.26-.29.48-.51.68-.23.19-.51.34-.84.45-.34.11-.72.17-1.15.17-.48 0-.95-.09-1.41-.27-.46-.19-.86-.41-1.2-.68z" fill="#535353"/></g></svg>"></a></div><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Zhang, J., Chen, Z. &amp; Tao, D. Towards High Performance Human Keypoint Detection.
                    <i>Int J Comput Vis</i> <b>129</b>, 2639–2662 (2021). https://doi.org/10.1007/s11263-021-01482-8</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" data-track-external="" rel="nofollow" href="https://citation-needed.springer.com/v2/references/10.1007/s11263-021-01482-8?format=refman&amp;flavour=citation">Download citation<svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2020-02-04">04 February 2020</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2021-05-24">24 May 2021</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2021-07-01">01 July 2021</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Issue Date<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2021-09">September 2021</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--doi"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value">https://doi.org/10.1007/s11263-021-01482-8</span></p></li></ul><div data-component="share-box" class="u-mt-0"><div class="c-article-share-box u-display-none" hidden=""><h3 class="c-article__sub-heading u-mt-0">Share this article</h3><p class="c-article-share-box__description">Anyone you share the following link with will be able to read this content:</p><button class="js-get-share-url c-article-share-box__button" type="button" id="get-share-url" data-track="click" data-track-label="button" data-track-external="" data-track-action="get shareable link">Get shareable link</button><div class="js-no-share-url-container u-display-none" hidden=""><p class="js-c-article-share-box__no-sharelink-info c-article-share-box__no-sharelink-info">Sorry, a shareable link is not currently available for this article.</p></div><div class="js-share-url-container u-display-none" hidden=""><p class="js-share-url c-article-share-box__only-read-input" id="share-url" data-track="click" data-track-label="button" data-track-action="select share url"></p><button class="js-copy-share-url c-article-share-box__button--link-like" type="button" id="copy-share-url" data-track="click" data-track-label="button" data-track-action="copy share url" data-track-external="">Copy to clipboard</button></div><p class="js-c-article-share-box__additional-info c-article-share-box__additional-info">
                            Provided by the Springer Nature SharedIt content-sharing initiative
                        </p></div></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span>Human Pose Estimation</span></li><li class="c-article-subject-list__subject"><span>Deep Nerual Networks</span></li><li class="c-article-subject-list__subject"><span>Sub-pixel Refinement</span></li><li class="c-article-subject-list__subject"><span>Context</span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>
                </div>
            </article>
        </main>

        <div class="c-article-extras u-text-sm u-hide-print" id="sidebar" data-container-type="reading-companion" data-track-component="reading companion">
            <aside>
                
                    
                        
                            <div class="u-mb-16 u-clear-both">
                                <a href="//wayf.springernature.com?redirect_uri&#x3D;https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs11263-021-01482-8" class="u-button u-button--full-width u-button--primary u-justify-content-space-between c-pdf-download__link" data-track="click" data-track-action="institution access" data-track-label="button">
                                    <span data-test="access-via-institution">Access via your institution</span>
                                    <svg aria-hidden="true" focusable="false" width="16" height="16" class="u-icon"><use xlink:href="#icon-springer-arrow-right"></use></svg>
                                </a>
                            </div>
                        
                    
                

                <div data-test="download-article-link-wrapper" class="js-context-bar-sticky-point-desktop">
                    
                </div>

                

                <div data-test="collections">
                    
    

                </div>

                <div data-test="editorial-summary">
                    
                </div>

                <div class="c-reading-companion">
                    <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
                        
                            
                                <div data-test="buy-box-desktop" class="c-article-buy-box">
                                    <div class="sprcom-buybox-articleSidebar" id="sprcom-buybox-articleSidebar">
 <!-- rendered: 2023-04-27T09:30:32.337671 -->
 <h2 class="c-box__heading">Access options</h2>
 <article class="c-box buying-option" data-test-id="buy-article">
  <h3 class="c-box__heading">Buy single article</h3>
  <div class="c-box__body">
   <div class="buybox__info">
    <p>Instant access to the full article PDF.</p>
   </div>
   <div class="buybox__buy">
    <p class="buybox__price">39,95 €</p>
    <p class="buybox__price-info">Price includes VAT (New Zealand)<br></p>
    <form action="https://order.springer.com/public/cart" method="post">
     <input type="hidden" name="type" value="article"><input type="hidden" name="doi" value="10.1007/s11263-021-01482-8"><input type="hidden" name="isxn" value="1573-1405"><input type="hidden" name="contenttitle" value="Towards High Performance Human Keypoint Detection"><input type="hidden" name="copyrightyear" value="2021"><input type="hidden" name="year" value="2021"><input type="hidden" name="authors" value="Jing Zhang, Zhe Chen, Dacheng Tao"><input type="hidden" name="title" value="International Journal of Computer Vision"><input type="hidden" name="mac" value="13FE6EA62CF0EDCF43FF37668A2C96BF"><input type="submit" class="c-box__button" onclick="dataLayer.push({&quot;event&quot;:&quot;addToCart&quot;,&quot;ecommerce&quot;:{&quot;currencyCode&quot;:&quot;EUR&quot;,&quot;add&quot;:{&quot;products&quot;:[{&quot;name&quot;:&quot;Towards High Performance Human Keypoint Detection&quot;,&quot;id&quot;:&quot;1573-1405&quot;,&quot;price&quot;:39.95,&quot;brand&quot;:&quot;Springer US&quot;,&quot;category&quot;:&quot;Computer Science&quot;,&quot;variant&quot;:&quot;ppv-article&quot;,&quot;quantity&quot;:1}]}}});" value="Buy article PDF">
    </form>
   </div>
  </div>
  <script>dataLayer.push({"ecommerce":{"currency":"EUR","impressions":[{"name":"Towards High Performance Human Keypoint Detection","id":"1573-1405","price":39.95,"brand":"Springer US","category":"Computer Science","variant":"ppv-article","quantity":1}]}});</script>
 </article>
 <article class="c-box buybox__rent-article" id="deepdyve" style="display: none" data-test-id="journal-subscription">
  <div class="c-box__body">
   <div class="buybox__info">
    <p><a class="deepdyve-link" target="deepdyve" rel="nofollow" data-track="click" data-track-action="rent article" data-track-label="rent action, new buybox">Rent this article via DeepDyve.</a></p>
   </div>
  </div>
  <script>
            function deepDyveResponse(data) {
                if (data.status === 'ok') {
                    [].slice.call(document.querySelectorAll('.c-box.buybox__rent-article')).forEach(function (article) {
                        article.style.display = 'flex'
                        var link = article.querySelector('.deepdyve-link')
                        if (link) {
                            link.setAttribute('href', data.url)
                        }
                    })
                }
            }

            var script = document.createElement('script')
            script.src = '//www.deepdyve.com/rental-link?docId=10.1007/s11263-021-01482-8&journal=1573-1405&fieldName=journal_doi&affiliateId=springer&format=jsonp&callback=deepDyveResponse'
            document.body.appendChild(script)
          </script>
 </article>
 <aside class="buybox__institutional-sub">
  <div class="c-box__body">
   <div class="buybox__info">
    <p><a href="https://www.springernature.com/gp/librarians/licensing/license-options?&amp;abtest=v2" data-track="click" data-track-action="institutional link" data-track-label="institutional subscriptions, new buybox">Learn more about Institutional subscriptions</a></p>
   </div>
  </div>
 </aside>
 <style>.sprcom-buybox-articleSidebar{
  box-shadow: 0px 0px 5px rgba(51,51,51,0.101);
  display: flex;
  flex-wrap: wrap;
  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen-Sans, Ubuntu, Cantarell, 'Helvetica Neue', sans-serif;
  text-align: center;
}
.sprcom-buybox-articleSidebar *{
  box-sizing: border-box;
  line-height: calc(100% + 4px);
  margin: 0px;
}
.sprcom-buybox-articleSidebar > *{
  display: flex;
  flex-basis: 240px;
  flex-direction: column;
  flex-grow: 1;
  flex-shrink: 1;
  margin: 0.5px;
}
.sprcom-buybox-articleSidebar > *{
  box-shadow: 0 0 0 1px rgba(204,204,204,0.494);
}
.sprcom-buybox-articleSidebar .c-box__body{
  display: flex;
  flex-direction: column-reverse;
  flex-grow: 1;
  justify-content: space-between;
  padding: 6%;
}
.sprcom-buybox-articleSidebar .c-box__body .buybox__buy{
  display: flex;
  flex-direction: column-reverse;
}
.sprcom-buybox-articleSidebar p{
  color: #333;
  font-size: 15px;
}
.sprcom-buybox-articleSidebar .buybox__price{
  font-size: 24px;
  font-weight: 500;
  line-height: calc(100% + 8px);
  margin: 20px 0;
  order: 1;
}
.sprcom-buybox-articleSidebar form{
  order: 1;
}
.sprcom-buybox-articleSidebar .buybox__price-info{
  margin-bottom: 20px;
}
.sprcom-buybox-articleSidebar .c-box__heading{
  background-color: #f0f0f0;
  color: #333;
  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen-Sans, Ubuntu, Cantarell, 'Helvetica Neue', sans-serif;
  font-size: 16px;
  margin: 0px;
  padding: 10px 12px;
  text-align: center;
}
.sprcom-buybox-articleSidebar .c-box__button{
  background-color: #3365A4;
  border: 1px solid transparent;
  border-radius: 2px;
  color: #fff;
  cursor: pointer;
  display: inline-block;
  font-family: inherit;
  font-size: 16px;
  max-width: 222px;
  padding: 10px 12px;
  text-decoration: none;
  width: 100%;
}
.sprcom-buybox-articleSidebar h3{
  clip: rect(1px, 1px, 1px, 1px);
  height: 1px;
  overflow: hidden;
  position: absolute;
  width: 1px;
}
.sprcom-buybox-articleSidebar h2{
  flex-basis: 100%;
  margin-bottom: 16px;
  text-align: left;
}
.sprcom-buybox-articleSidebar .buybox__institutional-sub, .buybox__rent-article .c-box__body{
  flex-direction: row;
}
.sprcom-buybox-articleSidebar .buybox__institutional-sub, .buybox__rent-article .buybox__info{
  text-align: left;
}
.sprcom-buybox-articleSidebar .buybox__institutional-sub{
  background-color: #f0f0f0;
}
.sprcom-buybox-articleSidebar .visually-hidden{
  clip: rect(1px, 1px, 1px, 1px);
  height: 1px;
  overflow: hidden;
  position: absolute;
  width: 1px;
}
.sprcom-buybox-articleSidebar style{
  display: none;
}
</style>
 <script style="display: none">
                ;(function () {
                    var timestamp = Date.now()
                    document.write('<div data-id="id_'+ timestamp +'"></div>')

                    var head = document.getElementsByTagName("head")[0]
                    var script = document.createElement("script")
                    script.type = "text/javascript"
                    script.src = "https://buy.springer.com/assets/js/buybox-bundle-abe5f44a67.js"
                    script.id = "ecommerce-scripts-" + timestamp
                    head.appendChild(script)

                    var buybox = document.querySelector("[data-id=id_"+ timestamp +"]").parentNode

                    ;[].slice.call(buybox.querySelectorAll(".buying-option")).forEach(init)

                    function init(buyingOption, index) {
                        var form = buyingOption.querySelector("form")

                        if (form) {
                            var formAction = form.getAttribute("action")
                            document.querySelector("#ecommerce-scripts-" + timestamp).addEventListener("load", bindModal(form, formAction, timestamp, index), false)
                        }
                    }

                    function bindModal(form, formAction, timestamp, index) {
                        var weHasBrowserSupport = window.fetch && Array.from

                        return function() {
                            console.log("ecommerce-scripts loaded, attempting to init modal …")
                            var Buybox = EcommScripts ? EcommScripts.Buybox : null
                            var Modal = EcommScripts ? EcommScripts.Modal : null
                            
                            if (weHasBrowserSupport && Buybox && Modal) {
                                var modalID = "ecomm-modal_" + timestamp + "_" + index
                                
                                var modal = new Modal(modalID)
                                modal.domEl.addEventListener("close", close)
                                function close() {
                                    form.querySelector("button[type=submit]").focus()
                                }

                                var cartURL = "/cart"
                                var cartModalURL = "/cart?messageOnly=1"

                                form.setAttribute(
                                    "action",
                                    formAction.replace(cartURL, cartModalURL)
                                )
                                
                                var formSubmit = Buybox.interceptFormSubmit(
                                    Buybox.fetchFormAction(window.fetch),
                                    function(responseBody) {
                                        document.body.dispatchEvent(new Event("updatedCart"))
                                        Buybox.triggerModalAfterAddToCartSuccess(modal)(responseBody)
                                    },
                                    function() {
                                        form.removeEventListener("submit", formSubmit, false)
                                        form.setAttribute(
                                            "action",
                                            formAction.replace(cartModalURL, cartURL)
                                        )
                                        form.submit()
                                    }
                                )

                                form.addEventListener("submit", formSubmit, false)
                                
                                document.body.appendChild(modal.domEl)
                            } else {
                                console.log("binding failed:", weHasBrowserSupport, EcommScripts)
                            }
                        }
                    }
                })()
              </script>
</div>
                                </div>
                            
                        

                        <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                            <div class="u-lazy-ad-wrapper u-mt-16 u-hide" data-component-mpu><div class="c-ad c-ad--300x250">
    <div class="c-ad__inner">
        <p class="c-ad__label">Advertisement</p>
        <div id="div-gpt-ad-MPU1"
             class="div-gpt-ad grade-c-hide"
             data-pa11y-ignore
             data-gpt
             data-gpt-unitpath="/270604982/springerlink/11263/article"
             data-gpt-sizes="300x250" data-test="MPU1-ad"
             data-gpt-targeting="pos=MPU1;articleid=s11263-021-01482-8;">
        </div>
    </div>
</div>

</div>
                        </div>
                        <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
                        <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
                    </div>
                </div>
            </aside>
        </div>
    </div>

    
    <script>
            
    </script>
    
        
    <footer class="app-footer" role="contentinfo" data-test="springerlink-footer">
        <div class="app-footer__aside-wrapper u-hide-print">
            <div class="app-footer__container">
                <p class="app-footer__strapline">Over 10 million scientific documents at your fingertips</p>
                
                    <div class="app-footer__edition" data-component="SV.EditionSwitcher">
                        <span class="u-visually-hidden" data-role="button-dropdown__title" data-btn-text="Switch between Academic & Corporate Edition">Switch Edition</span>
                        <ul class="app-footer-edition-list" data-role="button-dropdown__content" data-test="footer-edition-switcher-list">
                            <li class="selected">
                                <a data-test="footer-academic-link"
                                   href="/siteEdition/link"
                                   id="siteedition-academic-link">Academic Edition</a>
                            </li>
                            <li>
                                <a data-test="footer-corporate-link"
                                   href="/siteEdition/rd"
                                   id="siteedition-corporate-link">Corporate Edition</a>
                            </li>
                        </ul>
                    </div>
                
            </div>
        </div>
        <div class="app-footer__container">
            <ul class="app-footer__nav u-hide-print">
                <li><a href="/">Home</a></li>
                <li><a href="/impressum">Impressum</a></li>
                <li><a href="/termsandconditions">Legal information</a></li>
                <li><a href="/privacystatement">Privacy statement</a></li>
                <li><a href="https://www.springernature.com/ccpa">California Privacy Statement</a></li>
                <li><a href="/cookiepolicy">How we use cookies</a></li>
                
                <li><a class="optanon-toggle-display" data-cc-action="preferences" href="javascript:void(0);">Manage cookies/Do not sell my data</a></li>
                
                <li><a href="/accessibility">Accessibility</a></li>
                <li><a href="https://support.springer.com/en/support/home">FAQ</a></li>
                <li><a id="contactus-footer-link" href="https://support.springer.com/en/support/solutions/articles/6000206179-contacting-us">Contact us</a></li>
                <li><a href="https://www.springer.com/gp/shop/promo/affiliate/springer-nature">Affiliate program</a></li>
            </ul>
            <div class="c-user-metadata">
    
        <p class="c-user-metadata__item">
            <span data-test="footer-user-login-status">Not logged in</span>
            <span data-test="footer-user-ip"> - 203.211.111.109</span>
        </p>
        <p class="c-user-metadata__item" data-test="footer-business-partners">
            Not affiliated
        </p>
    
</div>

            <a class="app-footer__parent-logo" target="_blank" rel="noopener" href="//www.springernature.com"  title="Go to Springer Nature">
                <span class="u-visually-hidden">Springer Nature</span>
                <svg width="125" height="12" focusable="false" aria-hidden="true">
                    <image width="125" height="12" alt="Springer Nature logo"
                           src=/oscar-static/images/springerlink/png/springernature-60a72a849b.png
                           xmlns:xlink="http://www.w3.org/1999/xlink"
                           xlink:href=/oscar-static/images/springerlink/svg/springernature-b88bf25ad4.svg>
                    </image>
                </svg>
            </a>
            <p class="app-footer__copyright">&copy; 2023 Springer Nature Switzerland AG. Part of <a target="_blank" rel="noopener" href="//www.springernature.com">Springer Nature</a>.</p>
            
        </div>
    </footer>



    </div>
    
    

    
    <svg class="u-hide hide">
        <symbol id="global-icon-chevron-right" viewBox="0 0 10 10" xmlns="http://www.w3.org/2000/svg">
            <path d="m5.96738168 4.70639573 2.39518594-2.41447274c.37913917-.38219212.98637524-.38972225 1.35419292-.01894278.37750606.38054586.37784436.99719163-.00013556 1.37821513l-4.03074001 4.06319683c-.37758093.38062133-.98937525.38100976-1.367372-.00003075l-4.03091981-4.06337806c-.37759778-.38063832-.38381821-.99150444-.01600053-1.3622839.37750607-.38054587.98772445-.38240057 1.37006824.00302197l2.39538588 2.4146743.96295325.98624457z" fill="currentColor" fill-rule="evenodd" transform="matrix(0 -1 1 0 0 10)"/>
        </symbol>
        <symbol id="global-icon-download" viewBox="0 0 16 16">
            <path d="M2 14c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H3.002A1.006 1.006 0 012 14zM9 2v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L3.115 7.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L7 8.8V2c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-email" viewBox="0 0 18 18">
            <path d="M1.995 2h14.01A2 2 0 0118 4.006v9.988A2 2 0 0116.005 16H1.995A2 2 0 010 13.994V4.006A2 2 0 011.995 2zM1 13.994A1 1 0 001.995 15h14.01A1 1 0 0017 13.994V4.006A1 1 0 0016.005 3H1.995A1 1 0 001 4.006zM9 11L2 7V5.557l7 4 7-4V7z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-institution" viewBox="0 0 18 18">
            <path d="M14 8a1 1 0 011 1v6h1.5a.5.5 0 01.5.5v.5h.5a.5.5 0 01.5.5V18H0v-1.5a.5.5 0 01.5-.5H1v-.5a.5.5 0 01.5-.5H3V9a1 1 0 112 0v6h8V9a1 1 0 011-1zM6 8l2 1v4l-2 1zm6 0v6l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0116.081 7H1.92a.92.92 0 01-.528-1.674L8.427.401a1 1 0 011.146 0zM9 2.441L5.345 5h7.31z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-search" viewBox="0 0 22 22">
            <path fill-rule="evenodd" d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z"/>
        </symbol>
        <symbol id="icon-info" viewBox="0 0 18 18">
            <path d="m9 0c4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9zm0 7h-1.5l-.11662113.00672773c-.49733868.05776511-.88337887.48043643-.88337887.99327227 0 .47338693.32893365.86994729.77070917.97358929l.1126697.01968298.11662113.00672773h.5v3h-.5l-.11662113.0067277c-.42082504.0488782-.76196299.3590206-.85696816.7639815l-.01968298.1126697-.00672773.1166211.00672773.1166211c.04887817.4208251.35902055.761963.76398144.8569682l.1126697.019683.11662113.0067277h3l.1166211-.0067277c.4973387-.0577651.8833789-.4804365.8833789-.9932723 0-.4733869-.3289337-.8699473-.7707092-.9735893l-.1126697-.019683-.1166211-.0067277h-.5v-4l-.00672773-.11662113c-.04887817-.42082504-.35902055-.76196299-.76398144-.85696816l-.1126697-.01968298zm0-3.25c-.69035594 0-1.25.55964406-1.25 1.25s.55964406 1.25 1.25 1.25 1.25-.55964406 1.25-1.25-.55964406-1.25-1.25-1.25z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="icon-success" viewBox="0 0 18 18">
            <path d="M9 0a9 9 0 110 18A9 9 0 019 0zm3.486 4.982l-4.718 5.506L5.14 8.465a.991.991 0 00-1.423.133 1.06 1.06 0 00.13 1.463l3.407 2.733a1 1 0 001.387-.133l5.385-6.334a1.06 1.06 0 00-.116-1.464.991.991 0 00-1.424.119z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="icon-chevron-down" viewBox="0 0 16 16">
            <path d="m5.58578644 3-3.29289322-3.29289322c-.39052429-.39052429-.39052429-1.02368927 0-1.41421356s1.02368927-.39052429 1.41421356 0l4 4c.39052429.39052429.39052429 1.02368927 0 1.41421356l-4 4c-.39052429.39052429-1.02368927.39052429-1.41421356 0s-.39052429-1.02368927 0-1.41421356z" fill-rule="evenodd" transform="matrix(0 1 -1 0 11 1)"/>
        </symbol>
        <symbol id="icon-warning" viewBox="0 0 18 18">
            <path d="m9 11.75c.69035594 0 1.25.5596441 1.25 1.25s-.55964406 1.25-1.25 1.25-1.25-.5596441-1.25-1.25.55964406-1.25 1.25-1.25zm.41320045-7.75c.55228475 0 1.00000005.44771525 1.00000005 1l-.0034543.08304548-.3333333 4c-.043191.51829212-.47645714.91695452-.99654578.91695452h-.15973424c-.52008864 0-.95335475-.3986624-.99654576-.91695452l-.33333333-4c-.04586475-.55037702.36312325-1.03372649.91350028-1.07959124l.04148683-.00259031zm-.41320045 14c-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9 4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="icon-plus" viewBox="0 0 16 16">
            <path d="m2.00087166 7h4.99912834v-4.99912834c0-.55276616.44386482-1.00087166 1-1.00087166.55228475 0 1 .44463086 1 1.00087166v4.99912834h4.9991283c.5527662 0 1.0008717.44386482 1.0008717 1 0 .55228475-.4446309 1-1.0008717 1h-4.9991283v4.9991283c0 .5527662-.44386482 1.0008717-1 1.0008717-.55228475 0-1-.4446309-1-1.0008717v-4.9991283h-4.99912834c-.55276616 0-1.00087166-.44386482-1.00087166-1 0-.55228475.44463086-1 1.00087166-1z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="icon-minus" viewBox="0 0 16 16">
            <path d="m2.00087166 7h11.99825664c.5527662 0 1.0008717.44386482 1.0008717 1 0 .55228475-.4446309 1-1.0008717 1h-11.99825664c-.55276616 0-1.00087166-.44386482-1.00087166-1 0-.55228475.44463086-1 1.00087166-1z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="icon-error" viewBox="0 0 18 18">
            <path d="m9 0c4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9zm2.8630343 4.71100931-2.8630343 2.86303426-2.86303426-2.86303426c-.39658757-.39658757-1.03281091-.39438847-1.4265779-.00062147-.39651227.39651226-.39348876 1.03246767.00062147 1.4265779l2.86303426 2.86303426-2.86303426 2.8630343c-.39658757.3965875-.39438847 1.0328109-.00062147 1.4265779.39651226.3965122 1.03246767.3934887 1.4265779-.0006215l2.86303426-2.8630343 2.8630343 2.8630343c.3965875.3965876 1.0328109.3943885 1.4265779.0006215.3965122-.3965123.3934887-1.0324677-.0006215-1.4265779l-2.8630343-2.8630343 2.8630343-2.86303426c.3965876-.39658757.3943885-1.03281091.0006215-1.4265779-.3965123-.39651227-1.0324677-.39348876-1.4265779.00062147z" fill="currentColor" fill-rule="evenodd"/>
        </symbol>
        <symbol id="icon-springer-arrow-left">
            <path d="M15 7a1 1 0 000-2H3.385l2.482-2.482a.994.994 0 00.02-1.403 1.001 1.001 0 00-1.417 0L.294 5.292a1.001 1.001 0 000 1.416l4.176 4.177a.991.991 0 001.4.016 1 1 0 00-.003-1.42L3.385 7H15z"/>
        </symbol>
        <symbol id="icon-springer-arrow-right">
            <path d="M1 7a1 1 0 010-2h11.615l-2.482-2.482a.994.994 0 01-.02-1.403 1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L12.615 7H1z"/>
        </symbol>
        <symbol id="icon-arrow-up" viewBox="0 0 16 16">
            <path d="m12.716625 4.46975946-4.03074003-4.17620792c-.37758093-.39120768-.98937525-.39160691-1.367372.0000316l-4.03091981 4.1763942c-.37759778.39122514-.38381821 1.01908149-.01600053 1.40017357.37750607.39113012.98772445.3930364 1.37006824-.00310603l2.39538588-2.48183446v11.61478958l.00649339.1166211c.055753.4973387.46370161.8833789.95867408.8833789.49497246 0 .90292107-.3860402.95867408-.8833789l.00649338-.1166211v-11.61478958l2.39518592 2.4816273c.3791392.39282216.9863753.40056173 1.3541929.01946965.3775061-.39113012.3778444-1.02492687-.0001355-1.41654791z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="icon-tick" viewBox="0 0 24 24">
            <path d="M12,24 C5.372583,24 0,18.627417 0,12 C0,5.372583 5.372583,0 12,0 C18.627417,0 24,5.372583 24,12 C24,18.627417 18.627417,24 12,24 Z M7.657,10.79 C7.45285634,10.6137568 7.18569967,10.5283283 6.91717333,10.5534259 C6.648647,10.5785236 6.40194824,10.7119794 6.234,10.923 C5.87705269,11.3666969 5.93445559,12.0131419 6.364,12.387 L10.261,15.754 C10.6765468,16.112859 11.3037113,16.0695601 11.666,15.657 L17.759,8.713 C18.120307,8.27302248 18.0695334,7.62621189 17.644,7.248 C17.4414817,7.06995024 17.1751516,6.9821166 16.9064461,7.00476032 C16.6377406,7.02740404 16.3898655,7.15856958 16.22,7.368 L10.768,13.489 L7.657,10.79 Z"/>
        </symbol>
        <symbol id="icon-expand-image" viewBox="0 0 18 18">
            <path d="m7.49754099 11.9178212c.38955542-.3895554.38761957-1.0207846-.00290473-1.4113089-.39324695-.3932469-1.02238878-.3918247-1.41130883-.0029047l-4.10273549 4.1027355.00055454-3.5103985c.00008852-.5603185-.44832171-1.006032-1.00155062-1.0059446-.53903074.0000852-.97857527.4487442-.97866268 1.0021075l-.00093318 5.9072465c-.00008751.553948.44841131 1.001882 1.00174994 1.0017946l5.906983-.0009331c.5539233-.0000875 1.00197907-.4486389 1.00206646-1.0018679.00008515-.5390307-.45026621-.9784332-1.00588841-.9783454l-3.51010549.0005545zm3.00571741-5.83449376c-.3895554.38955541-.3876196 1.02078454.0029047 1.41130883.393247.39324696 1.0223888.39182478 1.4113089.00290473l4.1027355-4.10273549-.0005546 3.5103985c-.0000885.56031852.4483217 1.006032 1.0015506 1.00594461.5390308-.00008516.9785753-.44874418.9786627-1.00210749l.0009332-5.9072465c.0000875-.553948-.4484113-1.00188204-1.0017499-1.00179463l-5.906983.00093313c-.5539233.00008751-1.0019791.44863892-1.0020665 1.00186784-.0000852.53903074.4502662.97843325 1.0058884.97834547l3.5101055-.00055449z" fill="currentColor" fill-rule="evenodd"/>
        </symbol>
        <symbol id="icon-close" viewBox="0 0 16 16">
            <path d="m2.29679575 12.2772478c-.39658757.3965876-.39438847 1.0328109-.00062148 1.4265779.39651227.3965123 1.03246768.3934888 1.42657791-.0006214l4.27724782-4.27724787 4.2772478 4.27724787c.3965876.3965875 1.0328109.3943884 1.4265779.0006214.3965123-.3965122.3934888-1.0324677-.0006214-1.4265779l-4.27724787-4.2772478 4.27724787-4.27724782c.3965875-.39658757.3943884-1.03281091.0006214-1.42657791-.3965122-.39651226-1.0324677-.39348875-1.4265779.00062148l-4.2772478 4.27724782-4.27724782-4.27724782c-.39658757-.39658757-1.03281091-.39438847-1.42657791-.00062148-.39651226.39651227-.39348875 1.03246768.00062148 1.42657791l4.27724782 4.27724782z" fill="currentColor" fill-rule="evenodd"/>
        </symbol>
        <symbol id="icon-chevron-right" viewBox="0 0 7 12">
            <path d="M2.782 5 .3 2.518c-.393-.392-.4-1.022-.02-1.403a1.001 1.001 0 0 1 1.417 0l4.176 4.177a1.001 1.001 0 0 1 0 1.416l-4.176 4.177a.991.991 0 0 1-1.4.016A1 1 0 0 1 .3 9.481L2.782 7l1.013-.998L2.782 5Z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="icon-checklist-banner" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 56.69 56.69" style="enable-background:new 0 0 56.69 56.69" xml:space="preserve">
            <path style="fill:none" d="M0 0h56.69v56.69H0z"/><defs><path id="a" d="M0 .74h56.72v55.24H0z"/></defs><clipPath id="b"><use xlink:href="#a" style="overflow:visible"/></clipPath><path d="M21.14 34.46c0-6.77 5.48-12.26 12.24-12.26s12.24 5.49 12.24 12.26-5.48 12.26-12.24 12.26c-6.76-.01-12.24-5.49-12.24-12.26zm19.33 10.66 10.23 9.22s1.21 1.09 2.3-.12l2.09-2.32s1.09-1.21-.12-2.3l-10.23-9.22m-19.29-5.92c0-4.38 3.55-7.94 7.93-7.94s7.93 3.55 7.93 7.94c0 4.38-3.55 7.94-7.93 7.94-4.38-.01-7.93-3.56-7.93-7.94zm17.58 12.99 4.14-4.81" style="clip-path:url(#b);fill:none;stroke:#01324b;stroke-width:2;stroke-linecap:round"/><path d="M8.26 9.75H28.6M8.26 15.98H28.6m-20.34 6.2h12.5m14.42-5.2V4.86s0-2.93-2.93-2.93H4.13s-2.93 0-2.93 2.93v37.57s0 2.93 2.93 2.93h15.01M8.26 9.75H28.6M8.26 15.98H28.6m-20.34 6.2h12.5" style="clip-path:url(#b);fill:none;stroke:#01324b;stroke-width:2;stroke-linecap:round;stroke-linejoin:round"/>
        </symbol>
        <symbol id="icon-get-ftr" viewBox="0 0 24 24">
            <path fill="#0D8D8A" fill-rule="nonzero" d="M24 12c0 6.627-5.373 12-12 12-2.102 0-4.078-.54-5.796-1.49l1.485-1.484A9.96 9.96 0 0 0 12 22c5.523 0 10-4.477 10-10a9.96 9.96 0 0 0-.974-4.31l1.484-1.486A11.946 11.946 0 0 1 24 12ZM12 0c2.102 0 4.079.54 5.797 1.49l-1.485 1.485A9.96 9.96 0 0 0 12 2C6.477 2 2 6.477 2 12c0 1.544.35 3.006.975 4.312L1.49 17.797A11.946 11.946 0 0 1 0 12C0 5.373 5.373 0 12 0Z"/>
            <circle cx="12" cy="12" r="5.333" fill="#096A73"/>
        </symbol>
        <symbol id="icon-github" viewBox="0 0 100 100">
            <path fill-rule="evenodd" clip-rule="evenodd" d="M48.854 0C21.839 0 0 22 0 49.217c0 21.756 13.993 40.172 33.405 46.69 2.427.49 3.316-1.059 3.316-2.362 0-1.141-.08-5.052-.08-9.127-13.59 2.934-16.42-5.867-16.42-5.867-2.184-5.704-5.42-7.17-5.42-7.17-4.448-3.015.324-3.015.324-3.015 4.934.326 7.523 5.052 7.523 5.052 4.367 7.496 11.404 5.378 14.235 4.074.404-3.178 1.699-5.378 3.074-6.6-10.839-1.141-22.243-5.378-22.243-24.283 0-5.378 1.94-9.778 5.014-13.2-.485-1.222-2.184-6.275.486-13.038 0 0 4.125-1.304 13.426 5.052a46.97 46.97 0 0 1 12.214-1.63c4.125 0 8.33.571 12.213 1.63 9.302-6.356 13.427-5.052 13.427-5.052 2.67 6.763.97 11.816.485 13.038 3.155 3.422 5.015 7.822 5.015 13.2 0 18.905-11.404 23.06-22.324 24.283 1.78 1.548 3.316 4.481 3.316 9.126 0 6.6-.08 11.897-.08 13.526 0 1.304.89 2.853 3.316 2.364 19.412-6.52 33.405-24.935 33.405-46.691C97.707 22 75.788 0 48.854 0z" fill="#24292f"/>
        </symbol>
        <symbol id="icon-search-filter" viewBox="0 0 29 29">
            <defs><style>.cls-1{fill:none;}</style></defs><title/><g data-name="Layer 2" id="Layer_2"><path d="M28,9H11a1,1,0,0,1,0-2H28a1,1,0,0,1,0,2Z"/><path d="M7,9H4A1,1,0,0,1,4,7H7A1,1,0,0,1,7,9Z"/><path d="M21,17H4a1,1,0,0,1,0-2H21a1,1,0,0,1,0,2Z"/><path d="M11,25H4a1,1,0,0,1,0-2h7a1,1,0,0,1,0,2Z"/><path d="M9,11a3,3,0,1,1,3-3A3,3,0,0,1,9,11ZM9,7a1,1,0,1,0,1,1A1,1,0,0,0,9,7Z"/><path d="M23,19a3,3,0,1,1,3-3A3,3,0,0,1,23,19Zm0-4a1,1,0,1,0,1,1A1,1,0,0,0,23,15Z"/><path d="M13,27a3,3,0,1,1,3-3A3,3,0,0,1,13,27Zm0-4a1,1,0,1,0,1,1A1,1,0,0,0,13,23Z"/><path d="M28,17H25a1,1,0,0,1,0-2h3a1,1,0,0,1,0,2Z"/><path d="M28,25H15a1,1,0,0,1,0-2H28a1,1,0,0,1,0,2Z"/></g><g id="frame"><rect class="cls-1" height="32" width="32"/></g>
        </symbol>
        <symbol id="icon-book" viewBox="0 0 18 18">
            <path
                d="m4 13v-11h1v11h11v-11h-13c-.55228475 0-1 .44771525-1 1v10.2675644c.29417337-.1701701.63571286-.2675644 1-.2675644zm12 1h-13c-.55228475 0-1 .4477153-1 1s.44771525 1 1 1h13zm0 3h-13c-1.1045695 0-2-.8954305-2-2v-12c0-1.1045695.8954305-2 2-2h13c.5522847 0 1 .44771525 1 1v14c0 .5522847-.4477153 1-1 1zm-8.5-13h6c.2761424 0 .5.22385763.5.5s-.2238576.5-.5.5h-6c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5zm1 2h4c.2761424 0 .5.22385763.5.5s-.2238576.5-.5.5h-4c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5z"
                fill-rule="evenodd"/>
        </symbol>
        <symbol id="icon-submit-open" viewBox="0 0 16 17">
            <path d="M12 0c1.10457 0 2 .895431 2 2v5c0 .276142-.223858.5-.5.5S13 7.276142 13 7V2c0-.512836-.38604-.935507-.883379-.993272L12 1H6v3c0 1.10457-.89543 2-2 2H1v8c0 .512836.38604.935507.883379.993272L2 15h6.5c.276142 0 .5.223858.5.5s-.223858.5-.5.5H2c-1.104569 0-2-.89543-2-2V5.828427c0-.530433.210714-1.039141.585786-1.414213L4.414214.585786C4.789286.210714 5.297994 0 5.828427 0H12Zm3.41 11.14c.250899.250899.250274.659726 0 .91-.242954.242954-.649606.245216-.9-.01l-1.863671-1.900337.001043 5.869492c0 .356992-.289839.637138-.647372.637138-.347077 0-.647371-.285256-.647371-.637138l-.001043-5.869492L9.5 12.04c-.253166.258042-.649726.260274-.9.01-.242954-.242954-.252269-.657731 0-.91l2.942184-2.951303c.250908-.250909.66127-.252277.91353-.000017L15.41 11.14ZM5 1.413 1.413 5H4c.552285 0 1-.447715 1-1V1.413ZM11 3c.276142 0 .5.223858.5.5s-.223858.5-.5.5H7.5c-.276142 0-.5-.223858-.5-.5s.223858-.5.5-.5H11Zm0 2c.276142 0 .5.223858.5.5s-.223858.5-.5.5H7.5c-.276142 0-.5-.223858-.5-.5s.223858-.5.5-.5H11Z" fill-rule="nonzero"/>
        </symbol>
    </svg>

</body>
</html>


